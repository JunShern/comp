{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "1. Tutorial on TimeDistributed layer: [machinelearningmastery blog](https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/)\n",
    "2. Hints on [StackOverflow](https://stackoverflow.com/questions/47671732/keras-input-a-3-channel-image-into-lstm) about how to deal with image-LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Imports and user variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import pypianoroll\n",
    "from matplotlib import pyplot as plt\n",
    "import cPickle as pickle\n",
    "import pianoroll_utils\n",
    "import IPython\n",
    "import h5py\n",
    "\n",
    "from keras.layers import Input, Dense, Conv2D, Conv2DTranspose, BatchNormalization, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.layers import Activation, LSTM, RNN, Concatenate, concatenate, Dropout\n",
    "from keras import optimizers\n",
    "from keras.layers import TimeDistributed, Flatten, Reshape\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from six.moves import range\n",
    "\n",
    "NUM_FILES = 1000\n",
    "WINDOW_LENGTH = 4\n",
    "SEQ_FILE = './pickle_jar/seq_{}_songs_clipped96.h5'.format(NUM_FILES)\n",
    "\n",
    "# Music shape\n",
    "MIN_PITCH = 13 # C-1\n",
    "MAX_PITCH = 108 # C7 (MIDI 108)\n",
    "BEATS_PER_UNIT = 4\n",
    "NUM_TRANSPOSITIONS = 3 # Number of transpositions to perform (maximum 12)\n",
    "\n",
    "# Don't change unless you know what you're doing\n",
    "BEAT_RESOLUTION = 24 # This is set by the encoding of the lpd-5 dataset, corresponds to number of ticks per beat\n",
    "PARTITION_NOTE = 60 # Break into left- and right-accompaniments at middle C\n",
    "NUM_PITCHES = MAX_PITCH - MIN_PITCH + 1\n",
    "NUM_TICKS = BEATS_PER_UNIT * BEAT_RESOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "\n",
    "class TimeseriesGeneratorTwoInputs(Sequence):\n",
    "    \"\"\"Keras data reading class\n",
    "    Utility class for generating batches of temporal data.\n",
    "    This class takes in a sequence of data-points gathered at\n",
    "    equal intervals, along with time series parameters such as\n",
    "    stride, length of history, etc., to produce batches for\n",
    "    training/validation.\n",
    "    \n",
    "    Adapted from the TimeseriesGenerator class:\n",
    "    https://github.com/keras-team/keras/blob/master/keras/utils/data_utils.py#L302\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data1, data2, targets, length,\n",
    "                 sampling_rate=1,\n",
    "                 stride=1,\n",
    "                 start_index=0,\n",
    "                 end_index=None,\n",
    "                 shuffle=False,\n",
    "                 reverse=False,\n",
    "                 batch_size=128):\n",
    "        self.data1 = data1\n",
    "        self.data2 = data2\n",
    "        assert len(data1) == len(data2)\n",
    "        self.targets = targets\n",
    "        self.length = length\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.stride = stride\n",
    "        self.start_index = start_index + length\n",
    "        if end_index is None:\n",
    "            end_index = len(data1) - 1\n",
    "        self.end_index = end_index\n",
    "        self.shuffle = shuffle\n",
    "        self.reverse = reverse\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if self.start_index > self.end_index:\n",
    "            raise ValueError('`start_index+length=%i > end_index=%i` '\n",
    "                             'is disallowed, as no part of the sequence '\n",
    "                             'would be left to be used as current step.'\n",
    "                             % (self.start_index, self.end_index))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(\n",
    "            (self.end_index - self.start_index + 1) /\n",
    "            (self.batch_size * self.stride)))\n",
    "\n",
    "    def _empty_batch(self, num_rows):\n",
    "        samples_shape = [num_rows, self.length // self.sampling_rate]\n",
    "        samples_shape.extend(self.data1.shape[1:])\n",
    "        targets_shape = [num_rows]\n",
    "        targets_shape.extend(self.targets.shape[1:])\n",
    "        return np.empty(samples_shape), np.empty(samples_shape), np.empty(targets_shape)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.shuffle:\n",
    "            rows = np.random.randint(\n",
    "                self.start_index, self.end_index + 1, size=self.batch_size)\n",
    "        else:\n",
    "            i = self.start_index + self.batch_size * self.stride * index\n",
    "            rows = np.arange(i, min(i + self.batch_size *\n",
    "                                    self.stride, self.end_index + 1), self.stride)\n",
    "\n",
    "        samples1, samples2, targets = self._empty_batch(len(rows))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - self.length, rows[j], self.sampling_rate)\n",
    "            samples1[j] = self.data1[indices]\n",
    "            samples2[j] = self.data2[indices]\n",
    "            targets[j] = self.targets[rows[j]]\n",
    "        if self.reverse:\n",
    "            return [samples1[:, ::-1, ...], samples2[:, ::-1, ...]], targets\n",
    "        return [samples1, samples2], targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "Unable to open file (unable to open file: name = './pickle_jar/seq_1000_songs_clipped96.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5cdc0298c126>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEQ_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mseq_units_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mseq_units_comp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comp_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mseq_embed_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_embed_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/human/tensorflow/local/lib/python2.7/site-packages/h5py/_hl/files.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/human/tensorflow/local/lib/python2.7/site-packages/h5py/_hl/files.pyc\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: Unable to open file (unable to open file: name = './pickle_jar/seq_1000_songs_clipped96.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "f = h5py.File(SEQ_FILE, 'r')\n",
    "seq_units_input = f['input_test']\n",
    "seq_units_comp = f['comp_test']\n",
    "seq_embed_input = f['input_embed_test']\n",
    "seq_embed_comp = f['comp_embed_test']\n",
    "data_gen_train = TimeseriesGeneratorTwoInputs(f['input_embed_train'], f['comp_embed_train'], f['comp_embed_train'], \n",
    "                                              length=WINDOW_LENGTH, sampling_rate=1, batch_size=128)\n",
    "data_gen_test = TimeseriesGeneratorTwoInputs(f['input_embed_test'], f['comp_embed_test'], f['comp_embed_test'], \n",
    "                                              length=WINDOW_LENGTH, sampling_rate=1, batch_size=128)\n",
    "print(seq_units_input.shape)\n",
    "print(seq_units_comp.shape)\n",
    "print(seq_embed_input.shape)\n",
    "print(seq_embed_comp.shape)\n",
    "print(len(data_gen_train))\n",
    "print(len(data_gen_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V1: Direct LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model skips any convolutional steps, diving straight into LSTM on whole units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_20 (InputLayer)           (None, 4, 96, 96, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           (None, 4, 96, 96, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_15 (TimeDistri (None, 4, 9216)      0           input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_16 (TimeDistri (None, 4, 9216)      0           input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_16 (LSTM)                  (None, 800)          32054400    time_distributed_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_17 (LSTM)                  (None, 800)          32054400    time_distributed_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 1600)         0           lstm_16[0][0]                    \n",
      "                                                                 lstm_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1000)         1601000     concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 9216)         9225216     dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 96, 96, 1)    0           dense_16[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 74,935,016\n",
      "Trainable params: 74,935,016\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# data_dim = latent_dim # Our model predicts embeddings, so our data is the size of the embedding\n",
    "recurrent_dim = 800\n",
    "dense_hidden_dim = 1000\n",
    "\n",
    "# Inputs\n",
    "input1 = Input(shape=(WINDOW_LENGTH, NUM_PITCHES, NUM_TICKS, 1))\n",
    "input2 = Input(shape=(WINDOW_LENGTH, NUM_PITCHES, NUM_TICKS, 1))\n",
    "# For each time slice\n",
    "timeDist1 = TimeDistributed(Flatten())(input1)\n",
    "timeDist2 = TimeDistributed(Flatten())(input2)\n",
    "# LSTM\n",
    "lstm1 = LSTM(recurrent_dim)(timeDist1)\n",
    "lstm2 = LSTM(recurrent_dim)(timeDist2)\n",
    "# Dense\n",
    "merged = concatenate([lstm1, lstm2])\n",
    "hidden = Dense(dense_hidden_dim, activation='relu')(merged)\n",
    "# Output\n",
    "flat_output = Dense(NUM_PITCHES*NUM_TICKS, activation='sigmoid')(hidden)\n",
    "output = Reshape((NUM_PITCHES, NUM_TICKS, 1))(flat_output)\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=output)\n",
    "model.compile(loss='mse', optimizer='rmsprop') #, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V2: CVAE + LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model builds directly off of VAE_V7 and RLSTM_V5, simply joining them together to train the whole system at once. Let's see how it goes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
