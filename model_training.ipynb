{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "The ultimate goal of the model is to learn a __latent variable space of musical units__. Then, given a musical unit, we wish to encode that unit into a latent vector within the space, and predict the best accompaninment latent vector to that input. Finally, that accompaniment latent vector can be decoded to produce an accompanying musical unit.\n",
    "\n",
    "This involves many tricky steps, so development will be approached incrementally:\n",
    "\n",
    "#### 1. Convolutional Autoencoder\n",
    "\n",
    "Given an input unit of `[num_ticks, num_pitches]`, learn a Convolutional Autoencoder model to generate an encoding of that unit.\n",
    "\n",
    "```\n",
    "INPUT -> Convolution layers -> EMBEDDING -> Deconvolution layers -> INPUT\n",
    "```\n",
    "\n",
    "Autoencoding: To test this convolutional autoencoder, generate a response to a given input unit using \n",
    "- Decoder reconstruction of same input\n",
    "- Nearest-neighbor unit selection (Similar to what Bretan et al did)\n",
    "\n",
    "De-noising: Test de-noising abilities of the autoencoder. Given a partial accompaniment input unit, generate a response of\n",
    "- Decoder reconstruction of \"full\"/\"comp\" unit\n",
    "- Nearest-neighbor unit selection\n",
    "\n",
    "#### 2. LSTM of latent variables -> Generation using unit selection\n",
    "\n",
    "Given a sequence of embeddings (from the convolutional autoencoder), predict the next embedding - and perform NN-unit-selection as before, to generate the next unit in the sequence.\n",
    "\n",
    "#### 3. Convolutional Variational Autoencoder\n",
    "\n",
    "Learn a new latent space using a VAE architecture. Test how well resconstruction works using\n",
    "- Decoder reconstruction\n",
    "\n",
    "#### 4. LSTM of variational latent variables -> Generation using latent space sampling \n",
    "\n",
    "Given a sequence of embeddings (from the VAE), predict the next embedding and generate an output musical unit by decoding the predicted embedding!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import pypianoroll\n",
    "from matplotlib import pyplot as plt\n",
    "import cPickle as pickle\n",
    "import pianoroll_utils\n",
    "\n",
    "PICKLE_FILE = './pickle_jar/units_50_songs.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, Conv2DTranspose, BatchNormalization, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3268 units from ./pickle_jar/units_50_songs.pkl\n",
      "input_units.shape:  (3268, 96, 128)\n",
      "input_units_next.shape:  (3268, 96, 128)\n",
      "comp_units.shape:  (3268, 96, 128)\n",
      "comp_units_next.shape:  (3268, 96, 128)\n"
     ]
    }
   ],
   "source": [
    "units = {}\n",
    "with open(PICKLE_FILE, 'rb') as infile:\n",
    "    units = pickle.load( infile )\n",
    "\n",
    "# Print info\n",
    "print \"Loaded\", units[\"input\"].shape[0], \"units from\", PICKLE_FILE\n",
    "print \"input_units.shape: \", units[\"input\"].shape\n",
    "print \"input_units_next.shape: \", units[\"input_next\"].shape\n",
    "print \"comp_units.shape: \", units[\"comp\"].shape\n",
    "print \"comp_units_next.shape: \", units[\"comp_next\"].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: (3268, 96, 128)\n",
      "Reshaped: (3268, 128, 96, 1)\n",
      "Train: (2942, 128, 96, 1)\n",
      "Test: (326, 128, 96, 1)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "print \"Original:\", units[\"input\"].shape\n",
    "NUM_TICKS = units[\"input\"].shape[1] # 96\n",
    "NUM_PITCHES = units[\"input\"].shape[2] # 128\n",
    "assert NUM_TICKS == 96 and NUM_PITCHES == 128\n",
    "\n",
    "# Change from [M, ticks, pitches] to [M, pitches, ticks, channels=1]\n",
    "input_units = units[\"input\"].swapaxes(1,2).reshape(len(units[\"input\"]), NUM_PITCHES, NUM_TICKS, 1)\n",
    "comp_units = units[\"comp\"].swapaxes(1,2).reshape(len(units[\"comp\"]), NUM_PITCHES, NUM_TICKS, 1)\n",
    "# Normalize values between 0 and 1\n",
    "input_units = input_units.astype('float32') / 127. # 0-127 is the unnormalized velocity range\n",
    "comp_units = comp_units.astype('float32') / 127. # 0-127 is the unnormalized velocity range\n",
    "print \"Reshaped:\", input_units.shape\n",
    "\n",
    "# Create an array of True (train) and False (test) to split the dataset\n",
    "train_test_indices = np.random.choice([True, False], size=len(input_units), p=[.9, .1])\n",
    "# Training data\n",
    "input_train = input_units[train_test_indices, ...]\n",
    "comp_train = comp_units[train_test_indices, ...]\n",
    "# Testing data\n",
    "input_test = input_units[np.invert(train_test_indices), ...]\n",
    "comp_test = comp_units[np.invert(train_test_indices), ...]\n",
    "print \"Train:\", input_train.shape\n",
    "print \"Test:\", input_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convolutional Autoencoder\n",
    "\n",
    "Given an input unit of `[num_ticks, num_pitches]`, learn a Convolutional Autoencoder model to generate an encoding of that unit.\n",
    "\n",
    "```\n",
    "INPUT -> Convolution layers -> EMBEDDING -> Deconvolution layers -> INPUT\n",
    "```\n",
    "\n",
    "### Testing\n",
    "\n",
    "We will evaluate the autoencoder using two measures:\n",
    "\n",
    "1. __Autoencoding__: To test this convolutional autoencoder, generate a response to a given input unit using \n",
    "\n",
    "    - Decoder reconstruction of same input\n",
    "    - Nearest-neighbor unit selection (Similar to what Bretan et al did)\n",
    "\n",
    "2. __De-noising__: Test de-noising abilities of the autoencoder. Given a partial accompaniment input unit, generate a response of\n",
    "\n",
    "    - Decoder reconstruction of \"full\"/\"comp\" unit\n",
    "    - Nearest-neighbor unit selection\n",
    "\n",
    "    (inspired by Huang et al Counterpoint by Convolution, and Bretan et al Learning and Evaluating Musical Features with Deep Autoencoders)\n",
    "\n",
    "These two tests simply require training the model on two different datasets: \"full\"->\"full\" for autoencoding, and \"input\"->\"comp\" for de-noising.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "\n",
    "_Initial code adapted from the [Keras tutorial on autoencoders](https://blog.keras.io/building-autoencoders-in-keras.html)._\n",
    "\n",
    "_Inspiration for convolution autoencoder network from \"Learning and Evaluating Musical Features with Deep\n",
    "Autoencoders\"._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder V0\n",
    "\n",
    "`code given below`\n",
    "\n",
    "### Details\n",
    "\n",
    "Based on \"Learning and Evaluating Musical Features with Deep Autoencoders\", but adapted for different input size.\n",
    "\n",
    "```\n",
    "Data: -\n",
    "Embedding shape: (None, 1, 1, 800) -> 800 elements\n",
    "Epochs: -\n",
    "Batch size: -\n",
    "Final loss: -\n",
    "```\n",
    "\n",
    "### Notes\n",
    "\n",
    "Pretty sophisticated model, but unfortunately not able to train due to a `ResourceExhaustedError` upon running `model.fit`. This is most likely due to insufficient GPU memory (model is very large).\n",
    "\n",
    "Several attempts were made to shrink the model / reduce batch size (which apparently helps), but was not able to shake the error.\n",
    "\n",
    "### Next steps\n",
    "1. Look at how to shrink this model / use an alternative model. This [SO thread](https://stackoverflow.com/questions/41526071/why-is-keras-throwing-a-resourceexhaustederror) may be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_mat = Input(shape=(NUM_PITCHES, NUM_TICKS, 1))  # 'channels_last' data format (only 1 channel in our case)\n",
    "\n",
    "## ENCODER\n",
    "\n",
    "# First four layers are Conv2D\n",
    "x = Conv2D(100, (13, 21), strides=(5,5), activation='relu', padding='valid')(input_mat)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Conv2D(200, (2, 7), strides=(2,3), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Conv2D(400, (2, 2), strides=(2,2), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Conv2D(800, (2, 2), strides=(2,1), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "# Following three are fully connected\n",
    "x = Conv2D(800, (3, 1), strides=(1,1), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Dense(400, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "encoded = BatchNormalization()(x)\n",
    "\n",
    "# at this point the representation is a 100-dimensional vector\n",
    "\n",
    "## DECODER\n",
    "\n",
    "# Two fully connected\n",
    "decoded = Dense(400, activation='relu')(encoded)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2DTranspose(800, (3, 1), strides=(1,1), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "# Deconvolution / Convolution Transpose layers\n",
    "x = Conv2DTranspose(800, (2, 2), strides=(2,1), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Conv2DTranspose(400, (2, 2), strides=(2,2), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Conv2DTranspose(200, (2, 7), strides=(2,3), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Conv2DTranspose(100, (13, 21), strides=(5,5), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Conv2DTranspose(1, (NUM_TICKS, NUM_PITCHES), activation='relu', padding='valid')(x)\n",
    "decoded = BatchNormalization(axis=3)(x)\n",
    "\n",
    "autoencoder = Model(input_mat, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 128, 96, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 24, 16, 100)       27400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_85 (Batc (None, 24, 16, 100)       400       \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 12, 4, 200)        280200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_86 (Batc (None, 12, 4, 200)        800       \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 6, 2, 400)         320400    \n",
      "_________________________________________________________________\n",
      "batch_normalization_87 (Batc (None, 6, 2, 400)         1600      \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 3, 1, 800)         1280800   \n",
      "_________________________________________________________________\n",
      "batch_normalization_88 (Batc (None, 3, 1, 800)         3200      \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 1, 1, 800)         1920800   \n",
      "_________________________________________________________________\n",
      "batch_normalization_89 (Batc (None, 1, 1, 800)         3200      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1, 1, 400)         320400    \n",
      "_________________________________________________________________\n",
      "batch_normalization_90 (Batc (None, 1, 1, 400)         1600      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1, 1, 100)         40100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_92 (Batc (None, 1, 1, 100)         400       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_34 (Conv2DT (None, 3, 1, 800)         240800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_93 (Batc (None, 3, 1, 800)         3200      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_35 (Conv2DT (None, 6, 2, 800)         2560800   \n",
      "_________________________________________________________________\n",
      "batch_normalization_94 (Batc (None, 6, 2, 800)         3200      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_36 (Conv2DT (None, 12, 4, 400)        1280400   \n",
      "_________________________________________________________________\n",
      "batch_normalization_95 (Batc (None, 12, 4, 400)        1600      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_37 (Conv2DT (None, 24, 16, 200)       1120200   \n",
      "_________________________________________________________________\n",
      "batch_normalization_96 (Batc (None, 24, 16, 200)       800       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_38 (Conv2DT (None, 128, 96, 100)      5460100   \n",
      "_________________________________________________________________\n",
      "batch_normalization_97 (Batc (None, 128, 96, 100)      400       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_39 (Conv2DT (None, 128, 96, 1)        1228801   \n",
      "_________________________________________________________________\n",
      "batch_normalization_98 (Batc (None, 128, 96, 1)        4         \n",
      "=================================================================\n",
      "Total params: 16,101,605\n",
      "Trainable params: 16,091,403\n",
      "Non-trainable params: 10,202\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2929 samples, validate on 339 samples\n",
      "Epoch 1/100\n",
      " 256/2929 [=>............................] - ETA: 1:00 - loss: 0.6844"
     ]
    }
   ],
   "source": [
    "# Train model model\n",
    "autoencoder.fit(input_train, input_train,\n",
    "                epochs=100,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(input_test, input_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\n",
    "# Run `tensorboard --logdir=/tmp/autoencoder` to start tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder V1\n",
    "\n",
    "`code given below`\n",
    "\n",
    "### Details\n",
    "\n",
    "Pretty arbitrary variant of the convolutional autoencoder architecture suggested in the [Keras tutorial](https://blog.keras.io/building-autoencoders-in-keras.html).\n",
    "\n",
    "```\n",
    "Data: input->input\n",
    "Embedding shape: (None, 32, 24, 32) -> 24576 elements\n",
    "Epochs: 100\n",
    "Batch size: 32\n",
    "Final loss: [loss: -0.0047 - val_loss: -0.0052]\n",
    "```\n",
    "\n",
    "### Notes\n",
    "~~Final binary crossentropy loss (after 100 epochs) gives `loss: -0.0047 - val_loss: -0.0052`, which is strange since __I don't think binary crossentropy should give negative values__? Will have to investigate further.~~ \n",
    "\n",
    "Besides that, the decoded output looks really good. The graphs and playback are almost indistinguishable - can notice what appears to be quantization effects in the graphs, and some difference (note drops/additions, incorrect pitch) is occasionally audible. But mostly similar. \n",
    "\n",
    "On the whole, this model was a successful \"trial\" model. Demonstrates that the autoencoder actually produces a valid pianoroll, but our __input size was 12288 and embedding size is 24576__, which actually enlarges the dimensionality instead of shrinking it.\n",
    "\n",
    "### Next steps\n",
    "1. ~~Investigate negative loss values ([most likely](https://github.com/Lasagne/Recipes/issues/54) due to some normalization issue)~~ (Fixed with correct normalization of data in data_prep)\n",
    "2. Train on input->comp, or input->comp_next\n",
    "3. Shrink the embedding layer!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_mat = Input(shape=(NUM_PITCHES, NUM_TICKS, 1))  # 'channels_last' data format (only 1 channel in our case)\n",
    "\n",
    "# ENCODER\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_mat)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# At this point, the data is already represented in the embedding\n",
    "\n",
    "# DECODER\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_mat, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 128, 96, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 96, 32)       320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 64, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 128, 96, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 128, 96, 1)        289       \n",
      "=================================================================\n",
      "Total params: 28,353\n",
      "Trainable params: 28,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2942 samples, validate on 326 samples\n",
      "Epoch 1/100\n",
      "2942/2942 [==============================] - 35s 12ms/step - loss: 0.2273 - val_loss: 0.1643\n",
      "Epoch 2/100\n",
      "2942/2942 [==============================] - 28s 9ms/step - loss: 0.1640 - val_loss: 0.1642\n",
      "Epoch 3/100\n",
      "2942/2942 [==============================] - 30s 10ms/step - loss: 0.1177 - val_loss: 0.0276\n",
      "Epoch 4/100\n",
      "2942/2942 [==============================] - 30s 10ms/step - loss: 0.0235 - val_loss: 0.0197\n",
      "Epoch 5/100\n",
      "2942/2942 [==============================] - 29s 10ms/step - loss: 0.0163 - val_loss: 0.0128\n",
      "Epoch 6/100\n",
      "2942/2942 [==============================] - 29s 10ms/step - loss: 0.0114 - val_loss: 0.0103\n",
      "Epoch 7/100\n",
      "2942/2942 [==============================] - 29s 10ms/step - loss: 0.0098 - val_loss: 0.0093\n",
      "Epoch 8/100\n",
      "2942/2942 [==============================] - 29s 10ms/step - loss: 0.0090 - val_loss: 0.0089\n",
      "Epoch 9/100\n",
      "2942/2942 [==============================] - 29s 10ms/step - loss: 0.0086 - val_loss: 0.0085\n",
      "Epoch 10/100\n",
      "2942/2942 [==============================] - 29s 10ms/step - loss: 0.0083 - val_loss: 0.0083\n",
      "Epoch 11/100\n",
      "2942/2942 [==============================] - 29s 10ms/step - loss: 0.0082 - val_loss: 0.0082\n",
      "Epoch 12/100\n",
      "2942/2942 [==============================] - 29s 10ms/step - loss: 0.0080 - val_loss: 0.0081\n",
      "Epoch 13/100\n",
      "2942/2942 [==============================] - 29s 10ms/step - loss: 0.0079 - val_loss: 0.0080\n",
      "Epoch 14/100\n",
      "2942/2942 [==============================] - 29s 10ms/step - loss: 0.0078 - val_loss: 0.0079\n",
      "Epoch 15/100\n",
      "2942/2942 [==============================] - 29s 10ms/step - loss: 0.0078 - val_loss: 0.0078\n",
      "Epoch 16/100\n",
      "2942/2942 [==============================] - 29s 10ms/step - loss: 0.0077 - val_loss: 0.0079\n",
      "Epoch 17/100\n",
      "2942/2942 [==============================] - 29s 10ms/step - loss: 0.0077 - val_loss: 0.0078\n",
      "Epoch 18/100\n",
      "2942/2942 [==============================] - 29s 10ms/step - loss: 0.0076 - val_loss: 0.0077\n",
      "Epoch 19/100\n",
      "2942/2942 [==============================] - 29s 10ms/step - loss: 0.0076 - val_loss: 0.0077\n",
      "Epoch 20/100\n",
      "2942/2942 [==============================] - 29s 10ms/step - loss: 0.0075 - val_loss: 0.0076\n",
      "Epoch 21/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0075 - val_loss: 0.0076\n",
      "Epoch 22/100\n",
      "2942/2942 [==============================] - 29s 10ms/step - loss: 0.0074 - val_loss: 0.0076\n",
      "Epoch 23/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0074 - val_loss: 0.0076\n",
      "Epoch 24/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0074 - val_loss: 0.0075\n",
      "Epoch 25/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0074 - val_loss: 0.0075\n",
      "Epoch 26/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0073 - val_loss: 0.0074\n",
      "Epoch 27/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0073 - val_loss: 0.0074\n",
      "Epoch 28/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0073 - val_loss: 0.0074\n",
      "Epoch 29/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0073 - val_loss: 0.0074\n",
      "Epoch 30/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0072 - val_loss: 0.0074\n",
      "Epoch 31/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0072 - val_loss: 0.0074\n",
      "Epoch 32/100\n",
      "2942/2942 [==============================] - 30s 10ms/step - loss: 0.0072 - val_loss: 0.0073\n",
      "Epoch 33/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0072 - val_loss: 0.0074\n",
      "Epoch 34/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0072 - val_loss: 0.0073\n",
      "Epoch 35/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0071 - val_loss: 0.0073\n",
      "Epoch 36/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0071 - val_loss: 0.0073\n",
      "Epoch 37/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0071 - val_loss: 0.0073\n",
      "Epoch 38/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0071 - val_loss: 0.0073\n",
      "Epoch 39/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0071 - val_loss: 0.0072\n",
      "Epoch 40/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0071 - val_loss: 0.0073\n",
      "Epoch 41/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0071 - val_loss: 0.0073\n",
      "Epoch 42/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0071 - val_loss: 0.0072\n",
      "Epoch 43/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 44/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 45/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 46/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 47/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 48/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 49/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 50/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 51/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0069 - val_loss: 0.0071\n",
      "Epoch 52/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0069 - val_loss: 0.0071\n",
      "Epoch 53/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0069 - val_loss: 0.0071\n",
      "Epoch 54/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0069 - val_loss: 0.0071\n",
      "Epoch 55/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0069 - val_loss: 0.0071\n",
      "Epoch 56/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0069 - val_loss: 0.0071\n",
      "Epoch 57/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0069 - val_loss: 0.0071\n",
      "Epoch 58/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0069 - val_loss: 0.0071\n",
      "Epoch 59/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 60/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0069 - val_loss: 0.0071\n",
      "Epoch 61/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0069 - val_loss: 0.0071\n",
      "Epoch 62/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 63/100\n",
      "2942/2942 [==============================] - 30s 10ms/step - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 64/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0068 - val_loss: 0.0071\n",
      "Epoch 65/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 66/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 67/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 68/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 69/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 70/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 71/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 72/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 73/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 74/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 75/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 76/100\n",
      "2942/2942 [==============================] - 29s 10ms/step - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 77/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0068 - val_loss: 0.0069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 79/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 80/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 81/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 82/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 83/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 84/100\n",
      "2942/2942 [==============================] - 29s 10ms/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 85/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 86/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 87/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 88/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 89/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 90/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 91/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 92/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 93/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 94/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 95/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 96/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 97/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 98/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 99/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0067 - val_loss: 0.0068\n",
      "Epoch 100/100\n",
      "2942/2942 [==============================] - 28s 10ms/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Saved Keras model to ./models/autoencoder_v1.h5\n"
     ]
    }
   ],
   "source": [
    "# Train model model\n",
    "autoencoder.fit(input_train, input_train,\n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                shuffle=True,\n",
    "                validation_data=(input_test, input_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\n",
    "\n",
    "MODEL_AUTOENCODER_V1_FILE = './models/autoencoder_v1.h5'\n",
    "autoencoder.save(MODEL_AUTOENCODER_V1_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_AUTOENCODER_V1_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = load_model(MODEL_AUTOENCODER_V1_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test inputs through the autoencoder\n",
    "decoded_test = autoencoder.predict(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 128)\n",
      "(96, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAAGoCAYAAABbtxOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvFvnyVgAAIABJREFUeJzs3X183fdd3/33R7JkOZVqR4otKZYr2UrkuGRJi0zSkvUmJZRi0u3qHhC6XcDYdl1lwAiEcNeNjYttEAgtdGPX1t2wtcAuwmBNVyCUdrRJS9YGrCxpmiZWIkcix9GNK9mOlFiRLH+uPyQ5irGs85F+5/zO9/xez8fDj0rnHP/O9/f6yt9+c3RuzN0FAAAAYFlD3gMAAAAAagkbZAAAAGANNsgAAADAGmyQAQAAgDXYIAMAAABrsEEGAAAA1mCDDAAAAKzBBhl1x8xGzey2Ct/H/2Nmv13J+wCAlJjZ95vZE2b2splNmNm/M7NdZf7dTNftavz/AOobG2QAALAlZna3pF+W9JOSdkp6i6ReSZ81s+Y8xwZsBhtk1K2VRzP+zMw+ZGanzOw5M/v2Ndc/aGb3mNmfm9mLZvY/zKx95bp3mlnpouONmtltZvYeSf9Y0neb2ZyZPV7dMwOA2mFmr5f085J+xN0/7e6L7j4q6Q5JfZK+x8w+Zmb/cs3fubDGmtlvSXqDpD9YWVN/ysz6zMzN7ANm9oKZjZvZT6z5+6HjVTwC6g4bZNS7myUdk3SVpHsl/YaZ2Zrrv0/S35fULemcpH+90QHd/dOSflHS77p7q7vfmPmoASAd3yypRdIn1l7o7nOSHpD0rZf7y+7+vZL+UtJ7V9bUe9dcfaukayW9W9JPl/O0iQ2OB5SFDTLq3Zi7/0d3X5L0cS1vhDvXXP9b7v5Vd39J0j+VdIeZNeYxUABI1FWSvu7u5y5x3fjK9Zv18+7+krs/Iem/SPrbWzgWUDY2yKh3E6tfuPvLK1+2rrn++TVfj0lq0tYWcwAomq9LusrMtl3iuu6V6zfr4jX66i0cCygbG2QU3b41X79B0qKWF/OXJF2xesXKo8q719zWqzI6AKh9X5L0iqS/tfZCM2uV9O2S/lQXramSui46xnpr6sVr9AsrX2/2eEBZ2CCj6L7HzN5oZldI+ueSfn/l6RjDklrM7DvMrEnSz0ravubvTUrqMzP+DQEoNHc/o+UX6f26mb3HzJrMrE/Sf5NUkvRbkh6TdMTM2s2sS9KPXXSYSUkHLnH4f2pmV5jZN0j6e5J+d+XyzR4PKAv/546i+y1JH9PyUzFaJN0pXVjwf0jSf5J0QsuPVqx9V4vfW/nfaTN7tFqDBYBatPJCuH8s6UOSXpT0iJafHvEt7v6KltfaxyWNSvqMXt3orrpH0s+a2em171Yh6SFJz2r5UegPuftnVi7f7PGAspg7v4VAMZnZg5J+293/U95jAQC8auUR6OckNa3z4j+gongEGQAAAFiDDTIAAACwBk+xAAAAANbgEWQAAABgjUu9qXdNuuqqq3z3nj26YscVG98YF7x89mWaBdEsjmblGxsb1de//nXb+Ja156qrrvLe3j7mexNoFkOvOJrFPPro0Nfdffd61yezQe7t7dMnPvVH6uzs3PjGuGBycpJmQTSLo1n5brn5cN5D2LTe3j49/MhR5nsTaBZDrziaxexosrHLXZ/UUyyOj4zkPYTk0CyOZnE0KxbmO45mMfSKo1m2ktogLy4u5D2E5NAsjmZxNCsW5juOZjH0iqNZtpLaILe2tuU9hOTQLI5mcTQrFuY7jmYx9IqjWbaS2iDv7enJewjJoVkczeJoVizMdxzNYugVR7NsJbVBHhsdzXsIyaFZHM3iaFYszHcczWLoFUezbCW1QZ6fP5v3EJJDsziaxdGsWJjvOJrF0CuOZtlKaoPc0rIj7yEkh2ZxNIujWbEw33E0i6FXHM2yldQGubevL+8hJIdmcTSLo1mxMN9xNIuhVxzNspXUBvlEqZT3EJJDsziaxdGsWJjvOJrF0CuOZtlKaoM8Nzeb9xCSQ7M4msXRrFiY7ziaxdArjmbZSmqD3NTUnPcQkkOzOJrF0axYmO84msXQK45m2Upqg3ygvz/vISSHZnE0i6NZsTDfcTSLoVcczbKV1AZ5Ynw87yEkh2ZxNIujWbEw33E0i6FXHM2yldQG+cyZ03kPITk0i6NZHM2KhfmOo1kMveJolq2kNsgNjY15DyE5NIujWRzNioX5jqNZDL3iaJatpDbIAwMH8x5CcmgWR7M4mhUL8x1Hsxh6xdEsW0ltkKcmJ/MeQnJoFkezOJoVC/MdR7MYesXRLFtV2yCbWZeZ3WdmI2Y2ZGYPmNmAmd1rZk+a2VNm9q/NzNY7xszMdLWGWzdoFkezOJqlg7U4HzSLoVcczbK1rRp3srLQ3i/p4+7+/pXLbpTULekWSTes3PTPJL1D0oPVGBcAFAlrMQCUp1qPIN8qadHdP7p6gbs/LmlBUoukZknbJTVJWvd3BAevO1ThYdYfmsXRLI5myWAtzgnNYugVR7NsVeURZEnXSxq6+EJ3/5KZfV7SuCST9G/c/alLHaBUKulb3/UObdu2TefOLerIkdt15113a252VhMTy+/9t/9AvxYXFlQqPS9J6u3tkySNjY1Kknp69qmpuVnPHR+RJHV1dau1rU3PPjMsSdq9e4/aOzp07OnlIbS3d2hPZ6eGh4/p/NKSdu7cpa7ubh0fGdHi4oJaW9u0t6dHY6Ojmp8/q5aWHert69OJUklzc7NqamrWgf5+TYyP68yZ02pobNTAwEFNTU5e+FXIwesOaWZ6WidPTkmSrrl2INNzmn3xRR2+6ea6OqdKz9Pzzz+vb333t9XVOVV6nsZGR3XzW95aV+dUqXnK2ZbW4lKppJsGb9RLL72kxsZG1uHAOU1Pf73uzol1uLbmiXU4dk4bMXff8EZbZWZ3Strv7ndddPk1kv6VpO9eueizkn7K3b948TEGBw/7Pfd+SG9/xzsrPdy68oWHHqRZEM3iaFa+W24+rKGho+s+v7eStroWDw4e9ocfOcp8bwLNYugVR7OYHU025O6H17u+Wk+xeFLS4CUuf5+kL7v7nLvPSfpjSW+t0pgAoGhYiwGgDNXaIH9O0nYz+8DqBWZ2g6QrJL3DzLaZWZOWXxRyyadYSMsP0yOGZnE0i6NZMliLc0KzGHrF0SxbVdkg+/LzON4n6baVtxZ6UtI9ku6TNCLpCUmPS3rc3f9gvePMzc5WY7h1hWZxNIujWRpYi/NDsxh6xdEsW1V7H2R3f8Hd73D3fnf/Bnf/Dnc/5u4/4O6H3P2N7v7jlzvG6hO8UT6axdEsjmbpYC3OB81i6BVHs2wl9Ul6AAAAQKUltUHef6A/7yEkh2ZxNIujWbEw33E0i6FXHM2yldQGeXFhIe8hJIdmcTSLo1mxMN9xNIuhVxzNspXUBnn1TaVRPprF0SyOZsXCfMfRLIZecTTLVlIbZAAAAKDSktogr34cIcpHsziaxdGsWJjvOJrF0CuOZtlKaoMMAAAAVFpSG+SxsdG8h5AcmsXRLI5mxcJ8x9Eshl5xNMtWUhtkAAAAoNKS2iD39OzLewjJoVkczeJoVizMdxzNYugVR7NsJbVBbmpuznsIyaFZHM3iaFYszHcczWLoFUezbCW1QX7u+EjeQ0gOzeJoFkezYmG+42gWQ684mmUrqQ0yAAAAUGlJbZC7urrzHkJyaBZHsziaFQvzHUezGHrF0SxbSW2QW9va8h5CcmgWR7M4mhUL8x1Hsxh6xdEsW0ltkJ99ZjjvISSHZnE0i6NZsTDfcTSLoVcczbKV1AYZAAAAqLSkNsi7d+/JewjJoVkczeJoVizMdxzNYugVR7NsJbVBbu/oyHsIyaFZHM3iaFYszHcczWLoFUezbCW1QT729FN5DyE5NIujWRzNioX5jqNZDL3iaJatpDbIAAAAQKUltUFub+fXB1E0i6NZHM2KhfmOo1kMveJolq2kNsh7OjvzHkJyaBZHsziaFQvzHUezGHrF0SxbSW2Qh4eP5T2E5NAsjmZxNCsW5juOZjH0iqNZtpLaIJ9fWsp7CMmhWRzN4mhWLMx3HM1i6BVHs2xtq9YdmVmXpI9I+iZJpyVNSvofkn5wzc2uk/R+d//kpY6xc+euSg+z7tAsjmZxNEsHa3E+aBZDrziaZasqG2QzM0n3S/q4u79/5bIbJb3e3d+08n27pGclfWa943R1d1dhtPWFZnE0i6NZGliL80OzGHrF0Sxb1XqKxa2SFt39o6sXuPvj7v7FNbf5Tkl/7O4vr3eQ4yMjFRxifaJZHM3iaJYM1uKc0CyGXnE0y1a1nmJxvaShDW7zfkm/ut6VpVJJ/9ff+z5tb2nRuXOLOnLkdt15192am53VxMS4JGn/gX4tLiyoVHpektTb2ydJGhsblST19OxTU3Oznju+/EPU1dWt1rY2PfvMsKTlj2ls7+i48Gbb7e0d2tPZqeHhYzq/tKSdO3epq7tbx0dGtLi4oNbWNu3t6dHY6Kjm58+qpWWHevv6dKJU0tzcrJqamnWgv18T4+M6c+a0GhobNTBwUFOTk5qZmZYkHbzukGamp3Xy5JQk6ZprBzI9p7GxUR3o76+rc6r0PD3xxFfUf801dXVOlZ6nxx97TB0dHXV1TpWap5xtaS0ulUq6afBGvXjmRTVvb2YdDpzT1NSEvvDQg3V1TqzDtTVPrMOxc9qIufuGN9oqM7tT0n53v2ud67slfUXS1e6+eKnbDA4e9l//t/9e3zg4WMGR1p9Hh4ZoFkSzOJqV75abD2to6Kjlcd9bXYsHBw/7w48cZb43gWYx9IqjWcyOJhty98PrXV+tp1g8Kelys3aHpPvX2xyv2tvTk+mgioBmcTSLo1kyWItzQrMYesXRLFvV2iB/TtJ2M/vA6gVmdoOZvW3l278t6Xc2OsjY6GhlRlfHaBZHsziaJYO1OCc0i6FXHM2yVZUNsi8/j+N9km4zsxEze1LSPZImzKxP0j5JD210nPn5s5UcZl2iWRzN4miWBtbi/NAshl5xNMtW1d4H2d1f0PKv7y5lbznHqIEXuCSHZnE0i6NZOliL80GzGHrF0SxbSX2SXjmvOsRr0SyOZnE0KxbmO45mMfSKo1m2ktognyiV8h5CcmgWR7M4mhUL8x1Hsxh6xdEsW0ltkOfmZvMeQnJoFkezOJoVC/MdR7MYesXRLFtJbZCbmprzHkJyaBZHsziaFQvzHUezGHrF0SxbSW2QD/T35z2E5NAsjmZxNCsW5juOZjH0iqNZtpLaIE+Mj+c9hOTQLI5mcTQrFuY7jmYx9IqjWbaS2iCfOXM67yEkh2ZxNIujWbEw33E0i6FXHM2yldQGuaGxMe8hJIdmcTSLo1mxMN9xNIuhVxzNspXUBnlg4GDeQ0gOzeJoFkezYmG+42gWQ684mmUrqQ3y1ORk3kNIDs3iaBZHs2JhvuNoFkOvOJplK6kN8szMdN5DSA7N4mgWR7NiYb7jaBZDrziaZSupDTIAAABQaUltkA9edyjvISSHZnE0i6NZsTDfcTSLoVcczbKV1AZ5ZppfH0TRLI5mcTQrFuY7jmYx9IqjWbaS2iCfPDmV9xCSQ7M4msXRrFiY7ziaxdArjmbZSmqDDAAAAFRaUhvka64dyHsIyaFZHM3iaFYszHcczWLoFUezbCW1QZ6bnc17CMmhWRzN4mhWLMx3HM1i6BVHs2wltUGemBjPewjJoVkczeJoVizMdxzNYugVR7NsJbVBBgAAACotqQ3y/gP9eQ8hOTSLo1kczYqF+Y6jWQy94miWraQ2yIsLC3kPITk0i6NZHM2KhfmOo1kMveJolq2kNsil0vN5DyE5NIujWRzNioX5jqNZDL3iaJatpDbIAAAAQKUltUHu7e3LewjJoVkczeJoVizMdxzNYugVR7NsVW2DbGZdZnafmY2Y2ZCZPWBmA2b2BjP7jJk9ZWZfM7O+ao0JAIqGtRgANlaVDbKZmaT7JT3o7v3uPijpg5I6Jf2mpF9x90OSbpK07oeJj42NVmG09YVmcTSLo1kaWIvzQ7MYesXRLFvVegT5VkmL7v7R1Qvc/XFJ05K2uftnVy6bc/eXqzQmACga1mIAKMO2Kt3P9ZKGLnH5gKTTZvYJSfsl/U9JP+PuSxffsFQq6c4f/kE1NDbq3LlFHTlyu+68627Nzc5e+PSY/Qf6tbiwcOGVnKvPx1n9r6qenn1qam7Wc8dHJEldXd1qbWvTs88MS5J2796j9o4OHXv6KUlSe3uH9nR2anj4mM4vLWnnzl3q6u7W8ZERLS4uqLW1TXt7ejQ2Oqr5+bNqadmh3r4+nSiVNDc3q6amZh3o79fE+LjOnDmthsZGDQwc1NTkpGZmpiVJB687pJnpaZ08ufxgzTXXDmR6TvPz85qcnKyrc6r0PE1OTGhqaqquzqnS8/TCiRMaPnasrs6pUvOUsy2txaVSSTcN3qiXXz6rhgZjHQ6cU0vLDn3hoQfr6pxYh2trnliHY+e0EXP3DW+0VWZ2p6T97n7XRZd/p6TfkPRmSX8p6XclPeDuv3HxMQYHD/t9v3+/9u3bV/Hx1pPnn3+eZkE0i6NZ+W65+bCGho5aHve91bV4cPCwP/zIUeZ7E2gWQ684msXsaLIhdz+83vXVeorFk5IGL3F5SdJj7n7c3c9J+qSkb1zvIKv/NYHy0SyOZnE0SwZrcU5oFkOvOJplq1ob5M9J2m5mH1i9wMxukLRd0i4z271y8bskfa1KYwKAomEtBoAyVGWD7MvP43ifpNtW3lroSUn3SHpB0k9I+lMze0KSSfqP6x2nq6u7GsOtKzSLo1kczdLAWpwfmsXQK45m2arWi/Tk7i9IuuMSVz0j6YZyjtHa1pbpmIqAZnE0i6NZOliL80GzGHrF0SxbSX2S3uorGFE+msXRLI5mxcJ8x9Eshl5xNMtWUhtkAAAAoNKS2iDv3r0n7yEkh2ZxNIujWbEw33E0i6FXHM2yldQGub2jI+8hJIdmcTSLo1mxMN9xNIuhVxzNspXUBnn1U1NQPprF0SyOZsXCfMfRLIZecTTLVlIbZAAAAKDSktogt7fz64MomsXRLI5mxcJ8x9Eshl5xNMtWUhvkPZ2deQ8hOTSLo1kczYqF+Y6jWQy94miWraQ2yMPDx/IeQnJoFkezOJoVC/MdR7MYesXRLFtJbZDPLy3lPYTk0CyOZnE0KxbmO45mMfSKo1m2ktog79y5K+8hJIdmcTSLo1mxMN9xNIuhVxzNspXUBrmruzvvISSHZnE0i6NZsTDfcTSLoVcczbKV1Ab5+MhI3kNIDs3iaBZHs2JhvuNoFkOvOJplK6kN8uLiQt5DSA7N4mgWR7NiYb7jaBZDrziaZSupDXJra1veQ0gOzeJoFkezYmG+42gWQ684mmUrqQ3y3p6evIeQHJrF0SyOZsXCfMfRLIZecTTLVlIb5LHR0byHkByaxdEsjmbFwnzH0SyGXnE0y1ZSG+T5+bN5DyE5NIujWRzNioX5jqNZDL3iaJatpDbILS078h5CcmgWR7M4mhUL8x1Hsxh6xdEsW0ltkHv7+vIeQnJoFkezOJoVC/MdR7MYesXRLFtJbZBPlEp5DyE5NIujWRzNioX5jqNZDL3iaJatpDbIc3OzeQ8hOTSLo1kczYqF+Y6jWQy94miWraQ2yE1NzXkPITk0i6NZHM2KhfmOo1kMveJolq2kNsgH+vvzHkJyaBZHsziaFQvzHUezGHrF0SxbSW2QJ8bH8x5CcmgWR7M4mhUL8x1Hsxh6xdEsW1XbIJtZl5ndZ2YjZjZkZg+Y2YCZLZnZYyt/PnW5Y5w5c7paw60bNIujWRzN0sFanA+axdArjmbZ2laNOzEzk3S/pI+7+/tXLrtRUqeks+7+pnKO09DYWLlB1imaxdEsjmZpYC3OD81i6BVHs2xVZYMs6VZJi+7+0dUL3P1xSVper8szMHAw+5HVOZrF1WKzK2/6kbJud+rPf73CI7m0WmyGS2ItzgnNYmqx15Vv+5nXXrD238z5pQtfznzxl9fcpPx/V1tVi81SVq0N8vWShta5rsXMjko6J+mX3P2Tl7pRqVTSbe98m7Y1NencuUUdOXK77rzrbs3NzmpiYvl5N/sP9GtxYUGl0vOSpN7ePknS2NioJKmnZ5+ampv13PERSVJXV7da29r07DPDkqTdu/eovaNDx55+SpLU3t6hPZ2dGh4+pvNLS9q5c5e6urt1fGREi4sLam1t096eHo2Njmp+/qxaWnaot69PJ0olzc3NqqmpWQf6+zUxPq4zZ06robFRAwMHNTU5qZmZaUnSwesOaWZ6WidPTkmSrrl2INNzOn36tG5+y1vr6pwqPU9jo6P6tm8/UlPndP7lk7Km18lfOSP3JVljs6ypVb7wovz8OVnDNlnz6/Xo0FAu8/Tc8eN66zffUvf/nrI4p5xtaS0ulUq6afBGvTT3khq3NbIOB85pampKTz/1tbo6p8Ktwy9NLq+7r5ySnz8v27Zd1ty2vC4vvbK8Dm/fqUcfHdLc7Kyam5t1oP8a1uEa/dnbiLn7hjfaKjO7U9J+d7/rEtftdfcTZnZA0uckfYu7j1x8u8HBw37PvR/S29/xzoqPt5584aEHaRZUtGblPjq9nlN//uuFa7YVt9x8WENDR6v3sNIaW12LBwcP+8OPHGW+N4FmMUXr9ZpHpxfnX/264TJPm1hz3ak/u7dwzbZqR5MNufvh9a6v1ov0npQ0eKkr3P3Eyv8el/SgpDdXaUwAUDSsxQBQhmptkD8nabuZfWD1AjO7wczeZmbbV76/StItkr623kEOXneo4gOtNzSLo1kczZLBWpwTmsXQK45m2arKc5Dd3c3sfZI+YmY/LWle0qikeyUdNbPzWt6s/5K7r7soz0xPq7OzsxpDrhs0iytasyxe2Fe0ZqliLc4PzWKK1uvUF39py8coWrNKq9aL9OTuL0i64xJX/bVyj3Hy5JQO6Y3ZDaoAaBZHsziapYO1OB80i6FXHM2yldQn6QEAAACVVrVHkLNwzbUDeQ8hOTSLufKmH5Evzsma/nvZf6ca7z1c6++DnNXP2VbfUWOr8uqXGtaVOJqV78pvvlu+MCdr/gPJLvM4XkvrhS9P/enPVX5ct/zkay9Y897Hr3lHiYd/peJjuZTM1uF3/uyr38zPvfbKte+qsfZd0Na+3/Pay/18eX//Dd9w4ctTv/sPAqOtnKQeQZ6bnc17CMmhWZwvLeY9hOTwc1YszHcczWJ8aSHvISSHn7FsJbVBXn2TaZSPZnG++FLeQ0gOP2fFwnzH0SyGdTiOn7FsJfUUC6DSavVDL4ryq/+inCeA9Z36Xx+uzXU4p6dOVNupB/9l3kOoCUk9grz/QH/eQ0gOzeJoFkezYmG+42gWQ684mmUrqQ3y4gLPSYqiWRzN4mhWLMx3HM1i6BVHs2wl9RSLUul5Hejnv5AiarFZFu9UUMlfxddis0rKYj6WZktqbOvhKRIFUbR/I1motWZXvvXH179y7TsSrH0Xiabtr7lZJX8VX2u9Ku3Kt/zYq9+sfXeHgAvr8CP/KqNRFVtSjyADAAAAlZbUBrm3ty/vISSHZnE0i7Pm1+c9BFQR/0biaBZDrzjW4Wwl9RQL1Ad+DV9bspiPsdFR9fb1bX0wAKri1Jd+Ne8hYI1TX/7Ilo/BOpytpB5BHhsbzXsIyaFZHM3iaFYszHcczWLoFUezbCW1QQYAAAAqLamnWPT07Mt7CBWXxTsKrHX+lTNq2P7fL3sbnvLwqitv+pGymq1Fv+z+bWb987/WVuepnLEV5Weh3tfiK2/9Z69+8/KZS9+ooXH97xvX/F/ryjsSnJ8/pYbmP1pz+fnX/PVT/+vDmxlqXbryrT+u8/On1dDyqdd2sose09vVdeHLU3/8U1UaXe3KbB3+5rtf/eb80muvXDsHF/0MX9L2173m2+ce+LkLX+96XXNZ43ll8dUxdL39Jy89tm2vHmv6i/e+5u83NJg2I6lHkJuay4uJV9nFCwo2RLM4/m0WC/MdZ9a48Y1wgV38HyDYEP8us2W+yffbq7aGK/b4tr23qLGtJ9Pj1vsjPrX4cZ2beZSwmvNUi81qHc3Kd8vNhzU0dHRzD2nkrKFtr28//INaOvWcGls6Xr3i4v8fWfvIztr31F2z6XnDO9514evHf+E9WQ+15tTav5HXPEooSUvnLn3DNXNWzRf21VqvFNAsZkeTDbn74fWu56EyAAAAYI2kNsjW9LqNb4TX6OrqznsIyaFZHM2Kxba35j2E5PBvJIZecTTLVjIv0nvzoTfo9+6/V1dffXUu97/e0wJq/SkarW1teQ/hr6BZ/aFZMbx5oFsPf/Zn9cILL1RtLV489+oLgTq/52MXvt5/8NWn2w39/LurMpatqLV/I7X+osBa65UCmmUrqUeQn31mOO8hJIdmcTSLo1mxMN9xNIuhVxzNspXUBhkAAACotGSeYiFJu3fvye2+q/W0gMzfB/nsjBp28D7IEXn+nKWKZsVSzflu2vbq4zgz9/39qtznlW/7mVe/WZx/9evLveuTXf6NSc6fnVbDjk++esFFb2NW6095qDbWlDiaZSupR5DbOzo2vhFew7a15D2E5PBzFkezYmG+41iLY/gZi6NZtpLaIB97+qm8h5Cc8/MzeQ8hOfycxdGsWJjvuPPzp/IeQlL4GYujWbaSeopFEWT9dAfeOBwAYk598ZcyPyZrMZCWqj2CbGZdZnafmY2Y2ZCZPWBmAyvXvd7MSmb2by53jPZ2fn0QRbM4msXRLB2sxfmgWQy94miWrao8gmxmJul+SR939/evXHajpE5Jw5L+haQvbHScPZ2dlRxmXaJZHM3iaJYG1uL80CyGXnE0y1a1nmJxq6RFd//o6gXu/rgkmdmglhfnT0ta9zOxJWl4+Jj27OFVmhE0i6vFZuW+u0le70hSi81wSazFQfMLSxe+bl7zjhoNDZd/14qLFalZFmqx15W3/ORrLzj/6s/G2ncxyesdSWqxWcqq9RSL6yUNXXyhmTVI+rCknyjnIOeXlja+EV6DZnE0i6NZMliLc0KzGHrF0Sxbeb9I74ckPeDuJdvgPSRLpZITkDHcAAAgAElEQVR++Ad/QE1NTTp3blFHjtyuO++6W3Ozs5qYGJck7T/Qr8WFBZVKz0uSenv7JEljY6OSpJ6efWpqbtZzx0ckLX9ueWtb24VPn9m9e4/aOzouvBK0vb1Dezo7NTx8TOeXlrRz5y51dXfr+MiIFhcX1Nrapr09PRobHdX8/Fm1tOxQb1+fTpRKmpubVVNTsw7092tifFxnzpxWQ2OjBgYOampyUjMz05Kkg9cd0sz0tE6enJIkXXPtQKbnNDM9rcnJybo6p0rP03PHj+u6Q2+sqXM6//JJWdPr5K+ckfuSrLFZ1tQqX3hRfv6crGGbrPn1enRoKJd5evbZZwvx7ymLc6pRZa3FpVJJNw3eqLm5OW3btq0Q6/DC4pL6Vs5pcvzEhXNqaLDQOZ07t6QvPPRgTZxTCvNUk+vwS1Oy5tfJ508vr8O2TdbcKn/lxeXvG7bJtrMO5z1P5Z7TRswv98bnGTGzb5H0c+7+9osu/6+S3ibpvKRWSc2S/q27/8zFxxgcPOyf+NQfqZPn2IRMTk7SLKhozbL4cBpffFnWdAUfOlOGW24+rKGho7Hfz2dkq2vx4OBhf/iRo4X7N5IFmsUUrdeVb/mxV7/ZzAfSNLXIF16SNb+uIu/CUo92NNmQu6/7dLJqPcXic5K2m9kHVi8wsxskfdTd3+DufVr+1d5vXmpzvOr4yEjFB1pvaBZHszh/5UzeQ0B5WItzQrMYesX5K7zXdpaqskH25Yep3yfptpW3FnpS0j2SJiLHWVxcqMTw6hrN4mgW585z31LAWpwfmsXQK87Pn897CHWlas9BdvcXJN1xmes/JuljlztGa2tbtoMqAJrFFa1ZFk+LeHRoSN84OJjBaFBprMX5oFlM0Xqd+vJHtnwM1uFsJfVR03t7evIeQnJoFkezOJoVC/MdR7MYesXRLFt5v4tFyNjoaKGetJ8FmsVcedOP6PxLE2p4Xddlb8eL0V6rEj9nWbx4cD3M39awrsTRrHxXvuXHXl2HL/OCtekvvfqoa/R9qetRRdbhm380/pca12wtG5vWvVmtv5gwqUeQ5+fP5j2E5NAszs+fy3sIyeHnrFiY7ziaxbAOx/Ezlq2kNsg1/B6iNYtmcdaQ1C9WagI/Z8XCfMfRLIZ1OI6fsWwl9RNYzhs747VoFnPqz3+9cO+/mYVK/JzxNIjaxboSR7PynfryR1iHN6ESP2Nrn8YiFeupLEk9gnyiVMp7CMmhWRzN4mhWLMx3HM1i6BVHs2wltUGem5vNewjJoVkczeJoVizMdxzNYugVR7NsJfUUi6am5ryHkJxabLaZdyeo5q/ba7FZratEs1r/OSky/o3E1Vqz13y0sbT+u0Ws+WjjLN6rt1y11isFlWjW8Y4PvvaCc2s+wGXtx163XXXhy9H7f+LC1zuvWP9dLGpdUo8gH+jvz3sIyaFZHM3iaFYszHcczWLoFUezbCW1QZ4YH897CMmhWRzN4mhWLMx3HM1i6BVHs2yV9RQLM/tbkn5Z0h5JtvLH3f31FRzbX3HmzOlq3l1dqMVmtf5r8FpsVusq0azWf07ywFqcrlprVs2nS2xGrfVKQUXW4Rr/MI9KKvc5yPdKeq+7P1XJwWykobExz7tPEs3iaBZHs6phLU4UzWLoFUezbJX7FIvJvBdkSRoYOJj3EJJDsziaxdGsaliLE0WzGHrF0Sxbl30EeeXXeZJ01Mx+V9InJb2yer27f6KCY/srpiYntWfPnmreZdVt5pX7l3P+7NfVsOOq11zGr64vrwg/Z1mjWWWxFlfXlTf/6NYOsPbV/bb8ONT5l7+uhtZXP/ji1MO/srX7qHP1/jNWCTTL1kZPsXjvmq9flvTuNd+7pKouyjMz09W8u7rg5+bzHkJy+DmLo1nFsRYnzs+dzXsISeFnLI5m2brsBtnd/161BgIAuDTWYgCoLvP13hx87Y3MPi7pR9399Mr3V0r6sLv//QqP74KGK/Z4c/97ZU1XVOsuXyPVpyXwefZxNIujWfluufmwhoaO2sa3/KvyXosbWrt8+/XfJ198SWZlfgBAGf8f81dc4ikKknTqS78aP1aN4N9IDL3iaBazo8mG3P3weteX+yK9G1YXZEly91OS3rzVwUXxdIG4mWl+5RJFsziaVU1trMWLPF0gin8jMfSKo1m2yt0gN6w8UiFJMrN25fAx1X7u5WrfZfJOnpzKewjJoVkczaqGtThR/BuJoVcczbJV7sL6YUlfMrPfW/n+uyT9YmWGdGlvPvQG3XPvj+rt73hnNe8WAGpJrmvxmw/26OE/u1dfeOhB1mIAda2sDbK7/6aZHZX0rpWL/pa7f61yw7q0a64dqPZdJo9mcTSLo1l1sBani2Yx9IqjWbbK/ajp33L375X0tUtcVjVzs7PVvLu6QLM4msXRrDpYi9NFsxh6xdEsW+U+B/kb1n5jZo2SBrMfzuVNTIxX+y6TR7M4msXRrGpYixNFsxh6xdEsW5fdIJvZB81sVtINZvaimc2ufD8l6X9UZYQAUHCsxQBQXZfdILv7Pe7eJulX3P317t628qfD3T9YpTFesP9Af7XvMnk0i6NZHM0qi7U4fTSLoVcczbJ12ecgm9l17v60pN8zs2+8+Hp3f7RiI7uExYWFat5dXaBZHM3iaFZZrMXpo1kMveJolq2NnoP84yv/+2FJH1rzZ/X7splZl5ndZ2YjZjZkZg+Y2TvM7FEze8zMnjSzf3i5Y5RKz0fuEqLZZtAsjmYVx1qcOJrF0CuOZtm67CPI7v6BlS+PSPohSX9dkkv6oqR/V+6dmJlJul/Sx939/SuX3Shpl6S3uvsrZtYq6atm9il3fyF8JgBQp1iLAaC6yv2gkI9LelHSv175/u9I+k1Jd5T592+VtOjuH129wN0fv+g227XBI9q9vX1l3h1W0SyOZnE0qxrW4kTRLIZecTTLVrkb5Ovd/Y1rvv+8mUXenP56SUOXusLM9kn6I0nXSPrJ9R6xKJVK+pvv/XY1NDTq3LlFHTlyu+68627Nzc5eeGuT/Qf6tbiwcOHXDKs/LGNjo5Kknp59ampu1nPHRyRJXV3dam1r07PPDEuSdu/eo/aODh17+ilJUnt7h/Z0dmp4+JjOLy1p585d6uru1vGRES0uLqi1tU17e3o0Njqq+fmzamnZod6+Pp0olTQ3N6umpmYd6O/XxPi4zpw5rYbGRg0MHNTU5KRmZpY/M/3gdYc0Mz194SMir7l2INNzWlxYVMuOHXV1TpWep5NTU9pxxRV1dU6VnqeJiQm98sordXVOlZqnLcp1LS6VSrpp8EadffmsrMFYhwPn9PLLL124fb2cE+twbc0T63DsnDZi7r7xjcx+W9K/cfcvr3x/s6Qfdvfv2/AvL9/+Tkn73f2uy9zmakmflPRed5+8+PrBwcN+z70f4uNNg/hI2DiaxdGsfLfcfFhDQ0dtM38377V4cPCwP/zIUeZ7E2gWQ684msXsaLIhdz+83vXlflDIoKT/ZWajZjYq6UuSvsnMnjCzr5Tx95/UBm9mv/JoxVclva3MMQFA0bAWA0AVlPsUi/ds8X4+J+kXzewD7v4fJMnMbpC0U9JRdz9rZldq+YUnv7beQXp69m1xGMVDsziaxdGsaliLE0WzGHrF0SxbZW2Q3X1sK3fi7m5m75P0ETP7aUnzkka1/Gu8/9fMXJJJ+pC7P7HecZqam7cyjEKiWRzN4mhWHazF6aJZDL3iaJatch9B3rKVX9td6pXW/7HcYzx3fET79vFfSBE0i6NZHM3SwVqcD5rF0CuOZtkq9znIAAAAQCEktUHu6urOewjJoVkczeJoVizMdxzNYugVR7NsJbVBbm1ry3sIyaFZHM3iaFYszHcczWLoFUezbCW1QV59k2iUj2ZxNIujWbEw33E0i6FXHM2yldQGGQAAAKi0pDbIu3fvyXsIyaFZHM3iaFYszHcczWLoFUezbCW1QW7v6Mh7CMmhWRzN4mhWLMx3HM1i6BVHs2wltUE+9vRTeQ8hOTSLo1kczYqF+Y6jWQy94miWraQ2yAAAAEClJbVBbm/n1wdRNIujWRzNioX5jqNZDL3iaJatpDbIezo78x5CcmgWR7M4mhUL8x1Hsxh6xdEsW0ltkIeHj+U9hOTQLI5mcTQrFuY7jmYx9IqjWbaS2iCfX1rKewjJoVkczeJoVizMdxzNYugVR7NsJbVB3rlzV95DSA7N4mgWR7NiYb7jaBZDrziaZSupDXJXd3feQ0gOzeJoFkezYmG+42gWQ684mmUrqQ3y8ZGRvIeQHJrF0SyOZsXCfMfRLIZecTTLVlIb5MXFhbyHkByaxdEsjmbFwnzH0SyGXnE0y1ZSG+TW1ra8h5AcmsXRLI5mxcJ8x9Eshl5xNMtWUhvkvT09eQ8hOTSLo1kczYqF+Y6jWQy94miWraQ2yGOjo3kPITk0i6NZHM2KhfmOo1kMveJolq2kNsjz82fzHkJyaBZHsziaFQvzHUezGHrF0SxbSW2QW1p25D2E5NAsjmZxNCsW5juOZjH0iqNZtpLaIPf29eU9hOTQLI5mcTQrFuY7jmYx9IqjWbaS2iCfKJXyHkJyaBZHsziaFQvzHUezGHrF0SxbSW2Q5+Zm8x5CcmgWR7M4mhUL8x1Hsxh6xdEsW1XbIJtZl5ndZ2YjZjZkZg+Y2U1m9iUze9LMvmJm3325YzQ1NVdruHWDZnE0i6NZOliL80GzGHrF0Sxb26pxJ2Zmku6X9HF3f//KZTdK2iXp+9z9GTO7WtKQmf2Ju5++1HEO9PdXY7h1hWZxNIujWRpYi/NDsxh6xdEsW9V6BPlWSYvu/tHVC9z9cXd/yN2fWfn+BUlTknavd5CJ8fGKD7Te0CyOZnE0SwZrcU5oFkOvOJplqyqPIEu6XtLQ5W5gZjdJapY0cqnrS6WS/s53f6e2t7To3LlFHTlyu+68627Nzc5qYmL5h2L/gX4tLiyoVHpektTb2ydJGhsblST19OxTU3Oznju+fBddXd1qbWvTs88MS5J2796j9o4OHXv6KUlSe3uH9nR2anj4mM4vLWnnzl3q6u7W8ZERLS4uqLW1TXt7ejQ2Oqr5+bNqadmh3r4+nSiVNDc3q6amZh3o79fE+LjOnDmthsZGDQwc1NTkpGZmpiVJB687pJnpaZ08OSVJuubagUzPaWxsVF3d3XV1TpWepyee+Iq6r766rs6p0vP0+GOPaceOHXV1TpWap5xtaS0ulUq6afBGvXjmRTVvb2YdDpzT6OhxnTlzuq7OiXW4tuaJdTh2Thsxd9/wRltlZndK2u/ud61zfbekByX9XXf/8qVuMzh42H/5w7+mv/7X31a5gdahP/uzL9IsiGZxNCvfLTcf1tDQUcvjvre6Fg8OHvaHHznKfG8CzWLoFUezmB1NNuTuh9e7vlpPsXhS0uClrjCz10v6I0n/ZL3N8aqBgYMVGFp9o1kczeJolgzW4pzQLIZecTTLVrU2yJ+TtN3MPrB6gZndYGbv0PILRn7T3X9/o4NMTU5WcIj1iWZxNIujWTJYi3NCsxh6xdEsW1XZIPvy8zjeJ+m2lbcWelLSPZLevvLn+83ssZU/b1rvOKvPb0H5aBZHsziapYG1OD80i6FXHM2yVa0X6a2+MvqOS1z1L6o1BgAoOtZiANhYUp+kd/C6Q3kPITk0i6NZHM2KhfmOo1kMveJolq2kNsgz0/z6IIpmcTSLo1mxMN9xNIuhVxzNspXUBnn1PfVQPprF0SyOZsXCfMfRLIZecTTLVlIbZAAAAKDSktogX3PtQN5DSA7N4mgWR7NiYb7jaBZDrziaZSupDfLc7GzeQ0gOzeJoFkezYmG+42gWQ684mmUrqQ3y6ud4o3w0i6NZHM2KhfmOo1kMveJolq2kNsgAAABApSW1Qd5/oD/vISSHZnE0i6NZsTDfcTSLoVcczbKV1AZ5cWEh7yEkh2ZxNIujWbEw33E0i6FXHM2yldQGuVR6Pu8hJIdmcTSLo1mxMN9xNIuhVxzNspXUBhkAAACotKQ2yL29fXkPITk0i6NZHM2KhfmOo1kMveJolq2kNsgAAABApSW1QR4bG817CMmhWRzN4mhWLMx3HM1i6BVHs2wltUEGAAAAKi2pDXJPz768h5AcmsXRLI5mxcJ8x9Eshl5xNMtWUhvkpubmvIeQHJrF0SyOZsXCfMfRLIZecTTLVlIb5OeOj+Q9hOTQLI5mcTQrFuY7jmYx9IqjWbaS2iADAAAAlZbUBrmrqzvvISSHZnE0i6NZsTDfcTSLoVcczbKV1Aa5ta0t7yEkh2ZxNIujWbEw33E0i6FXHM2yldQG+dlnhvMeQnJoFkezOJoVC/MdR7MYesXRLFtJbZABAACASktqg7x79568h5AcmsXRLI5mxcJ8x9Eshl5xNMtW1TbIZtZlZveZ2YiZDZnZA2Y2YGafNrPTZvaHGx2jvaOjGkOtKzSLo1kczdLBWpwPmsXQK45m2arKBtnMTNL9kh509353H5T0QUmdkn5F0veWc5xjTz9VuUHWKZrF0SyOZmlgLc4PzWLoFUezbG2r0v3cKmnR3T+6eoG7P776tZm9s0rjAIAiYy0GgDJUa4N8vaShrRygVCrpH/3gD2hbU5POnVvUkSO368677tbc7KwmJsYlSfsP9GtxYUGl0vOSpN7ePknS2NiopOXPKW9qbr7waTNdXd1qbWu78MrP3bv3qL2j48J/hbW3d2hPZ6eGh4/p/NKSdu7cpa7ubh0fGdHi4oJaW9u0t6dHY6Ojmp8/q5aWHert69OJUklzc7NqamrWgf5+TYyP68yZ02pobNTAwEFNTU5qZmZaknTwukOamZ7WyZNTkqRrrh3I9JxOnz6tycnJujqnSs/T2Oiopg5N1dU5VXqenjt+vO7OqVLzlLMtrcWlUkk3Dd6ol+ZeUuO2RtbhwDm5S1946MG6OifW4dqaJ9bh2DltxNx9wxttlZndKWm/u9+1zvXvlPQT7n77escYHDzs9//BA9qzhyehR0xNTdEsiGZxNCvfLTcf1tDQUcvjvre6Fg8OHvaHHznKfG8CzWLoFUezmB1NNuTuh9e7vlov0ntS0uBWDzI8fCyDoRQLzeJoFkezZLAW54RmMfSKo1m2qrVB/pyk7Wb2gdULzOwGM3tb5CDnl5YyH1i9o1kczeJolgzW4pzQLIZecTTLVlU2yL78PI73Sbpt5a2FnpR0j6QJM/uipN+T9C1mVjKzb1vvODt37qrGcOsKzeJoFkezNLAW54dmMfSKo1m2qvUiPbn7C5LuuMRVZT9y0dXdnd2ACoJmcTSLo1k6WIvzQbMYesXRLFtJfZLe8ZGRvIeQHJrF0SyOZsXCfMfRLIZecTTLVlIb5MXFhbyHkByaxdEsjmbFwnzH0SyGXnE0y1ZSG+TW1ra8h5AcmsXRLI5mxcJ8x9Eshl5xNMtWUhvkvT09eQ8hOTSLo1kczYqF+Y6jWQy94miWraQ2yGOjo3kPITk0i6NZHM2KhfmOo1kMveJolq2kNsjz82fzHkJyaBZHsziaFQvzHUezGHrF0SxbSW2QW1p25D2E5NAsjmZxNCsW5juOZjH0iqNZtpLaIPf29eU9hOTQLI5mcTQrFuY7jmYx9IqjWbaS2iCfKJXyHkJyaBZHsziaFQvzHUezGHrF0SxbSW2Q5+Zm8x5CcmgWR7M4mhUL8x1Hsxh6xdEsW0ltkJuamvMeQnJoFkezOJoVC/MdR7MYesXRLFtJbZAP9PfnPYTk0CyOZnE0KxbmO45mMfSKo1m2ktogT4yP5z2E5NAsjmZxNCsW5juOZjH0iqNZtpLaIJ85czrvISSHZnE0i6NZsTDfcTSLoVcczbKV1Aa5obEx7yEkh2ZxNIujWbEw33E0i6FXHM2yldQGeWDgYN5DSA7N4mgWR7NiYb7jaBZDrziaZSupDfLU5GTeQ0gOzeJoFkezYmG+42gWQ684mmUrqQ3yzMx03kNIDs3iaBZHs2JhvuNoFkOvOJplK6kNMgAAAFBpSW2QD153KO8hJIdmcTSLo1mxMN9xNIuhVxzNspXUBnlmml8fRNEsjmZxNCsW5juOZjH0iqNZtpLaIJ88OZX3EJJDsziaxdGsWJjvOJrF0CuOZtlKaoMMAAAAVFpSG+Rrrh3IewjJoVkczeJoVizMdxzNYugVR7NsJbVBnpudzXsIyaFZHM3iaFYszHcczWLoFUezbFVtg2xmXWZ2n5mNmNmQmT1gZgNm9nfN7JmVP3/3cseYmBiv1nDrBs3iaBZHs3SwFueDZjH0iqNZtrZV407MzCTdL+nj7v7+lctulNQt6eckHZbkkobM7FPufqoa4wKAImEtBoDyVOsR5FslLbr7R1cvcPfHJV0t6bPuPrOyEH9W0nvWO8j+A/0VH2i9oVkczeJolgzW4pzQLIZecTTLVrU2yNdLGrrE5XslPb/m+9LKZZe0uLCQ8bDqH83iaBZHs2SwFueEZjH0iqNZtqryFIsslEolvffIt2l7S4vOnVvUkSO368677tbc7OyF593sP9CvxYUFlUrL63xvb58kaWxsVJLU07NPTc3Neu74iCSpq6tbrW1tevaZYUnS7t171N7RoWNPPyVJam/v0J7OTg0PH9P5pSXt3LlLXd3dOj4yosXFBbW2tmlvT4/GRkc1P39WLS071NvXpxOlkubmZtXU1KwD/f2aGB/XmTOn1dDYqIGBg5qanLzwmekHrzukmenpC+9feM21A5me09jYqF7X2lpX51TpeXriia/ou+54f12dU6Xn6fHHHtO3vefb6+qcKjVPKSuVSrpp8Ea9eOZFNW9vZh0OnNPw8NMX/n69nBPrcG3NE+tw7Jw2Yu6+4Y22ysy+RdLPufvbL7r8b0t6p7v/wMr3/17Sg+7+OxcfY3DwsN9z74f09ne8s+LjrSdfeOhBmgXRLI5m5bvl5sMaGjpqedz3VtfiwcHD/vAjR5nvTaBZDL3iaBazo8mG3P3wetdX6ykWn5O03cw+sHqBmd0g6QVJ7zazK83sSknvlvQn6x1k9b8yUD6axdEsjmbJYC3OCc1i6BVHs2xVZYPsyw9Tv0/SbStvLfSkpHu0vCj/C0l/sfLnn7v7TDXGBABFw1oMAOWp2vsgu/sL7n6Hu/e7+ze4+3e4+zPu/p/d/ZqVP//lcsdYfX4KykezOJrF0SwdrMX5oFkMveJolq2kPkkPAAAAqLSkNsg9PfvyHkJyaBZHsziaFQvzHUezGHrF0SxbSW2Qm5qb8x5CcmgWR7M4mhUL8x1Hsxh6xdEsW0ltkFffEw/lo1kczeJoVizMdxzNYugVR7NsJbVBBgAAACotqQ1yV1d33kNIDs3iaBZHs2JhvuNoFkOvOJplK6kNcmtbW95DSA7N4mgWR7NiYb7jaBZDrziaZSupDfLq53CjfDSLo1kczYqF+Y6jWQy94miWraQ2yAAAAEClJbVB3r17T95DSA7N4mgWR7NiYb7jaBZDrziaZSupDXJ7R0feQ0gOzeJoFkezYmG+42gWQ684mmUrqQ3ysaefynsIyaFZHM3iaFYszHcczWLoFUezbCW1QQYAAAAqLakNcns7vz6IolkczeJoVizMdxzNYugVR7NsJbVB3tPZmfcQkkOzOJrF0axYmO84msXQK45m2Upqgzw8fCzvISSHZnE0i6NZsTDfcTSLoVcczbKV1Ab5/NJS3kNIDs3iaBZHs2JhvuNoFkOvOJplK6kN8s6du/IeQnJoFkezOJoVC/MdR7MYesXRLFtJbZC7urvzHkJyaBZHsziaFQvzHUezGHrF0SxbSW2Qj4+M5D2E5NAsjmZxNCsW5juOZjH0iqNZtpLaIC8uLuQ9hOTQLI5mcTQrFuY7jmYx9IqjWbaS2iC3trblPYTk0CyOZnE0KxbmO45mMfSKo1m2ktog7+3pyXsIyaFZHM3iaFYszHcczWLoFUezbCW1QR4bHc17CMmhWRzN4mhWLMx3HM1i6BVHs2wltUGenz+b9xCSQ7M4msXRrFiY7ziaxdArjmbZSmqD3NKyI+8hJIdmcTSLo1mxMN9xNIuhVxzNslXRDbKZdZnZfWY2YmZDZvaAmQ1cdJu3m9mjZnbOzL7zcsfr7eur5HDrEs3iaBZHs9qV9TosMd+bQbMYesXRLFsV2yCbmUm6X9KD7t7v7oOSPiip86Kb/qWk75f0/210zBOlUtbDrHs0i6NZHM1qUyXWYYn53gyaxdArjmbZ2lbBY98qadHdP7p6gbs/fvGN3H1Ukszs/EYHnJubzXJ8hUCzOJrF0axmZb4OS8z3ZtAshl5xNMtWJTfI10sayupgpVJJP/gD/7e2b9+uc+cWdeTI7brzrrs1NzuriYlxSdL+A/1aXFhQqfS8JKm3t0+SNDY2Kknq6dmnpuZmPXd8+dNmurq61drWpmefGZYk7d69R+0dHTr29FOSpPb2Du3p7NTw8DGdX1rSzp271NXdreMjI1pcXFBra5v29vRobHRU8/Nn1dKyQ719fTpRKmlublZNTc060N+vifFxnTlzWg2NjRoYOKipyUnNzExLkg5ed0gz09M6eXJKknTNtQOZntOJEyc0OTlZV+dU6Xl66mtf03WH3lhX51TpeXryq1+tu3Oq1DxVWebr8E2DN+rFF2fV3NzEOhw4p9nZOX3hoQfr6pxYh2trnliHY+e0EXP3DW+0GWZ2p6T97n5Xmbf/mKQ/dPffv9T1g4OH/ROf+iN1dl78m0FczuTkJM2CaBZHs/LdcvNhDQ0dtWrcVyXW4YcfOcp8bwLNYugVR7OYHU025O6H17u+ki/Se1LS4MUXmtkvmNljZvZY9IAT4+OZDKxIaBZHszia1azM12GJ+d4MmsXQK45m2arkBvlzkrab2QdWLzCzGyR92t3f5O5vih7wzJnTWY6vEGgWR7M4mtWszNdhifneDJrF0CuOZtmq2AbZl5+78T5Jt628vdCTku6RNCt1kqYAAAq/SURBVLH2dmb2TWZWkvRdkv79yu0uPdjGxkoNt27RLI5mcTSrTZVYhyXmezNoFkOvOJplq5Iv0pO7vyDpjg1u8xeSyvoA8YGBg1kMq1BoFkezOJrVrqzXYYn53gyaxdArjmbZSuqT9KYmJ/MeQnJoFkezOJoVC/MdR7MYesXRLFtJbZBX30IE5aNZHM3iaFYszHcczWLoFUezbCW1QQYAAAAqLakN8sHrDuU9hOTQLI5mcTQrFuY7jmYx9IqjWbaS2iDPTPPrgyiaxdEsjmbFwnzH0SyGXnE0y1ZSG+TVjy1E+WgWR7M4mhUL8x1Hsxh6xdEsW0ltkAEAAIBKS2qDfM21A3kPITk0i6NZHM2KhfmOo1kMveJolq2kNshzs7N5DyE5NIujWRzNioX5jqNZDL3iaJatpDbIExPjeQ8hOTSLo1kczYqF+Y6jWQy94miWraQ2yAAAAEClJbVB3n+gP+8hJIdmcTSLo1mxMN9xNIuhVxzNspXUBnlxYSHvISSHZnE0i6NZsTDfcTSLoVcczbKV1Aa5VHo+7yEkh2ZxNIujWbEw33E0i6FXHM2yldQGGQAAAKi0pDbIvb19eQ8hOTSLo1kczYqF+Y6jWQy94miWraQ2yAAAAEClJbVBHhsbzXsIyaFZHM3iaFYszHcczWLoFUezbCW1QQYAAAAqLakNck/PvryHkByaxdEsjmbFwnzH0SyGXnE0y1ZSG+Sm5ua8h5AcmsXRLI5mxcJ8x9Eshl5xNMtWUhvk546P5D2E5NAsjmZxNCsW5juOZjH0iqNZtpLaIAMAAACVltQGuaurO+8hJIdmcTSLo1mxMN9xNIuhVxzNspXUBrm1rS3vISSHZnE0i6NZsTDfcTSLoVcczbJV0Q2ymXWZ2X1mNmJmQ2b2gJkNXHSbHzezr5nZV8zsT82sd73jPfvMcCWHW5doFkezOJrVrqzXYYn53gyaxdArjmbZqtgG2cxM0v2SHnT3fncflPRBSZ0X3fR/Szrs7jdI+n1J9653zAf+8A8qNdy6RbM4msXRrDZVYh2WmO/NoFkMveJolq1KPoJ8q6RFd//o6gXu/ri7f3Htjdz98+7+8sq3X5bUs94BP/OZP6nIQOsZzeJoFkezmpX5Oiwx35tBsxh6xdEsW9sqeOzrJQ0F/84/kPTHl7qiVCrpxRfP6BtvvF7nzi3qyJHbdeddd2tudlYTE+OSpP0H+rW4sKBS6XlJUm9vn6RXP36xp2efmpqbL7wVSldXt1rb2i78WmL37j1q7+jQsaefkiS1t3doT2enhoeP6fzSknbu3KWu7m4dHxnR4uKCWlvbtLenR2Ojo5qfP6uWlh3q7evTiVJJc3Ozampq1oH+fk2Mj+vMmdNqaGzUwMBBTU1OamZmWpJ08LpDmpme1smTU5Kka64dyPSczp1b1OTkZF2dU6Xn6ZVX5jU1NVVX51TpeZqfP6vhY8fq6pwqNU9Vlvk6fNPgjTo+8qxu+IaDrMOBc3I/ry889GBdnRPrcG3NE+tw7Jw2Yu6+4Y02w8zulLTf3e8q8/bfI+kfSXqHu79yietPSnpJ0tczHWj9u0o0i6JZHM3K1+vuu6txRxVah8fEfG8GzWLoFUezmMuuxZV8BPlJSd958YVm9guSvkOS3P1NK5fdJumfaJ1FeeW2Vfk/FACoI6zDALAJlXwE2bT8XLbfcPf/sHLZDZJ2rn3+m5m9WcsvCnmPuz9TkcEAQAGxDgPA5lRsgyxJZna1pI9IGpQ0L2lU0o+tXYDN7H9K+muSxlcu+kt3/xsVGxQAFAjrMADEVXSDDAAAAKQmqU/SAwAAACqNDTIAAACwBhvkLTKz/2xmU2b21bzHkgIz22dmn1/5WNsnzexH8x5TrTOzFjP7czN7fKXZz+c9phSYWaOZ/W8z+8O8x4LKYh2OYy2OYy3enFTXYjbIW/cxSe/JexAJOSfpbnd/o6S3SPphM3tjzmOqda9Iepe73yjpTZLeY2ZvyXlMKfhRSU/lPQhUxcfEOhzFWhzHWrw5Sa7FbJC3yN2/IGkm73Gkwt3H3f3Rla9ntfyPZm++o6ptvmxu5dumlT+8uvYyzKxHy+/z+5/yHgsqj3U4jrU4jrU4LuW1mA0ycmNmfZLeLOmRfEdS+1Z+RfWYpClJn3V3ml3eRyT9lKTzeQ8EqHWsxeVjLQ5Ldi1mg4xcmFmrpP+u5fdjfTHv8dQ6d19a+cSzHkk3mdn1eY+pVpnZ7ZKm3H0o77EAtY61OIa1uHypr8VskFF1Ztak5QX5v7r7J/IeT0rc/bSkz4vnW17OLZL+hpmNSrpP0rvM7LfzHRJQe1iLN4+1uCxJr8VskFFVKx99+xuSnnL3X817PCkws91mtmvl6x2SvlXS0/mOqna5+wfdvcfd+yS9X9Ln3P17ch4WUFNYi+NYi2NSX4vZIG+Rmf2OpC9JOmhmJTP7B3mPqcbdIul7tfxfko+t/DmS96BqXLekz5vZVyT9hZaf95bU2+UAlcQ6vCmsxXGsxQXCR00DAAAAa/AIMgAAALAGG2QAAABgDTbIAAAAwBpskAEAAIA12CADAAAAa7BBRs0ys11m9kNrvr/azH6/Qvf1f5jZP1v5+mNm9p0ZHPPi8e82s09v9bgAUE2sxSgiNsioZbskXVjU3P0Fd9/yYrmOn5L0bzM+5sXjPylp3Mxuyfh+AKCSWItROGyQUct+SVL/yhvY/4qZ9ZnZVyXJzL7fzD5pZp81s1Ez+0dm9uNm9r/N7Mtm1r5yu34z+7SZDZnZF83suovvxMwGJL3i7l9fc/FtZnbUzIZXPk9eZta4Mo6/MLOvmNkPrFzeamZ/amaPmtkTZvY3LzX+lcs+Ken/rEgtAKgM1mIUzra8BwBcxs9Iut7d3yRJZtZ30fXXS3qzpBZJz0r6aXd/s5n9mqTvk/QRSf9B0j9092fM7GYtPzLxrouOc4ukRy+6rE/STZL6tfzJSdesHPOMu3+TmW2X9LCZfUbS85Le5+4vmtlVkr5sZp+6ePwrjkr6l5uqAQD5YC1G4bBBRso+7+6zkmbN7IykP1i5/AlJN5hZq6RvlvR7Zrb6d7Zf4jjdkk5edNl/c/fzkp4xs+OSrpP07pXjrv5qcaekayWVJP2imb1d0nlJeyV1rjPmKUlXx04TAGoaazHqDhtkpOyVNV+fX/P9eS3/bDdIOn3RowaXclbLC+xaF38Gu0syST/i7n/y/7dv/ypxBVEcx78/QRDF0lpQsQ2SvIBPYGVlI1jlRVIKloJtniAECVhZphDE/5ViGRAhaQSLk+JuMcgqZGF1434/5b1z78w0h8OZM+2LJJvAHPCxqh6T3NBVUvqZ6s0nSe+FsVjvjj3IGmV/gNlBP66q38B1knWAdD70GXoBLD15tp5kIskisABcAT+Az0kme/9bTjJDF9B/9QLyKjD/wvqXgdNB9yRJb8BYrLFjgqyRVVV3dL1lp83Fin+1AWwlOQbOgLU+Yw6BlTRnf8At8BPYp+ubewD2gHPgqHdBZZeuOvIV+JTkhK437vKF9a8C3wfciyS9OmOxxlGqnp5eSOMnyQ7wraoOhjzPIbBWVffDnEeS/kfGYo0KK8hS5wswPcwJkswB2wZkSXqWsVgjwQqyJEmS1LCCLEmSJDVMkCVJkqSGCbIkSZLUMEGWJEmSGibIkiRJUuMvMAL6SmWJJtUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect a random input-output sample\n",
    "sample_index = np.random.randint(len(input_test))\n",
    "sample_input = input_test[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "sample_output = decoded_test[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "\n",
    "print(sample_input.shape)\n",
    "print(sample_output.shape)\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(1,2)\n",
    "fig.set_size_inches(10, 6, forward=True)\n",
    "ax[0].set_title('Input')\n",
    "ax[1].set_title('Output')\n",
    "pypianoroll.plot_pianoroll(ax[0], sample_input, beat_resolution=24)\n",
    "pypianoroll.plot_pianoroll(ax[1], sample_output, beat_resolution=24)\n",
    "fig.tight_layout()\n",
    "\n",
    "# Play comparison\n",
    "pianoroll_utils.playPianoroll(sample_input)\n",
    "pianoroll_utils.playPianoroll(sample_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
