{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "The ultimate goal of the model is to learn a __latent variable space of musical units__. Then, given a musical unit, we wish to encode that unit into a latent vector within the space, and predict the best accompaninment latent vector to that input. Finally, that accompaniment latent vector can be decoded to produce an accompanying musical unit.\n",
    "\n",
    "This involves many tricky steps, so development will be approached incrementally:\n",
    "\n",
    "#### 1. Convolutional Autoencoder\n",
    "\n",
    "Given an input unit of `[num_ticks, num_pitches]`, learn a Convolutional Autoencoder model to generate an encoding of that unit.\n",
    "\n",
    "```\n",
    "INPUT -> Convolution layers -> EMBEDDING -> Deconvolution layers -> INPUT\n",
    "```\n",
    "\n",
    "Autoencoding: To test this convolutional autoencoder, generate a response to a given input unit using \n",
    "- Decoder reconstruction of same input\n",
    "- Nearest-neighbor unit selection (Similar to what Bretan et al did)\n",
    "\n",
    "De-noising: Test de-noising abilities of the autoencoder. Given a partial accompaniment input unit, generate a response of\n",
    "- Decoder reconstruction of \"full\"/\"comp\" unit\n",
    "- Nearest-neighbor unit selection\n",
    "\n",
    "#### 2. LSTM of latent variables -> Generation using unit selection\n",
    "\n",
    "Given a sequence of embeddings (from the convolutional autoencoder), predict the next embedding - and perform NN-unit-selection as before, to generate the next unit in the sequence.\n",
    "\n",
    "#### 3. Convolutional Variational Autoencoder\n",
    "\n",
    "Learn a new latent space using a VAE architecture. Test how well resconstruction works using\n",
    "- Decoder reconstruction\n",
    "\n",
    "#### 4. LSTM of variational latent variables -> Generation using latent space sampling \n",
    "\n",
    "Given a sequence of embeddings (from the VAE), predict the next embedding and generate an output musical unit by decoding the predicted embedding!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import pypianoroll\n",
    "from matplotlib import pyplot as plt\n",
    "import cPickle as pickle\n",
    "import pianoroll_utils\n",
    "\n",
    "PICKLE_FILE = './pickle_jar/units_50_songs.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3276 units from ./pickle_jar/units_50_songs.pkl\n",
      "full_units.shape:  (3276, 96, 128)\n",
      "input_units.shape:  (3276, 96, 128)\n",
      "input_units_next.shape:  (3276, 96, 128)\n",
      "comp_units.shape:  (3276, 96, 128)\n",
      "comp_units_next.shape:  (3276, 96, 128)\n"
     ]
    }
   ],
   "source": [
    "units = {}\n",
    "with open(PICKLE_FILE, 'rb') as infile:\n",
    "    units = pickle.load( infile )\n",
    "\n",
    "units[\"full\"] = units[\"input\"] + units[\"comp\"]\n",
    "\n",
    "# Print info\n",
    "print \"Loaded\", units[\"input\"].shape[0], \"units from\", PICKLE_FILE\n",
    "print \"full_units.shape: \", units[\"full\"].shape\n",
    "print \"input_units.shape: \", units[\"input\"].shape\n",
    "print \"input_units_next.shape: \", units[\"input_next\"].shape\n",
    "print \"comp_units.shape: \", units[\"comp\"].shape\n",
    "print \"comp_units_next.shape: \", units[\"comp_next\"].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convolutional Autoencoder\n",
    "\n",
    "Given an input unit of `[num_ticks, num_pitches]`, learn a Convolutional Autoencoder model to generate an encoding of that unit.\n",
    "\n",
    "```\n",
    "INPUT -> Convolution layers -> EMBEDDING -> Deconvolution layers -> INPUT\n",
    "```\n",
    "\n",
    "### Testing\n",
    "\n",
    "We will evaluate the autoencoder using two measures:\n",
    "\n",
    "1. __Autoencoding__: To test this convolutional autoencoder, generate a response to a given input unit using \n",
    "\n",
    "    - Decoder reconstruction of same input\n",
    "    - Nearest-neighbor unit selection (Similar to what Bretan et al did)\n",
    "\n",
    "2. __De-noising__: Test de-noising abilities of the autoencoder. Given a partial accompaniment input unit, generate a response of\n",
    "\n",
    "    - Decoder reconstruction of \"full\"/\"comp\" unit\n",
    "    - Nearest-neighbor unit selection\n",
    "\n",
    "    (inspired by Huang et al Counterpoint by Convolution, and Bretan et al Learning and Evaluating Musical Features with Deep Autoencoders)\n",
    "\n",
    "These two tests simply require training the model on two different datasets: \"full\"->\"full\" for autoencoding, and \"input\"->\"comp\" for de-noising.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "\n",
    "_Initial code adapted from the [Keras tutorial on autoencoders](https://blog.keras.io/building-autoencoders-in-keras.html)._\n",
    "\n",
    "_Inspiration for convolution autoencoder network from \"Learning and Evaluating Musical Features with Deep\n",
    "Autoencoders\"._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, Conv2DTranspose, BatchNormalization, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: (3276, 96, 128)\n",
      "Reshaped: (3276, 128, 96, 1)\n",
      "Train: (2958, 128, 96, 1)\n",
      "Test: (318, 128, 96, 1)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "print \"Original:\", units[\"input\"].shape\n",
    "NUM_TICKS = units[\"input\"].shape[1] # 96\n",
    "NUM_PITCHES = units[\"input\"].shape[2] # 128\n",
    "assert NUM_TICKS == 96 and NUM_PITCHES == 128\n",
    "\n",
    "# Change from [M, ticks, pitches] to [M, pitches, ticks, channels=1]\n",
    "input_units = units[\"input\"].swapaxes(1,2).reshape(len(units[\"input\"]), NUM_PITCHES, NUM_TICKS, 1)\n",
    "# Normalize values between 0 and 1\n",
    "input_units = input_units.astype('float32') / 127. # 0-127 is the unnormalized velocity range\n",
    "print \"Reshaped:\", input_units.shape\n",
    "\n",
    "# Create an array of True (train) and False (test) to split the dataset\n",
    "train_test_indices = np.random.choice([True, False], size=len(input_units), p=[.9, .1])\n",
    "input_train = input_units[train_test_indices, ...]\n",
    "input_test = input_units[np.invert(train_test_indices), ...]\n",
    "print \"Train:\", input_train.shape\n",
    "print \"Test:\", input_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder V0\n",
    "\n",
    "`code given below`\n",
    "\n",
    "### Details\n",
    "\n",
    "Based on \"Learning and Evaluating Musical Features with Deep Autoencoders\", but adapted for different input size.\n",
    "\n",
    "```\n",
    "Data: -\n",
    "Embedding shape: (None, 1, 1, 800) -> 800 elements\n",
    "Epochs: -\n",
    "Batch size: -\n",
    "Final loss: -\n",
    "```\n",
    "\n",
    "### Notes\n",
    "\n",
    "Pretty sophisticated model, but unfortunately not able to train due to a `ResourceExhaustedError` upon running `model.fit`. This is most likely due to insufficient GPU memory (model is very large).\n",
    "\n",
    "Several attempts were made to shrink the model / reduce batch size (which apparently helps), but was not able to shake the error.\n",
    "\n",
    "### Next steps\n",
    "1. Look at how to shrink this model / use an alternative model. This [SO thread](https://stackoverflow.com/questions/41526071/why-is-keras-throwing-a-resourceexhaustederror) may be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_mat = Input(shape=(NUM_PITCHES, NUM_TICKS, 1))  # 'channels_last' data format (only 1 channel in our case)\n",
    "\n",
    "## ENCODER\n",
    "\n",
    "# First four layers are Conv2D\n",
    "x = Conv2D(100, (13, 21), strides=(5,5), activation='relu', padding='valid')(input_mat)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Conv2D(200, (2, 7), strides=(2,3), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Conv2D(400, (2, 2), strides=(2,2), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Conv2D(800, (2, 2), strides=(2,1), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "# Following three are fully connected\n",
    "x = Conv2D(800, (3, 1), strides=(1,1), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Dense(400, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "encoded = BatchNormalization()(x)\n",
    "\n",
    "# at this point the representation is a 100-dimensional vector\n",
    "\n",
    "## DECODER\n",
    "\n",
    "# Two fully connected\n",
    "decoded = Dense(400, activation='relu')(encoded)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2DTranspose(800, (3, 1), strides=(1,1), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "# Deconvolution / Convolution Transpose layers\n",
    "x = Conv2DTranspose(800, (2, 2), strides=(2,1), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Conv2DTranspose(400, (2, 2), strides=(2,2), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Conv2DTranspose(200, (2, 7), strides=(2,3), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Conv2DTranspose(100, (13, 21), strides=(5,5), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Conv2DTranspose(1, (NUM_TICKS, NUM_PITCHES), activation='relu', padding='valid')(x)\n",
    "decoded = BatchNormalization(axis=3)(x)\n",
    "\n",
    "autoencoder = Model(input_mat, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 128, 96, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 24, 16, 100)       27400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_85 (Batc (None, 24, 16, 100)       400       \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 12, 4, 200)        280200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_86 (Batc (None, 12, 4, 200)        800       \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 6, 2, 400)         320400    \n",
      "_________________________________________________________________\n",
      "batch_normalization_87 (Batc (None, 6, 2, 400)         1600      \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 3, 1, 800)         1280800   \n",
      "_________________________________________________________________\n",
      "batch_normalization_88 (Batc (None, 3, 1, 800)         3200      \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 1, 1, 800)         1920800   \n",
      "_________________________________________________________________\n",
      "batch_normalization_89 (Batc (None, 1, 1, 800)         3200      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1, 1, 400)         320400    \n",
      "_________________________________________________________________\n",
      "batch_normalization_90 (Batc (None, 1, 1, 400)         1600      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1, 1, 100)         40100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_92 (Batc (None, 1, 1, 100)         400       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_34 (Conv2DT (None, 3, 1, 800)         240800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_93 (Batc (None, 3, 1, 800)         3200      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_35 (Conv2DT (None, 6, 2, 800)         2560800   \n",
      "_________________________________________________________________\n",
      "batch_normalization_94 (Batc (None, 6, 2, 800)         3200      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_36 (Conv2DT (None, 12, 4, 400)        1280400   \n",
      "_________________________________________________________________\n",
      "batch_normalization_95 (Batc (None, 12, 4, 400)        1600      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_37 (Conv2DT (None, 24, 16, 200)       1120200   \n",
      "_________________________________________________________________\n",
      "batch_normalization_96 (Batc (None, 24, 16, 200)       800       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_38 (Conv2DT (None, 128, 96, 100)      5460100   \n",
      "_________________________________________________________________\n",
      "batch_normalization_97 (Batc (None, 128, 96, 100)      400       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_39 (Conv2DT (None, 128, 96, 1)        1228801   \n",
      "_________________________________________________________________\n",
      "batch_normalization_98 (Batc (None, 128, 96, 1)        4         \n",
      "=================================================================\n",
      "Total params: 16,101,605\n",
      "Trainable params: 16,091,403\n",
      "Non-trainable params: 10,202\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2923 samples, validate on 353 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[128,100,24,16] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: batch_normalization_119/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=\"NHWC\", epsilon=0.001, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_42/Relu, batch_normalization_119/gamma/read, batch_normalization_119/beta/read, batch_normalization_119/Const_4, batch_normalization_119/Const_4)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss_8/mul/_3855 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5422_loss_8/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op u'batch_normalization_119/FusedBatchNorm', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/junshern/Scripts/fyp-virtualenv/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/tornado/ioloop.py\", line 1065, in start\n    handler_func(fd_obj, events)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-45-54b84faa8e27>\", line 11, in <module>\n    x = BatchNormalization(axis=3)(x)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/keras/engine/topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/keras/layers/normalization.py\", line 181, in call\n    epsilon=self.epsilon)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 1827, in normalize_batch_in_training\n    epsilon=epsilon)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 1802, in _fused_normalize_batch_in_training\n    data_format=tf_data_format)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_impl.py\", line 906, in fused_batch_norm\n    name=name)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2224, in _fused_batch_norm\n    is_training=is_training, name=name)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[128,100,24,16] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: batch_normalization_119/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=\"NHWC\", epsilon=0.001, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_42/Relu, batch_normalization_119/gamma/read, batch_normalization_119/beta/read, batch_normalization_119/Const_4, batch_normalization_119/Const_4)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss_8/mul/_3855 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5422_loss_8/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-f19ce3718e2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                 callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\n\u001b[0m",
      "\u001b[0;32m/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[128,100,24,16] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: batch_normalization_119/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=\"NHWC\", epsilon=0.001, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_42/Relu, batch_normalization_119/gamma/read, batch_normalization_119/beta/read, batch_normalization_119/Const_4, batch_normalization_119/Const_4)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss_8/mul/_3855 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5422_loss_8/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op u'batch_normalization_119/FusedBatchNorm', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/junshern/Scripts/fyp-virtualenv/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/tornado/ioloop.py\", line 1065, in start\n    handler_func(fd_obj, events)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-45-54b84faa8e27>\", line 11, in <module>\n    x = BatchNormalization(axis=3)(x)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/keras/engine/topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/keras/layers/normalization.py\", line 181, in call\n    epsilon=self.epsilon)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 1827, in normalize_batch_in_training\n    epsilon=epsilon)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 1802, in _fused_normalize_batch_in_training\n    data_format=tf_data_format)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_impl.py\", line 906, in fused_batch_norm\n    name=name)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2224, in _fused_batch_norm\n    is_training=is_training, name=name)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[128,100,24,16] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: batch_normalization_119/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=\"NHWC\", epsilon=0.001, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_42/Relu, batch_normalization_119/gamma/read, batch_normalization_119/beta/read, batch_normalization_119/Const_4, batch_normalization_119/Const_4)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss_8/mul/_3855 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5422_loss_8/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "# Train model model\n",
    "autoencoder.fit(input_train, input_train,\n",
    "                epochs=100,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(input_test, input_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder V1\n",
    "\n",
    "`code given below`\n",
    "\n",
    "### Details\n",
    "\n",
    "Pretty arbitrary variant of the convolutional autoencoder architecture suggested in the [Keras tutorial](https://blog.keras.io/building-autoencoders-in-keras.html).\n",
    "\n",
    "```\n",
    "Data: input->input\n",
    "Embedding shape: (None, 32, 24, 32) -> 24576 elements\n",
    "Epochs: 100\n",
    "Batch size: 32\n",
    "Final loss: [loss: -0.0047 - val_loss: -0.0052]\n",
    "```\n",
    "\n",
    "### Notes\n",
    "Final binary crossentropy loss (after 100 epochs) gives `loss: -0.0047 - val_loss: -0.0052`, which is strange since __I don't think binary crossentropy should give negative values__? Will have to investigate further.\n",
    "\n",
    "Besides that, the decoded output looks really good. The graphs and playback are almost indistinguishable - can notice what appears to be quantization effects in the graphs, and some difference (note drops/additions, incorrect pitch) is occasionally audible. But mostly similar. \n",
    "\n",
    "On the whole, this model was a successful \"trial\" model. Demonstrates that the autoencoder actually produces a valid pianoroll, but our __input size was 12288 and embedding size is 24576__, which actually enlarges the dimensionality instead of shrinking it.\n",
    "\n",
    "### Next steps\n",
    "1. Investigate negative loss values ([most likely](https://github.com/Lasagne/Recipes/issues/54) due to some normalization issue - fix this in data preparation)\n",
    "2. Am I overfitting?\n",
    "3. Train on input->comp, or input->comp_next\n",
    "4. Shrink the embedding layer!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_mat = Input(shape=(NUM_PITCHES, NUM_TICKS, 1))  # 'channels_last' data format (only 1 channel in our case)\n",
    "\n",
    "# ENCODER\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_mat)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# At this point, the data is already represented in the embedding\n",
    "\n",
    "# DECODER\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_mat, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 128, 96, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 128, 96, 32)       320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 64, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 64, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 32, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 64, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 64, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 128, 96, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 128, 96, 1)        289       \n",
      "=================================================================\n",
      "Total params: 28,353\n",
      "Trainable params: 28,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2934 samples, validate on 342 samples\n",
      "Epoch 1/100\n",
      "2934/2934 [==============================] - 35s 12ms/step - loss: 0.2378 - val_loss: 0.1807\n",
      "Epoch 2/100\n",
      "2934/2934 [==============================] - 29s 10ms/step - loss: 0.1764 - val_loss: 0.1690\n",
      "Epoch 3/100\n",
      "2934/2934 [==============================] - 33s 11ms/step - loss: 0.0393 - val_loss: 0.0224\n",
      "Epoch 4/100\n",
      "2934/2934 [==============================] - 29s 10ms/step - loss: 0.0181 - val_loss: 0.0139\n",
      "Epoch 5/100\n",
      "2934/2934 [==============================] - 29s 10ms/step - loss: 0.0097 - val_loss: 0.0068\n",
      "Epoch 6/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: 0.0047 - val_loss: 0.0033\n",
      "Epoch 7/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: 0.0023 - val_loss: 0.0014\n",
      "Epoch 8/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: 9.3306e-04 - val_loss: 8.2079e-04\n",
      "Epoch 9/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -1.2816e-04 - val_loss: -7.8373e-04\n",
      "Epoch 10/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -9.0205e-04 - val_loss: -0.0017\n",
      "Epoch 11/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0014 - val_loss: -0.0020\n",
      "Epoch 12/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0018 - val_loss: -0.0024\n",
      "Epoch 13/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0021 - val_loss: -0.0027\n",
      "Epoch 14/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0023 - val_loss: -0.0029\n",
      "Epoch 15/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0025 - val_loss: -0.0030\n",
      "Epoch 16/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0026 - val_loss: -0.0031\n",
      "Epoch 17/100\n",
      "2934/2934 [==============================] - 30s 10ms/step - loss: -0.0027 - val_loss: -0.0032\n",
      "Epoch 18/100\n",
      "2934/2934 [==============================] - 29s 10ms/step - loss: -0.0028 - val_loss: -0.0033\n",
      "Epoch 19/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0029 - val_loss: -0.0033\n",
      "Epoch 20/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0030 - val_loss: -0.0035\n",
      "Epoch 21/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0031 - val_loss: -0.0036\n",
      "Epoch 22/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0032 - val_loss: -0.0037\n",
      "Epoch 23/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0032 - val_loss: -0.0038\n",
      "Epoch 24/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0033 - val_loss: -0.0039\n",
      "Epoch 25/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0034 - val_loss: -0.0038\n",
      "Epoch 26/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0034 - val_loss: -0.0039\n",
      "Epoch 27/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0035 - val_loss: -0.0040\n",
      "Epoch 28/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0035 - val_loss: -0.0036\n",
      "Epoch 29/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0035 - val_loss: -0.0041\n",
      "Epoch 30/100\n",
      "2934/2934 [==============================] - 29s 10ms/step - loss: -0.0036 - val_loss: -0.0041\n",
      "Epoch 31/100\n",
      "2934/2934 [==============================] - 29s 10ms/step - loss: -0.0037 - val_loss: -0.0042\n",
      "Epoch 32/100\n",
      "2934/2934 [==============================] - 30s 10ms/step - loss: -0.0037 - val_loss: -0.0042\n",
      "Epoch 33/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0037 - val_loss: -0.0042\n",
      "Epoch 34/100\n",
      "2934/2934 [==============================] - 33s 11ms/step - loss: -0.0038 - val_loss: -0.0042\n",
      "Epoch 35/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0038 - val_loss: -0.0042\n",
      "Epoch 36/100\n",
      "2934/2934 [==============================] - 29s 10ms/step - loss: -0.0038 - val_loss: -0.0043\n",
      "Epoch 37/100\n",
      "2934/2934 [==============================] - 29s 10ms/step - loss: -0.0039 - val_loss: -0.0043\n",
      "Epoch 38/100\n",
      "2934/2934 [==============================] - 30s 10ms/step - loss: -0.0039 - val_loss: -0.0044\n",
      "Epoch 39/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0039 - val_loss: -0.0044\n",
      "Epoch 40/100\n",
      "2934/2934 [==============================] - 29s 10ms/step - loss: -0.0039 - val_loss: -0.0044\n",
      "Epoch 41/100\n",
      "2934/2934 [==============================] - 29s 10ms/step - loss: -0.0040 - val_loss: -0.0044\n",
      "Epoch 42/100\n",
      "2934/2934 [==============================] - 29s 10ms/step - loss: -0.0040 - val_loss: -0.0043\n",
      "Epoch 43/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0040 - val_loss: -0.0045\n",
      "Epoch 44/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0040 - val_loss: -0.0045\n",
      "Epoch 45/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0041 - val_loss: -0.0043\n",
      "Epoch 46/100\n",
      "2934/2934 [==============================] - 30s 10ms/step - loss: -0.0041 - val_loss: -0.0046\n",
      "Epoch 47/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0041 - val_loss: -0.0046\n",
      "Epoch 48/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0041 - val_loss: -0.0046\n",
      "Epoch 49/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0041 - val_loss: -0.0046\n",
      "Epoch 50/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0042 - val_loss: -0.0047\n",
      "Epoch 51/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0042 - val_loss: -0.0045\n",
      "Epoch 52/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0042 - val_loss: -0.0047\n",
      "Epoch 53/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0042 - val_loss: -0.0047\n",
      "Epoch 54/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0042 - val_loss: -0.0047\n",
      "Epoch 55/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0042 - val_loss: -0.0048\n",
      "Epoch 56/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0043 - val_loss: -0.0048\n",
      "Epoch 57/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0043 - val_loss: -0.0048\n",
      "Epoch 58/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0043 - val_loss: -0.0048\n",
      "Epoch 59/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0043 - val_loss: -0.0048\n",
      "Epoch 60/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0043 - val_loss: -0.0048\n",
      "Epoch 61/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0043 - val_loss: -0.0048\n",
      "Epoch 62/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0044 - val_loss: -0.0048\n",
      "Epoch 63/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0044 - val_loss: -0.0048\n",
      "Epoch 64/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0044 - val_loss: -0.0048\n",
      "Epoch 65/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0043 - val_loss: -0.0048\n",
      "Epoch 66/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0044 - val_loss: -0.0048\n",
      "Epoch 67/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0044 - val_loss: -0.0049\n",
      "Epoch 68/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0044 - val_loss: -0.0049\n",
      "Epoch 69/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0045 - val_loss: -0.0049\n",
      "Epoch 70/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0045 - val_loss: -0.0049\n",
      "Epoch 71/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0045 - val_loss: -0.0049\n",
      "Epoch 72/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0045 - val_loss: -0.0049\n",
      "Epoch 73/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0045 - val_loss: -0.0050\n",
      "Epoch 74/100\n",
      "2934/2934 [==============================] - 28s 9ms/step - loss: -0.0045 - val_loss: -0.0049\n",
      "Epoch 75/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0045 - val_loss: -0.0050\n",
      "Epoch 76/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0045 - val_loss: -0.0049\n",
      "Epoch 77/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0045 - val_loss: -0.0050\n",
      "Epoch 78/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0045 - val_loss: -0.0050\n",
      "Epoch 79/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0046 - val_loss: -0.0050\n",
      "Epoch 80/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0045 - val_loss: -0.0050\n",
      "Epoch 81/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0046 - val_loss: -0.0049\n",
      "Epoch 82/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0046 - val_loss: -0.0051\n",
      "Epoch 83/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0046 - val_loss: -0.0051\n",
      "Epoch 84/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0046 - val_loss: -0.0050\n",
      "Epoch 85/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0046 - val_loss: -0.0050\n",
      "Epoch 86/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0046 - val_loss: -0.0051\n",
      "Epoch 87/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0046 - val_loss: -0.0051\n",
      "Epoch 88/100\n",
      "2934/2934 [==============================] - 28s 9ms/step - loss: -0.0046 - val_loss: -0.0051\n",
      "Epoch 89/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0047 - val_loss: -0.0051\n",
      "Epoch 90/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0047 - val_loss: -0.0051\n",
      "Epoch 91/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0046 - val_loss: -0.0051\n",
      "Epoch 92/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0046 - val_loss: -0.0051\n",
      "Epoch 93/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0047 - val_loss: -0.0051\n",
      "Epoch 94/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0047 - val_loss: -0.0051\n",
      "Epoch 95/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0047 - val_loss: -0.0051\n",
      "Epoch 96/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0046 - val_loss: -0.0051\n",
      "Epoch 97/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0047 - val_loss: -0.0051\n",
      "Epoch 98/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0047 - val_loss: -0.0052\n",
      "Epoch 99/100\n",
      "2934/2934 [==============================] - 28s 9ms/step - loss: -0.0047 - val_loss: -0.0051\n",
      "Epoch 100/100\n",
      "2934/2934 [==============================] - 28s 10ms/step - loss: -0.0047 - val_loss: -0.0052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4517f9dad0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model model\n",
    "autoencoder.fit(input_train, input_train,\n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                shuffle=True,\n",
    "                validation_data=(input_test, input_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\n",
    "\n",
    "MODEL_AUTOENCODER_V1_FILE = './models/autoencoder_v1.h5'\n",
    "autoencoder.save(MODEL_AUTOENCODER_V1_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_AUTOENCODER_V1_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = load_model(MODEL_AUTOENCODER_V1_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test inputs through the autoencoder\n",
    "decoded_test = autoencoder.predict(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a random input-output sample\n",
    "sample_index = np.random.randint(len(input_test))\n",
    "sample_input = input_test[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "sample_output = decoded_test[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "\n",
    "print(sample_input.shape)\n",
    "print(sample_output.shape)\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(1,2)\n",
    "fig.set_size_inches(10, 6, forward=True)\n",
    "ax[0].set_title('Input')\n",
    "ax[1].set_title('Output')\n",
    "pypianoroll.plot_pianoroll(ax[0], sample_input, beat_resolution=24)\n",
    "pypianoroll.plot_pianoroll(ax[1], sample_output, beat_resolution=24)\n",
    "fig.tight_layout()\n",
    "\n",
    "# Play comparison\n",
    "pianoroll_utils.playPianoroll(sample_input)\n",
    "pianoroll_utils.playPianoroll(sample_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
