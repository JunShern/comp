{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The ultimate goal of the model is to learn a __latent variable space of musical units__. Then, given a musical unit, we wish to encode that unit into a latent vector within the space, and predict the best accompaninment latent vector to that input. Finally, that accompaniment latent vector can be decoded to produce an accompanying musical unit.\n",
    "\n",
    "This involves many tricky steps, so development will be approached incrementally:\n",
    "\n",
    "#### 1. Convolutional Autoencoder\n",
    "\n",
    "Given an input unit of `[num_ticks, num_pitches]`, learn a Convolutional Autoencoder model to generate an encoding of that unit.\n",
    "\n",
    "```\n",
    "INPUT -> Convolution layers -> EMBEDDING -> Deconvolution layers -> INPUT\n",
    "```\n",
    "\n",
    "Autoencoding: To test this convolutional autoencoder, generate a response to a given input unit using \n",
    "- Decoder reconstruction of same input\n",
    "- Nearest-neighbor unit selection (Similar to what Bretan et al did)\n",
    "\n",
    "De-noising: Test de-noising abilities of the autoencoder. Given a partial accompaniment input unit, generate a response of\n",
    "- Decoder reconstruction of \"full\"/\"comp\" unit\n",
    "- Nearest-neighbor unit selection\n",
    "\n",
    "#### 2. LSTM of latent variables -> Generation using unit selection\n",
    "\n",
    "Given a sequence of embeddings (from the convolutional autoencoder), predict the next embedding - and perform NN-unit-selection as before, to generate the next unit in the sequence.\n",
    "\n",
    "#### 3. Convolutional Variational Autoencoder\n",
    "\n",
    "Learn a new latent space using a VAE architecture (great explanation on VAEs in [this blogpost](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf)). Test how well resconstruction works using\n",
    "- Decoder reconstruction\n",
    "\n",
    "#### 4. LSTM of variational latent variables -> Generation using latent space sampling \n",
    "\n",
    "Given a sequence of embeddings (from the VAE), predict the next embedding and generate an output musical unit by decoding the predicted embedding!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and user variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import pypianoroll\n",
    "from matplotlib import pyplot as plt\n",
    "import cPickle as pickle\n",
    "import pianoroll_utils\n",
    "import IPython\n",
    "import h5py\n",
    "import time\n",
    "from __future__ import print_function\n",
    "import custom_loss\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import sklearn.externals\n",
    "\n",
    "from keras.layers import Input, Dense, Conv2D, Conv2DTranspose, BatchNormalization, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard, History\n",
    "from keras.models import load_model\n",
    "\n",
    "from scipy.stats import norm\n",
    "from keras.layers import Lambda, Flatten, Reshape, Dropout\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "\n",
    "# Music shape\n",
    "MIN_PITCH = 13 # A-1 (MIDI 21)\n",
    "MAX_PITCH = 108 # C7 (MIDI 108)\n",
    "BEATS_PER_UNIT = 4\n",
    "BEAT_RESOLUTION = 24 # This is set by the encoding of the lpd-5 dataset, corresponds to number of ticks per beat\n",
    "PARTITION_NOTE = 60 # Break into left- and right-accompaniments at middle C\n",
    "NUM_PITCHES = MAX_PITCH - MIN_PITCH + 1\n",
    "NUM_TICKS = BEATS_PER_UNIT * BEAT_RESOLUTION\n",
    "\n",
    "NUM_FILES = 200\n",
    "PICKLE_FILE = './pickle_jar/norm_units_{}_songs_clipped96.pkl'.format(NUM_FILES)\n",
    "UNITS_FILE = './pickle_jar/units_{}_songs_clipped96.h5'.format(NUM_FILES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = {}\n",
    "with open(PICKLE_FILE, 'rb') as infile:\n",
    "    units = pickle.load( infile )\n",
    "units['full'] = units['input'] + units['comp']\n",
    "\n",
    "# Print info\n",
    "print \"Loaded\", units[\"input\"].shape[0], \"units from\", PICKLE_FILE\n",
    "print \"input_units.shape: \", units[\"input\"].shape\n",
    "print \"input_units.shape: \", units[\"input_noisy\"].shape\n",
    "print \"comp_units.shape: \", units[\"comp\"].shape\n",
    "print \"full_units.shape: \", units[\"full\"].shape\n",
    "\n",
    "# Create an array of True (train) and False (test) to split the dataset\n",
    "# Also reshape each pianoroll from (128, 96) to (128, 96, 1); \n",
    "# the '1' corresponds to number of img channels\n",
    "train_test_indices = np.random.choice([True, False], size=len(units[\"input\"]), p=[.9, .1])\n",
    "# Training data\n",
    "input_train = units[\"input\"][train_test_indices, ...].reshape(-1, NUM_PITCHES, NUM_TICKS, 1)\n",
    "input_train_noisy = units[\"input_noisy\"][train_test_indices, ...].reshape(-1, NUM_PITCHES, NUM_TICKS, 1)\n",
    "comp_train = units[\"comp\"][train_test_indices, ...].reshape(-1, NUM_PITCHES, NUM_TICKS, 1)\n",
    "full_train = units[\"full\"][train_test_indices, ...].reshape(-1, NUM_PITCHES, NUM_TICKS, 1)\n",
    "# Testing data\n",
    "input_test = units[\"input\"][np.invert(train_test_indices), ...].reshape(-1, NUM_PITCHES, NUM_TICKS, 1)\n",
    "input_test_noisy = units[\"input_noisy\"][np.invert(train_test_indices), ...].reshape(-1, NUM_PITCHES, NUM_TICKS, 1)\n",
    "comp_test = units[\"comp\"][np.invert(train_test_indices), ...].reshape(-1, NUM_PITCHES, NUM_TICKS, 1)\n",
    "full_test = units[\"full\"][np.invert(train_test_indices), ...].reshape(-1, NUM_PITCHES, NUM_TICKS, 1)\n",
    "print \"Train:\", input_train.shape\n",
    "print \"Test:\", input_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (38012, 96, 96, 1)\n",
      "Noisy train: (38012, 96, 96, 1)\n",
      "Test: (3047, 96, 96, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "f = h5py.File(UNITS_FILE, 'r')\n",
    "\n",
    "# Training data\n",
    "input_train = f['units_train']\n",
    "input_train_noisy = f['units_train_noisy']\n",
    "# Testing data\n",
    "input_test = f['units_test']\n",
    "print(\"Train:\", input_train.shape)\n",
    "print(\"Noisy train:\", input_train_noisy.shape)\n",
    "print(\"Test:\", input_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Convolutional Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Given an input unit of `[num_ticks, num_pitches]`, learn a Convolutional Autoencoder model to generate an encoding of that unit.\n",
    "\n",
    "```\n",
    "INPUT -> Convolution layers -> EMBEDDING -> Deconvolution layers -> INPUT\n",
    "```\n",
    "\n",
    "### Testing\n",
    "\n",
    "We will evaluate the autoencoder using two measures:\n",
    "\n",
    "1. __Autoencoding__: To test this convolutional autoencoder, generate a response to a given input unit using \n",
    "\n",
    "    - Decoder reconstruction of same input\n",
    "    - Nearest-neighbor unit selection (Similar to what Bretan et al did)\n",
    "\n",
    "2. __De-noising__: Test de-noising abilities of the autoencoder. Given a partial accompaniment input unit, generate a response of\n",
    "\n",
    "    - Decoder reconstruction of \"full\"/\"comp\" unit\n",
    "    - Nearest-neighbor unit selection\n",
    "\n",
    "    (inspired by Huang et al Counterpoint by Convolution, and Bretan et al Learning and Evaluating Musical Features with Deep Autoencoders)\n",
    "\n",
    "These two tests simply require training the model on two different datasets: \"full\"->\"full\" for autoencoding, and \"input\"->\"comp\" for de-noising.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "\n",
    "_Initial code adapted from the [Keras tutorial on autoencoders](https://blog.keras.io/building-autoencoders-in-keras.html)._\n",
    "\n",
    "_Inspiration for convolution autoencoder network from \"Learning and Evaluating Musical Features with Deep\n",
    "Autoencoders\"._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Autoencoder V0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Details\n",
    "\n",
    "Based on \"Learning and Evaluating Musical Features with Deep Autoencoders\", but adapted for different input size.\n",
    "\n",
    "```\n",
    "Data: -\n",
    "Embedding shape: (None, 1, 1, 800) -> 800 elements\n",
    "Epochs: -\n",
    "Batch size: -\n",
    "Final loss: -\n",
    "```\n",
    "\n",
    "### Notes\n",
    "\n",
    "Pretty sophisticated model, but unfortunately not able to train due to a `ResourceExhaustedError` upon running `model.fit`. This is most likely due to insufficient GPU memory (model is very large).\n",
    "\n",
    "Several attempts were made to shrink the model / reduce batch size (which apparently helps), but was not able to shake the error.\n",
    "\n",
    "### Next steps\n",
    "1. Look at how to shrink this model / use an alternative model. This [SO thread](https://stackoverflow.com/questions/41526071/why-is-keras-throwing-a-resourceexhaustederror) may be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "input_mat = Input(shape=(NUM_PITCHES, NUM_TICKS, 1))  # 'channels_last' data format (only 1 channel in our case)\n",
    "\n",
    "## ENCODER\n",
    "\n",
    "# First four layers are Conv2D\n",
    "x = Conv2D(100, (13, 21), strides=(5,5), activation='relu', padding='valid')(input_mat)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Conv2D(200, (2, 7), strides=(2,3), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Conv2D(400, (2, 2), strides=(2,2), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Conv2D(800, (2, 2), strides=(2,1), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "# Following three are fully connected\n",
    "x = Conv2D(800, (3, 1), strides=(1,1), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Dense(400, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "encoded = BatchNormalization()(x)\n",
    "\n",
    "# at this point the representation is a 100-dimensional vector\n",
    "\n",
    "## DECODER\n",
    "\n",
    "# Two fully connected\n",
    "decoded = Dense(400, activation='relu')(encoded)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2DTranspose(800, (3, 1), strides=(1,1), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "# Deconvolution / Convolution Transpose layers\n",
    "x = Conv2DTranspose(800, (2, 2), strides=(2,1), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Conv2DTranspose(400, (2, 2), strides=(2,2), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Conv2DTranspose(200, (2, 7), strides=(2,3), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Conv2DTranspose(100, (13, 21), strides=(5,5), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Conv2DTranspose(1, (NUM_TICKS, NUM_PITCHES), activation='relu', padding='valid')(x)\n",
    "decoded = BatchNormalization(axis=3)(x)\n",
    "\n",
    "autoencoder = Model(input_mat, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Train model model\n",
    "autoencoder.fit(input_train, input_train,\n",
    "                epochs=100,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(input_test, input_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\n",
    "# Run `tensorboard --logdir=/tmp/autoencoder` to start tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Autoencoder V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`code given below`\n",
    "\n",
    "### Details\n",
    "\n",
    "Pretty arbitrary variant of the convolutional autoencoder architecture suggested in the [Keras tutorial](https://blog.keras.io/building-autoencoders-in-keras.html).\n",
    "\n",
    "```\n",
    "Data: input->input\n",
    "Embedding shape: (None, 32, 24, 32) -> 24576 elements\n",
    "Epochs: 100\n",
    "Batch size: 32\n",
    "Final loss: [loss: -0.0047 - val_loss: -0.0052]\n",
    "```\n",
    "\n",
    "### Notes\n",
    "\n",
    "#### Test \\#1\n",
    "~~Final binary crossentropy loss (after 100 epochs) gives `loss: -0.0047 - val_loss: -0.0052`, which is strange since __I don't think binary crossentropy should give negative values__? Will have to investigate further.~~ \n",
    "\n",
    "Besides that, the decoded output looks really good. The graphs and playback are almost indistinguishable - can notice what appears to be quantization effects in the graphs, and some difference (note drops/additions, incorrect pitch) is occasionally audible. But mostly similar. \n",
    "\n",
    "On the whole, this model was a successful \"trial\" model. Demonstrates that the autoencoder actually produces a valid pianoroll, but our __input size was 12288 and embedding size is 24576__, which actually enlarges the dimensionality instead of shrinking it.\n",
    "\n",
    "#### Test \\#2\n",
    "Tried `input->comp`, it also sounds like someone dropped a piano off a cliff (like in AEV2). \n",
    "\n",
    "#### Test \\#3\n",
    "Tried `input->full`, it sounds like a baby crawling across the keyboard while a pianist is playing. Which is to say it's better than `input->comp` (as expected), but only marginally so, and there's still no proof that this particular model will ever be able to learn accompaniments (at least on this dataset).\n",
    "\n",
    "It's clear from the pianoroll plot, that the \"dropped piano\" sound comes from the fact that almost all values are non-zero (hence all notes are sounding at the same time). Which in hindsight is not surprising, given a continuous prediction space. Perhaps we need to __tweak the loss function to impose a penalty on too many notes sounding at the same time__?\n",
    "\n",
    "#### Follow-up: Fixing the dropped-piano problem\n",
    "Holy shit this made all the difference!!! \n",
    "\n",
    "```\n",
    "# Set all velocity values < threshold to zero\n",
    "sample_output[sample_output < 10] = 0\n",
    "```\n",
    "\n",
    "Output sounds sooo much better now.\n",
    "\n",
    "For the case of `input->full`, you can now clearly hear the input recreated in the output (which is what we expect), but there are also some additional notes thrown into the mix (though they aren't always \"good\" accompaniment notes).\n",
    "\n",
    "For the case of `input->comp`, which we expect to be more difficult, the comp notes are actually not very bad! They aren't very perfect, but they actually sound musically-coherent! Very interesting...\n",
    "\n",
    "### Next steps\n",
    "1. ~~Investigate negative loss values ([most likely](https://github.com/Lasagne/Recipes/issues/54) due to some normalization issue)~~ (Fixed with correct normalization of data in data_prep)\n",
    "2. ~~Train on input->comp~~ (Tried, it sucks)\n",
    "3. ~~Train on input->full~~\n",
    "4. Shrink the embedding layer!!\n",
    "5. Some sort of unit selection task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "input_mat = Input(shape=(NUM_PITCHES, NUM_TICKS, 1))  # 'channels_last' data format (only 1 channel in our case)\n",
    "\n",
    "# ENCODER\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_mat)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# At this point, the data is already represented in the embedding\n",
    "\n",
    "# DECODER\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_mat, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model model\n",
    "autoencoder.fit(input_train, input_train,\n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                shuffle=True,\n",
    "                validation_data=(input_test, input_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\n",
    "\n",
    "MODEL_AUTOENCODER_V1_FILE = './models/autoencoder_v1.h5'\n",
    "autoencoder.save(MODEL_AUTOENCODER_V1_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_AUTOENCODER_V1_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model model\n",
    "autoencoder.fit(input_train, comp_train,\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                shuffle=True,\n",
    "                validation_data=(input_test, comp_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\n",
    "\n",
    "MODEL_AUTOENCODER_V1_FILE = './models/autoencoder_v1_input_comp.h5'\n",
    "autoencoder.save(MODEL_AUTOENCODER_V1_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_AUTOENCODER_V1_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model model\n",
    "autoencoder.fit(input_train, full_train,\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                shuffle=True,\n",
    "                validation_data=(input_test, full_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\n",
    "\n",
    "MODEL_AUTOENCODER_V1_FILE = './models/autoencoder_v1_input_full.h5'\n",
    "autoencoder.save(MODEL_AUTOENCODER_V1_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_AUTOENCODER_V1_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MODEL_AUTOENCODER_V1_FILE = './models/autoencoder_v1_input_comp.h5'\n",
    "autoencoder = load_model(MODEL_AUTOENCODER_V1_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Run test inputs through the autoencoder\n",
    "decoded_test = autoencoder.predict(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Inspect a random input-output sample\n",
    "# sample_index = np.random.randint(len(input_test))\n",
    "sample_input = input_test[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "sample_comp = comp_test[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "sample_output = decoded_test[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "\n",
    "# Set all velocity values < threshold to zero\n",
    "sample_output[sample_output < 10] = 0\n",
    "\n",
    "print(sample_input.shape)\n",
    "print(sample_comp.shape)\n",
    "print(sample_output.shape)\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(1,3)\n",
    "fig.set_size_inches(10, 6, forward=True)\n",
    "ax[0].set_title('Input')\n",
    "ax[1].set_title('Comp')\n",
    "ax[2].set_title('Output')\n",
    "pypianoroll.plot_pianoroll(ax[0], sample_input, beat_resolution=24)\n",
    "pypianoroll.plot_pianoroll(ax[1], sample_comp, beat_resolution=24)\n",
    "pypianoroll.plot_pianoroll(ax[2], sample_output, beat_resolution=24)\n",
    "fig.tight_layout()\n",
    "\n",
    "# Play comparison\n",
    "pianoroll_utils.playPianoroll(sample_input)\n",
    "pianoroll_utils.playPianoroll(sample_comp)\n",
    "pianoroll_utils.playPianoroll(sample_output)\n",
    "pianoroll_utils.playPianoroll(sample_input+sample_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Autoencoder V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`code given below`\n",
    "\n",
    "### Details\n",
    "\n",
    "Iterating on the faults of the previous model: \n",
    "1. Change from an `input->input` autoencoder to a `input->comp` de-noising autoencoder,\n",
    "2. Reduce size of embedding layer by increasing number of conv/pooling layers.\n",
    "\n",
    "```\n",
    "Data: input->comp, input->input\n",
    "Embedding shape: (None, 3, 2, 100) -> 600 elements\n",
    "Epochs: 50\n",
    "Batch size: 128\n",
    "Final loss: [loss: 0.0322 - val_loss: 0.0334]\n",
    "```\n",
    "\n",
    "### Notes\n",
    "\n",
    "#### Test \\#1\n",
    "`input->comp`\n",
    "\n",
    "This test was an absolute disaster! The output produced _actually_ sounded like someone dropping a piano off a cliff (which is fun but not very useful for the purpose of this project).\n",
    "\n",
    "Interesting things to note - the training and validation loss started at `loss: 0.0375 - val_loss: 0.0302`, then ended at `[loss: 0.0322 - val_loss: 0.0334]`, barely learning anything at all. The model seemed to be improving consistently at the beginning, reaching a loss of about `0.02` at the halfway point (25th epoch), but then started getting worse consistently. Why did this happen? I thought that GD should only ever improve the model (or in the case of BGD, deteriorate slightly but improve overall). Also, what I hear is clearly not only 2% error. Maybe need to re-evaluate the loss function.\n",
    "\n",
    "One other thing about the model, is that perhaps it is too much to expect it to be able to predict the left and right accompaniments. Might be worth trying `input->full` instead of `input->comp`, and maybe think of a completely new way to structure the data...\n",
    "\n",
    "Finally, the model itself was also changed significantly from the previous version. Maybe the drop in hidden layer dimension is too steep, or the embedding size is too small. Should test with basic `input->input` case first.\n",
    "\n",
    "#### Test \\#2\n",
    "`input->input`\n",
    "\n",
    "Hmm, interestingly this wasn't any better than the input->comp instance. Which implies that the model itself is broken. Onwards!\n",
    "\n",
    "#### Follow-up\n",
    "\n",
    "Applying a threshold-to-zero filter on notes with low velocity helped in removing the \"dropped piano\" noise, but the remaining output was consistently bad. It appears that the model is taking wild guesses with too many notes at low velocities (close to no notes remain after a velocity-50 threshold), even for the simplest `input-input` autoencoder task.\n",
    "\n",
    "Comparing this to AEV1, we can conclude that the output thresholding is likely a good idea, and that the right network should indeed be able to reproduce a close decoding. The good performance of AEV1 and poor performance of AEV2 implies that the data is usable, the loss function is acceptable, and most likely the main problem is just finding the right autoencoder network. This should be the next priority - designing a network which produces a smaller embedding than AEV1 while maintaining output quality.\n",
    "\n",
    "### Next steps\n",
    "1. Investigate output (why doesn't the pianoroll plot look like a piano being thrown off a cliff?)\n",
    "2. Investigate behaviour of the trainig/validation losses.\n",
    "3. ~~Try using `input-input` and `input-full` to make sure the model can actually something useful.~~ \n",
    "4. Try a different autoencoder architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "input_mat = Input(shape=(NUM_PITCHES, NUM_TICKS, 1))  # 'channels_last' data format (only 1 channel in our case)\n",
    "\n",
    "# ENCODER\n",
    "x = Conv2D(200, (13, 21), strides=(5,5), activation='relu', padding='valid')(input_mat)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Conv2D(100, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(100, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(100, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# At this point, the data is already represented in the embedding\n",
    "\n",
    "# DECODER\n",
    "x = Conv2D(100, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(100, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(200, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "\n",
    "decoded = Conv2DTranspose(1, (13, 21), strides=(5,5), activation='relu', padding='valid')(x)\n",
    "\n",
    "autoencoder = Model(input_mat, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model [INPUT->COMP]\n",
    "autoencoder.fit(input_train, comp_train,\n",
    "                epochs=50,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(input_test, comp_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\n",
    "\n",
    "MODEL_AUTOENCODER_V2_FILE = './models/autoencoder_v2.h5'\n",
    "autoencoder.save(MODEL_AUTOENCODER_V2_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_AUTOENCODER_V2_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model [INPUT->INPUT]\n",
    "autoencoder.fit(input_train, input_train,\n",
    "                epochs=50,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(input_test, input_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\n",
    "\n",
    "MODEL_AUTOENCODER_V2_FILE = './models/autoencoder_v2_input_input.h5'\n",
    "autoencoder.save(MODEL_AUTOENCODER_V2_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_AUTOENCODER_V2_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MODEL_AUTOENCODER_V2_FILE = './models/autoencoder_v2_input_input.h5'\n",
    "autoencoder = load_model(MODEL_AUTOENCODER_V2_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Run test inputs through the autoencoder\n",
    "decoded_test = autoencoder.predict(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Inspect a random input-output sample\n",
    "sample_index = np.random.randint(len(input_test))\n",
    "sample_input = input_test[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "# sample_comp = comp_test[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "sample_output = decoded_test[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "\n",
    "# Set all velocity values < threshold to zero\n",
    "sample_output[sample_output < 20] = 0\n",
    "\n",
    "print(sample_input.shape)\n",
    "# print(sample_comp.shape)\n",
    "print(sample_output.shape)\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(1,2)\n",
    "fig.set_size_inches(10, 6, forward=True)\n",
    "ax[0].set_title('Input')\n",
    "# ax[1].set_title('Target')\n",
    "ax[1].set_title('Output')\n",
    "pypianoroll.plot_pianoroll(ax[0], sample_input, beat_resolution=24)\n",
    "# pypianoroll.plot_pianoroll(ax[1], sample_comp, beat_resolution=24)\n",
    "pypianoroll.plot_pianoroll(ax[1], sample_output, beat_resolution=24)\n",
    "fig.tight_layout()\n",
    "\n",
    "# Play comparison\n",
    "pianoroll_utils.playPianoroll(sample_input)\n",
    "# pianoroll_utils.playPianoroll(sample_comp)\n",
    "pianoroll_utils.playPianoroll(sample_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Autoencoder V3 - Unit Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`code given below`\n",
    "\n",
    "### Details\n",
    "\n",
    "The focus of this model is to use a small enough embedding layer, such that we can do nearest-neighbour unit selection.\n",
    "\n",
    "```\n",
    "Data: input->input\n",
    "Embedding shape: (None, 3, 2, 100) -> 600 elements\n",
    "Epochs: 50\n",
    "Batch size: 128\n",
    "Final loss: \n",
    "```\n",
    "\n",
    "We begin with a baseline embedding of 600 elements (by comparison, Bretan uses a 500-value embedding), loaded directly from AEV2 ([reference](https://github.com/keras-team/keras/issues/41) for loading encoder weights in Keras; [docs](https://keras.io/layers/about-keras-layers/) on Keras layers). \n",
    "\n",
    "### Notes\n",
    "\n",
    "Pretty happy with this! Lots more to go, but I'm glad to have a working implementation of unit selection - it's quite a milestone to have something that successfully uses an encoder to get an embedding, which we can subsequently do nearest-neighbor selection from!\n",
    "\n",
    "This is my first approach which produces non-piano-off-the-cliff results, which is encouraging. But then again, legible output is pretty much guaranteed by the unit selection method, so it's still to early to celebrate.\n",
    "\n",
    "Interestingly, testing the system with training data does not give back the exact same input, but it seems to give a reasonably \"close-sounding\" match. Which is reasonable, given that our embedding dimensionality is so much smaller than the original. \n",
    "\n",
    "Using testing data, the corresponding kNN match is much less similar to the data, which is expected to some extent. There seems to be some sort of rhythmic or note-density similarities in the input->matched pair, but this may be my own confirmation bias. \n",
    "\n",
    "Most important thing to do now is to find a way to measure the \"appropriateness\" of the generated outputs, and then we can start to benchmark/quantify our results. :))\n",
    "\n",
    "### Next steps\n",
    "1. Try different embeddings\n",
    "2. Quantify results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MODEL_AUTOENCODER_V2_FILE = './models/autoencoder_v2_input_input.h5'\n",
    "ae_V2 = load_model(MODEL_AUTOENCODER_V2_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# for layer in ae_V2.layers:\n",
    "#     print layer, len(layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Recreate just the encoder portion of AEV2 \n",
    "input_mat = Input(shape=(NUM_PITCHES, NUM_TICKS, 1))  # 'channels_last' data format (only 1 channel in our case)\n",
    "\n",
    "# ENCODER\n",
    "x = Conv2D(200, (13, 21), strides=(5,5), activation='relu', padding='valid')(input_mat)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Conv2D(100, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(100, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(100, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "encoder = Model(input_mat, encoded)\n",
    "encoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "## Load up the weights trained with AEV2\n",
    "for i, layer in enumerate(encoder.layers):\n",
    "    encoder.layers[i].set_weights(ae_V2.layers[i].get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MODEL_FILE = './models/encoder_v2_input_input.h5'\n",
    "encoder.save(MODEL_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "del encoder\n",
    "encoder = load_model(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getFlattenedEncodings(inputs, enc):\n",
    "    \"\"\"\n",
    "    Given an input matrix of shape (M, 128, 96, 1) and a trained encoder model,\n",
    "    run each M pianorolls through the encoder and return an (M, F) matrix \n",
    "    where F is the length of the FLATTENED embedding layer.\n",
    "    \"\"\"\n",
    "    assert inputs.shape[1] == 128\n",
    "    assert inputs.shape[2] == 96\n",
    "    assert inputs.shape[3] == 1\n",
    "    \n",
    "    encodings = enc.predict(inputs)\n",
    "    flat_encodings = encodings.reshape(encodings.shape[0], -1)\n",
    "    return flat_encodings\n",
    "\n",
    "# Build a dataset of embeddings of our input units\n",
    "encoded_inputs = getFlattenedEncodings(input_train, encoder)\n",
    "print(encoded_inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import sklearn.externals\n",
    "\n",
    "# Training the nearest neighbors model\n",
    "knn_model = NearestNeighbors(n_neighbors=1).fit(encoded_inputs)\n",
    "\n",
    "# Save model along with the units used to learn the model\n",
    "KNN_MODEL_FILENAME = \"./models/unit_selector_knn.pkl\"\n",
    "sklearn.externals.joblib.dump((knn_model, input_train, comp_train), KNN_MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Testing the model with training data!\n",
    "\n",
    "# Choose a sample input to find the nearest neighbor of \n",
    "# (in this case, the 1nn should be itself, or a unit which is almost identical to itself)\n",
    "sample_index = np.random.randint(len(input_train))\n",
    "sample_input = input_train[sample_index].reshape(1, 128, 96, 1)\n",
    "sample_encoding = getFlattenedEncodings(sample_input, encoder)\n",
    "# Prediction\n",
    "knn_index = knn_model.kneighbors(sample_encoding, return_distance = False)[0][0]\n",
    "print sample_index, knn_index\n",
    "\n",
    "sample_input = input_train[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "knn_input = input_train[knn_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "knn_comp = comp_train[knn_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(1,3)\n",
    "fig.set_size_inches(10, 6, forward=True)\n",
    "ax[0].set_title('Input')\n",
    "ax[1].set_title('Full')\n",
    "ax[2].set_title('Output')\n",
    "pypianoroll.plot_pianoroll(ax[0], sample_input, beat_resolution=24)\n",
    "pypianoroll.plot_pianoroll(ax[1], knn_input, beat_resolution=24)\n",
    "pypianoroll.plot_pianoroll(ax[2], knn_comp, beat_resolution=24)\n",
    "fig.tight_layout()\n",
    "\n",
    "# Play comparison\n",
    "pianoroll_utils.playPianoroll(sample_input)\n",
    "pianoroll_utils.playPianoroll(knn_input)\n",
    "pianoroll_utils.playPianoroll(knn_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Testing the model with test data!\n",
    "\n",
    "# Inspect a random input-output sample\n",
    "# Choose a test input to find the nearest neighbor of\n",
    "sample_index = np.random.randint(len(input_test))\n",
    "sample_input = input_test[sample_index].reshape(1, 128, 96, 1)\n",
    "sample_encoding = getFlattenedEncodings(sample_input, encoder)\n",
    "# Prediction\n",
    "knn_index = knn_model.kneighbors(sample_encoding, return_distance = False)[0][0]\n",
    "sample_input = input_train[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "knn_input = input_train[knn_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "knn_comp = comp_train[knn_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(1,3)\n",
    "fig.set_size_inches(10, 6, forward=True)\n",
    "ax[0].set_title('Input')\n",
    "ax[1].set_title('Full')\n",
    "ax[2].set_title('Output')\n",
    "pypianoroll.plot_pianoroll(ax[0], sample_input, beat_resolution=24)\n",
    "pypianoroll.plot_pianoroll(ax[1], knn_input, beat_resolution=24)\n",
    "pypianoroll.plot_pianoroll(ax[2], knn_comp, beat_resolution=24)\n",
    "fig.tight_layout()\n",
    "\n",
    "# Play comparison\n",
    "pianoroll_utils.playPianoroll(sample_input)\n",
    "pianoroll_utils.playPianoroll(knn_input)\n",
    "pianoroll_utils.playPianoroll(knn_comp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Autoencoder V4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`code given below`\n",
    "\n",
    "### Details\n",
    "\n",
    "Trying to reduce the embedding size of AEV1 without losing all musicality (as happened in AEV2).\n",
    "\n",
    "```\n",
    "Data: input->input\n",
    "Embedding shape: (None, 3, 2, 100) -> 600 elements\n",
    "Epochs: 100\n",
    "Batch size: 32\n",
    "Final loss: [loss: -0.0047 - val_loss: -0.0052]\n",
    "```\n",
    "\n",
    "### Notes\n",
    "\n",
    "#### Attempt \\#1\n",
    "\n",
    "Adding more conv-pooling pairs (even just one more) to AEV1 had a devastating effect on the decoded output. The training/validation loss also seemed to stagnate at around 16% after just one or two iterations, so I quickly aborted the mission.\n",
    "\n",
    "#### Attempt \\#2\n",
    "\n",
    "Tried to copy the model of AEV2, but removed one conv-pooling pair in hopes that the output would be better than AEV2. Not much difference.\n",
    "\n",
    "#### Attempt \\#3\n",
    "\n",
    "**Here's where it gets so much better**: AEV2 with one less conv-pooling pair, and changed the loss function from binary crossentropy to _mean-squared error_, and the results are MAGNITUDES BETTER. (I mean, who's bright idea was it to use binary crossentropy for a pianoroll matrix anyway?) \n",
    "\n",
    "The output is acceptable (sounds mostly similar to the input except for some stray wrong notes or occasional dropped notes) even without any thresholding to zero. \n",
    "\n",
    "The best loss now is: `loss: 4.7768e-04 - val_loss: 8.2842e-04`. The logical next step will be to add back the conv-pooling pair and see what happens...\n",
    "\n",
    "#### Attempt \\#4\n",
    "\n",
    "Recap: The model is exactly the same as AEV2, but with mean-squared error instead of binary crossentropy. This makes sense, and (more importantly?) it gives a good decoded output with an embedding size of just 600! \n",
    "\n",
    "The best loss now is: `loss: 0.0010 - val_loss: 0.0015` which is slightly poorer than the previous attempt, but this is expected given that we have shrunk the embedding size from 2400 to 600 (This is a good thing, but may not be necessary considering the tradeoff in reconstruction accuracy).\n",
    "\n",
    "Importantly, we begin to get a sense of how the convolutional autoencoder starts to fail in this current architecture - the notes are treated simply as numbers - there is no category, no understanding of relationship between notes, which pairs go together, and whatnot. The autoencoder tries to numerically approximate the closest numerical reconstruction, but does not appear to have learnt any musical rules (this is my guess, given the seeming randomness of wrong note-attempts and note drops in the reconstruction).\n",
    "\n",
    "As such, I expect that the model will do quite poorly in the next task: to learn a encoder-decoder mapping for `input->full`.\n",
    "\n",
    "#### Attempt \\#5\n",
    "\n",
    "Here, for the task of `input->full`, we approach the limitations of our current approach. The reconstruction is actually not very bad - the output is doing well at replicating the input, but the 'accompaniment' it adds on to the input to make it more full, are more often than not, bad choices. Overall, the output sounds acceptable, and on occasion the accompaniment seems interesting and almost believable as the work of a particularly audacious composer. But alas, this is not enough.\n",
    "\n",
    "The important thing to remember now, before running off to change from a convolutional autoencoder to a one-hot encoded RNN autoencoder, is that if we can successfully create a variational autoencoder, it may not be necessary at all to master the `input->full` or `input->comp` tasks (although I cannot deny how wonderful it would be to do so)! \n",
    "\n",
    "A variational autoencoder would give us the flexibility (via latent space arithmetic) to produce a sufficiently different output given an input, that it would no longer be considered a copy! So, so long as we can learn a musically correct latent space, we are in the gold.\n",
    "\n",
    "#### Attempt \\#6\n",
    "\n",
    "Task: `input->comp`. Results: Actually much better than I thought it would be! But the main observation comes after trying to integrate it into the demo - the output sounds much less interesting (few, unconvincing notes) than what happens here when listening to a test sample. This is most probably a data problem: The test set are too similar to the training set (presumably due to repetition in many songs) to actually give the true test error, so the model is probably overfit on the current songs, which are far too few.\n",
    "\n",
    "Thus, it is important to try again on the much larger full dataset as soon as possible, and if that doesn't work, we may need to record our own dataset based on my playing (since that will be the target user input).\n",
    "\n",
    "### Next steps\n",
    "1. ~~Try putting this into a live demo context. :)~~ Tried. Unfortunately live input seems to work much more poorly than the test input, maybe because the input distribution is too different from the training distribution?\n",
    "2. Start work on RNNs for next-unit prediction.\n",
    "3. Consider changing the convolutional autoencoder architecture (though this should hardly be a priority for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "input_mat = Input(shape=(NUM_PITCHES, NUM_TICKS, 1))  # 'channels_last' data format (only 1 channel in our case)\n",
    "\n",
    "# ENCODER\n",
    "x = Conv2D(200, (13, 21), strides=(5,5), activation='relu', padding='valid')(input_mat)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Conv2D(100, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(100, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(100, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# At this point, the data is already represented in the embedding\n",
    "\n",
    "# DECODER\n",
    "x = Conv2D(100, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(100, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(200, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "\n",
    "decoded = Conv2DTranspose(1, (13, 21), strides=(5,5), activation='relu', padding='valid')(x)\n",
    "\n",
    "autoencoder = Model(input_mat, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model model\n",
    "autoencoder.fit(input_train, input_train,\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                shuffle=True,\n",
    "                validation_data=(input_test, input_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\n",
    "\n",
    "MODEL_AUTOENCODER_V1_FILE = './models/autoencoder_v4.h5'\n",
    "autoencoder.save(MODEL_AUTOENCODER_V1_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_AUTOENCODER_V1_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model model\n",
    "autoencoder.fit(input_train, full_train,\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                shuffle=True,\n",
    "                validation_data=(input_test, full_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\n",
    "\n",
    "MODEL_AUTOENCODER_V1_FILE = './models/autoencoder_v4_input_full.h5'\n",
    "autoencoder.save(MODEL_AUTOENCODER_V1_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_AUTOENCODER_V1_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model model\n",
    "autoencoder.fit(input_train, comp_train,\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                shuffle=True,\n",
    "                validation_data=(input_test, comp_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\n",
    "\n",
    "MODEL_AUTOENCODER_V1_FILE = './models/autoencoder_v4_input_comp.h5'\n",
    "autoencoder.save(MODEL_AUTOENCODER_V1_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_AUTOENCODER_V1_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MODEL_AUTOENCODER_FILE = './models/autoencoder_v4_input_comp.h5'\n",
    "autoencoder = load_model(MODEL_AUTOENCODER_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Run test inputs through the autoencoder\n",
    "decoded_test = autoencoder.predict(input_test)\n",
    "print decoded_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Inspect a random input-output sample\n",
    "sample_index = np.random.randint(len(input_test))\n",
    "sample_input = input_test[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "sample_full = full_test[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "sample_output = decoded_test[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "\n",
    "# Set all velocity values < threshold to zero\n",
    "sample_output[sample_output < 10] = 0\n",
    "sample_output[sample_output >= 10] = 80\n",
    "\n",
    "print(sample_input.shape)\n",
    "print(sample_full.shape)\n",
    "print(sample_output.shape)\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(1,3)\n",
    "fig.set_size_inches(10, 6, forward=True)\n",
    "ax[0].set_title('Input')\n",
    "ax[1].set_title('Target')\n",
    "ax[2].set_title('Output')\n",
    "pypianoroll.plot_pianoroll(ax[0], sample_input, beat_resolution=24, cmap='inferno')\n",
    "pypianoroll.plot_pianoroll(ax[1], sample_full, beat_resolution=24, cmap='inferno')\n",
    "pypianoroll.plot_pianoroll(ax[2], sample_output, beat_resolution=24, cmap='inferno')\n",
    "fig.tight_layout()\n",
    "\n",
    "# Play comparison\n",
    "pianoroll_utils.playPianoroll(sample_input)\n",
    "# pianoroll_utils.playPianoroll(sample_full)\n",
    "# pianoroll_utils.playPianoroll(sample_output)\n",
    "pianoroll_utils.playPianoroll_events(sample_output.swapaxes(0,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Variational Autoencoder V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`code given below`\n",
    "\n",
    "### Details\n",
    "\n",
    "Trying to adapt the model of AEV4 into a variational autoencoder. Code adapted from [this tutorial](https://blog.keras.io/building-autoencoders-in-keras.html), and [this example](https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder_deconv.py).\n",
    "\n",
    "```\n",
    "Data: input->input\n",
    "Embedding shape: 2400\n",
    "Epochs: 50\n",
    "Batch size: 32\n",
    "Final loss: loss: 266.8356 - val_loss: 273.1442\n",
    "```\n",
    "\n",
    "### Notes\n",
    "\n",
    "#### Attempt 1 (at least, first recorded one haha)\n",
    "```\n",
    "Latent dim = 2400\n",
    "Intermediate_dim = None (removed all dense layers except latent)\n",
    "Kernel size = (3,3)\n",
    "Output activation: sigmoid\n",
    "Loss: xent + kldiv\n",
    "Optimizer: adam\n",
    "Best error: loss: 375.8985 - val_loss: 364.5504 (after 10 epochs)\n",
    "```\n",
    "\n",
    "Actually not that bad. There is some inkling of the original sound in the output, but notes are held pretty much all the time (similar problem as we saw in RNNs).\n",
    "\n",
    "#### Attempt 2\n",
    "```\n",
    "Attempt 1 modified\n",
    "Output activation: relu\n",
    "Best error: Around 2000\n",
    "```\n",
    "\n",
    "Quite terrible, loss wasn't even going down. Aborted.\n",
    "\n",
    "#### Attempt 3\n",
    "```\n",
    "Attempt 1 modified\n",
    "Kernel size = (6,6)\n",
    "Best error: loss: 390.4387 - val_loss: 384.1872\n",
    "```\n",
    "\n",
    "Not easily distinguishable from Attempt 1, but seems a little worse based on loss.\n",
    "\n",
    "#### Attempt 3\n",
    "```\n",
    "Attempt 1 modified\n",
    "Loss: mse + kldiv\n",
    "Best error: Plateau and stayed at (loss: 0.0080 - val_loss: 0.0082) after 3 epochs\n",
    "```\n",
    "\n",
    "This is nowhere as good as the xent error version (why?). It seems to learn a completely uniformly 0 (or close to zero) output across the whole matrix.\n",
    "\n",
    "#### Attempt 4\n",
    "```\n",
    "Attempt 1 modified\n",
    "Latent dim: 600 (achieved by adding one more convolutional+max pooling pair)\n",
    "Best error: loss: 413.7631 - val_loss: 403.9150\n",
    "```\n",
    "\n",
    "Unsurprisingly, it does a little worse than original (since we have a smaller latent dimension).\n",
    "\n",
    "#### Attempt 5 (saved as vae_v1.h5)\n",
    "```\n",
    "Attempt 1 modified\n",
    "Epochs: 50\n",
    "Best error: loss: 266.8356 - val_loss: 273.1442\n",
    "```\n",
    "\n",
    "SOOOOO much better! Successfully maintains some semblance of the input even without output quantization, but still struggles heavily with more complicated note sequences (seems to only learn the note onsets and always drags the note out to the end of the unit).\n",
    "\n",
    "#### Attempt 6\n",
    "```\n",
    "Attempt 1 modified\n",
    "Optimizer: rmsprop\n",
    "Epochs: 50\n",
    "Best error: loss: 283.0813 - val_loss: 312.6207\n",
    "```\n",
    "\n",
    "Almost the same as adam, but slightly poorer learning. Best to stick with adam.\n",
    "\n",
    "\n",
    "### Next steps\n",
    "1. Investigate using a different dataset: Try maybe 2-beat measures, drop from 128 to 88 pitches. \n",
    "2. Also, **seriously consider** changing to a one-hot encoded scheme!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Autoencoder portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "latent_dim = 2400\n",
    "intermediate_dim = 600\n",
    "epsilon_std = 1.0\n",
    "NUM_PITCHES = 128\n",
    "NUM_TICKS = 96\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., # latent_dim\n",
    "                              stddev=epsilon_std) # epsilon_std\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "# Encoder stuff\n",
    "input_mat = Input(shape=(NUM_PITCHES, NUM_TICKS, 1))  # 'channels_last' data format (only 1 channel in our case)\n",
    "x = Conv2D(200, (13, 21), strides=(5,5), activation='relu', padding='valid')(input_mat)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(100, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(100, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "# x = Conv2D(100, (3, 3), activation='relu', padding='same')(x)\n",
    "# x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Flatten()(x)\n",
    "# x = Dense(intermediate_dim, activation='relu')(x)\n",
    "\n",
    "# Modelling the embedding\n",
    "z_mean = Dense(latent_dim)(x)\n",
    "z_log_var = Dense(latent_dim)(x)\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var]) # \"output_shape\" not needed using tf backend\n",
    "\n",
    "# Decoder stuff\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_hid = Dense(intermediate_dim, activation='relu')\n",
    "decoder_reshape = Reshape((6, 4, 100))\n",
    "decoder_conv1 = Conv2D(100, (3, 3), activation='relu', padding='same')\n",
    "decoder_upsamp1 = UpSampling2D((2, 2))\n",
    "decoder_conv2 = Conv2D(100, (3, 3), activation='relu', padding='same')\n",
    "decoder_upsamp2 = UpSampling2D((2, 2))\n",
    "decoder_conv3 = Conv2D(200, (3, 3), activation='relu', padding='same')\n",
    "decoder_upsamp3 = UpSampling2D((2, 2))\n",
    "decoder_conv4 = Conv2DTranspose(1, (13, 21), strides=(5,5), activation='sigmoid', padding='valid')\n",
    "# now we apply the decoder layers we previously defined\n",
    "# x = decoder_hid(z)\n",
    "x = decoder_reshape(z)\n",
    "# x = decoder_conv1(x)\n",
    "# x = decoder_upsamp1(x)\n",
    "x = decoder_conv2(x)\n",
    "x = decoder_upsamp2(x)\n",
    "x = decoder_conv3(x)\n",
    "x = decoder_upsamp3(x)\n",
    "x_decoded_mean = decoder_conv4(x)\n",
    "\n",
    "# instantiate VAE model\n",
    "autoencoder = Model(input_mat, x_decoded_mean)\n",
    "\n",
    "# Compute VAE loss\n",
    "def vae_loss(input_x, output_x):\n",
    "    xent_loss = NUM_PITCHES * NUM_TICKS * metrics.binary_crossentropy(K.flatten(input_x), K.flatten(output_x))\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "# def vae_loss(input_x, output_x):\n",
    "#     mse_loss = K.mean(K.square(K.flatten(input_x) - K.flatten(output_x)))\n",
    "#     kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "#     return K.mean(mse_loss + kl_loss)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss=vae_loss)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the VAE model\n",
    "autoencoder.fit(input_train, input_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        validation_data=(input_test, input_test))\n",
    "\n",
    "MODEL_FILE = './models/vae_v1.h5'\n",
    "autoencoder.save(MODEL_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Encoder model\n",
    "encoder = Model(input_mat, z_mean)\n",
    "\n",
    "MODEL_FILE = './models/vae_v1_encoder.h5'\n",
    "encoder.save(MODEL_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_FILE\n",
    "\n",
    "# Decoder model\n",
    "_z = Input(shape=(latent_dim,))\n",
    "# _x = decoder_hid(_z)\n",
    "_x = decoder_reshape(_z)\n",
    "# _x = decoder_conv1(_x)\n",
    "# _x = decoder_upsamp1(_x)\n",
    "_x = decoder_conv2(_x)\n",
    "_x = decoder_upsamp2(_x)\n",
    "_x = decoder_conv3(_x)\n",
    "_x = decoder_upsamp3(_x)\n",
    "_x_decoded_mean = decoder_conv4(_x)\n",
    "generator = Model(_z, _x_decoded_mean)\n",
    "\n",
    "MODEL_FILE = './models/vae_v1_generator.h5'\n",
    "generator.save(MODEL_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MODEL_FILE = './models/vae_v1.h5'\n",
    "autoencoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std, 'vae_loss': vae_loss})\n",
    "MODEL_FILE = './models/vae_v1_encoder.h5'\n",
    "encoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})\n",
    "MODEL_FILE = './models/vae_v1_generator.h5'\n",
    "generator = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Run test inputs through the autoencoder\n",
    "decoded_test = autoencoder.predict(input_test)\n",
    "print decoded_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Inspect a random input-output sample\n",
    "sample_index = np.random.randint(len(input_test))\n",
    "sample_input = input_test[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "sample_full = full_test[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "sample_output = decoded_test[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "\n",
    "# Set all velocity values < threshold to zero\n",
    "sample_output[sample_output < 10] = 0\n",
    "sample_output = np.clip(sample_output * 2, 0, 127)\n",
    "\n",
    "print(sample_input.shape)\n",
    "print(sample_full.shape)\n",
    "print(sample_output.shape)\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(1,3)\n",
    "fig.set_size_inches(10, 6, forward=True)\n",
    "ax[0].set_title('Input')\n",
    "ax[1].set_title('Target')\n",
    "ax[2].set_title('Output')\n",
    "pypianoroll.plot_pianoroll(ax[0], sample_input, beat_resolution=24, cmap='inferno')\n",
    "pypianoroll.plot_pianoroll(ax[1], sample_full, beat_resolution=24, cmap='inferno')\n",
    "pypianoroll.plot_pianoroll(ax[2], sample_output, beat_resolution=24, cmap='inferno')\n",
    "fig.tight_layout()\n",
    "\n",
    "# Play comparison\n",
    "pianoroll_utils.play_pianoroll_events(sample_input.T)\n",
    "# pianoroll_utils.play_pianoroll_events(sample_full)\n",
    "# pianoroll_utils.play_pianoroll_events(sample_output)\n",
    "pianoroll_utils.play_pianoroll_events(sample_output.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Encoding-decoding test\n",
    "sample_index = np.random.randint(len(input_test))\n",
    "x = input_test[sample_index].reshape(1, input_test[sample_index].shape[0], input_test[sample_index].shape[1], 1)\n",
    "\n",
    "z = encoder.predict(x)\n",
    "x_decoded = generator.predict(z)\n",
    "\n",
    "print(x.shape)\n",
    "print(z.shape)\n",
    "print(x_decoded.shape)\n",
    "\n",
    "# Playback\n",
    "sample_input = input_test[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "sample_output = x_decoded[0].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "\n",
    "pianoroll_utils.play_pianoroll_events(sample_input.T)\n",
    "pianoroll_utils.play_pianoroll_events(sample_output.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Variational Autoencoder V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Using architecture inspired by [this tutorial](https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder_deconv.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img_chns = 1\n",
    "latent_dim = 128\n",
    "intermediate_dim = 1024\n",
    "filters = 64\n",
    "num_conv = 3\n",
    "\n",
    "epsilon_std = 1.0\n",
    "NUM_PITCHES = 128\n",
    "NUM_TICKS = 96\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., # latent_dim\n",
    "                              stddev=epsilon_std) # epsilon_std\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "# Encoder stuff\n",
    "x = Input(shape=(NUM_PITCHES, NUM_TICKS, 1))\n",
    "conv_0 = Conv2D(200, (13, 21), strides=(5,5), activation='relu', padding='valid')(x)\n",
    "batch_norm = BatchNormalization()(conv_0)\n",
    "conv_1 = Conv2D(img_chns,\n",
    "                kernel_size=(2, 2),\n",
    "                padding='same', activation='relu')(batch_norm)\n",
    "conv_2 = Conv2D(filters,\n",
    "                kernel_size=(2, 2),\n",
    "                padding='same', activation='relu',\n",
    "                strides=(2, 2))(conv_1)\n",
    "conv_3 = Conv2D(filters,\n",
    "                kernel_size=num_conv,\n",
    "                padding='same', activation='relu',\n",
    "                strides=1)(conv_2)\n",
    "conv_4 = Conv2D(filters,\n",
    "                kernel_size=num_conv,\n",
    "                padding='same', activation='relu',\n",
    "                strides=1)(conv_3)\n",
    "flat = Flatten()(conv_4)\n",
    "hidden = Dense(intermediate_dim, activation='relu')(flat)\n",
    "\n",
    "z_mean = Dense(latent_dim)(hidden)\n",
    "z_log_var = Dense(latent_dim)(hidden)\n",
    "\n",
    "\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_hid = Dense(intermediate_dim, activation='relu')\n",
    "decoder_upsample = Dense(filters * 12 * 8, activation='relu')\n",
    "decoder_reshape = Reshape((12, 8, filters))\n",
    "decoder_deconv_1 = Conv2DTranspose(filters,\n",
    "                                   kernel_size=num_conv,\n",
    "                                   padding='same',\n",
    "                                   strides=1,\n",
    "                                   activation='relu')\n",
    "decoder_deconv_2 = Conv2DTranspose(filters,\n",
    "                                   kernel_size=num_conv,\n",
    "                                   padding='same',\n",
    "                                   strides=1,\n",
    "                                   activation='relu')\n",
    "decoder_deconv_3_upsamp = Conv2DTranspose(filters,\n",
    "                                          kernel_size=(3, 3),\n",
    "                                          strides=(2, 2),\n",
    "                                          padding='valid',\n",
    "                                          activation='relu')\n",
    "decoder_mean_squash = Conv2D(img_chns,\n",
    "                             kernel_size=2,\n",
    "                             padding='valid',\n",
    "                             activation='sigmoid')\n",
    "\n",
    "decoder_deconv_0 = Conv2DTranspose(1, (13, 21), strides=(5,5), activation='sigmoid', padding='valid')\n",
    "\n",
    "hid_decoded = decoder_hid(z)\n",
    "up_decoded = decoder_upsample(hid_decoded)\n",
    "reshape_decoded = decoder_reshape(up_decoded)\n",
    "deconv_1_decoded = decoder_deconv_1(reshape_decoded)\n",
    "deconv_2_decoded = decoder_deconv_2(deconv_1_decoded)\n",
    "x_decoded_relu = decoder_deconv_3_upsamp(deconv_2_decoded)\n",
    "x_decoded_mean_squash = decoder_mean_squash(x_decoded_relu)\n",
    "x_decoded_mean_squash = decoder_deconv_0(x_decoded_mean_squash)\n",
    "\n",
    "# instantiate VAE model\n",
    "vae = Model(x, x_decoded_mean_squash)\n",
    "\n",
    "# Compute VAE loss\n",
    "# def vae_loss(input_x, output_x):\n",
    "#     xent_loss = NUM_PITCHES * NUM_TICKS * metrics.binary_crossentropy(K.flatten(input_x), K.flatten(output_x))\n",
    "#     kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "#     return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "def vae_loss(input_x, output_x):\n",
    "    mse_loss = K.mean(K.square(K.flatten(input_x) - K.flatten(output_x)))\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return K.mean(mse_loss + kl_loss)\n",
    "\n",
    "vae.compile(optimizer='rmsprop', loss=vae_loss)\n",
    "vae.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the VAE model\n",
    "vae.fit(input_train, \n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        validation_data=(input_test, None))\n",
    "\n",
    "MODEL_FILE = './models/vae_v2.h5'\n",
    "vae.save(MODEL_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Run test inputs through the autoencoder\n",
    "decoded_test = vae.predict(input_test)\n",
    "print decoded_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Inspect a random input-output sample\n",
    "sample_index = np.random.randint(len(input_test))\n",
    "sample_input = input_test[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "sample_full = full_test[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "sample_output = decoded_test[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "\n",
    "# Set all velocity values < threshold to zero\n",
    "# sample_output[sample_output < 10] = 0\n",
    "# sample_output[sample_output >= 10] = 80\n",
    "\n",
    "print(sample_input.shape)\n",
    "print(sample_full.shape)\n",
    "print(sample_output.shape)\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(1,3)\n",
    "fig.set_size_inches(10, 6, forward=True)\n",
    "ax[0].set_title('Input')\n",
    "ax[1].set_title('Target')\n",
    "ax[2].set_title('Output')\n",
    "pypianoroll.plot_pianoroll(ax[0], sample_input, beat_resolution=24, cmap='inferno')\n",
    "pypianoroll.plot_pianoroll(ax[1], sample_full, beat_resolution=24, cmap='inferno')\n",
    "pypianoroll.plot_pianoroll(ax[2], sample_output, beat_resolution=24, cmap='inferno')\n",
    "fig.tight_layout()\n",
    "\n",
    "# Play comparison\n",
    "# pianoroll_utils.playPianoroll(sample_input)\n",
    "# pianoroll_utils.playPianoroll(sample_full)\n",
    "# pianoroll_utils.playPianoroll(sample_output)\n",
    "# pianoroll_utils.playPianoroll_events(sample_output.swapaxes(0,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Variational Autoencoder V3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`code given below`\n",
    "\n",
    "### Details\n",
    "\n",
    "Testing the model of VAEV1 on new 88-pitch dataset, 200 songs. GCP debut woot woot! Also trying larger batch size.\n",
    "\n",
    "```\n",
    "Data: input->input\n",
    "Embedding shape: 1600\n",
    "Epochs: 50\n",
    "Batch size: 128\n",
    "Final loss:\n",
    "```\n",
    "\n",
    "### Notes\n",
    "\n",
    "#### Attempt 1\n",
    "```\n",
    "Latent dim = 1600\n",
    "Intermediate_dim = None (removed all dense layers except latent)\n",
    "Kernel size = (3,3)\n",
    "Output activation: sigmoid\n",
    "Loss: xent + kldiv\n",
    "Optimizer: adam\n",
    "Best error: loss: 208.0337 - val_loss: 212.2165 (after 50 epochs)\n",
    "```\n",
    "\n",
    "This GCP comper-gpu machine is AMAZEBALLLSSSS. Training 18000+ samples in like 5 minutes, didn't even break a sweat. WOOOHOOO. \n",
    "\n",
    "On the other hand, the actual reconstructed output isn't that much better, unfortunately. The output certainly looks a lot more convincing, but there's still a problem of ghost notes and random wrong notes - which makes the playback sound less than ideal.\n",
    "\n",
    "### Next steps\n",
    "1. Best bet from here is probably to implement the tick-vocabulary thing (word2vec), and go from convolutional autoencoder to RNN-autoencoder.\n",
    "2. But before that, let's try to do the next step prediction thing. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Autoencoder portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "latent_dim = 1600\n",
    "intermediate_dim = 600\n",
    "epsilon_std = 1.0\n",
    "NUM_PITCHES = 88\n",
    "NUM_TICKS = 96\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., # latent_dim\n",
    "                              stddev=epsilon_std) # epsilon_std\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "# Encoder stuff\n",
    "input_mat = Input(shape=(NUM_PITCHES, NUM_TICKS, 1))  # 'channels_last' data format (only 1 channel in our case)\n",
    "x = Conv2D(200, (13, 21), strides=(5,5), activation='relu', padding='valid')(input_mat)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(100, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(100, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "# x = Conv2D(100, (3, 3), activation='relu', padding='same')(x)\n",
    "# x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Flatten()(x)\n",
    "# x = Dense(intermediate_dim, activation='relu')(x)\n",
    "\n",
    "# Modelling the embedding\n",
    "z_mean = Dense(latent_dim)(x)\n",
    "z_log_var = Dense(latent_dim)(x)\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var]) # \"output_shape\" not needed using tf backend\n",
    "\n",
    "# Decoder stuff\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_hid = Dense(intermediate_dim, activation='relu')\n",
    "decoder_reshape = Reshape((4, 4, 100))\n",
    "decoder_conv1 = Conv2D(100, (3, 3), activation='relu', padding='same')\n",
    "decoder_upsamp1 = UpSampling2D((2, 2))\n",
    "decoder_conv2 = Conv2D(100, (3, 3), activation='relu', padding='same')\n",
    "decoder_upsamp2 = UpSampling2D((2, 2))\n",
    "decoder_conv3 = Conv2D(200, (3, 3), activation='relu', padding='same')\n",
    "decoder_upsamp3 = UpSampling2D((2, 2))\n",
    "decoder_conv4 = Conv2DTranspose(1, (13, 21), strides=(5,5), activation='sigmoid', padding='valid')\n",
    "# now we apply the decoder layers we previously defined\n",
    "# x = decoder_hid(z)\n",
    "x = decoder_reshape(z)\n",
    "# x = decoder_conv1(x)\n",
    "# x = decoder_upsamp1(x)\n",
    "x = decoder_conv2(x)\n",
    "x = decoder_upsamp2(x)\n",
    "x = decoder_conv3(x)\n",
    "x = decoder_upsamp3(x)\n",
    "x_decoded_mean = decoder_conv4(x)\n",
    "\n",
    "# instantiate VAE model\n",
    "autoencoder = Model(input_mat, x_decoded_mean)\n",
    "\n",
    "# Compute VAE loss\n",
    "def vae_loss(input_x, output_x):\n",
    "    xent_loss = NUM_PITCHES * NUM_TICKS * metrics.binary_crossentropy(K.flatten(input_x), K.flatten(output_x))\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "# def vae_loss(input_x, output_x):\n",
    "#     mse_loss = K.mean(K.square(K.flatten(input_x) - K.flatten(output_x)))\n",
    "#     kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "#     return K.mean(mse_loss + kl_loss)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss=vae_loss)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the VAE model\n",
    "autoencoder.fit(input_train, input_train,\n",
    "        epochs=50,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        validation_data=(input_test, input_test))\n",
    "\n",
    "MODEL_FILE = './models/vae_v3.h5'\n",
    "autoencoder.save(MODEL_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Encoder model\n",
    "encoder = Model(input_mat, z_mean)\n",
    "\n",
    "MODEL_FILE = './models/vae_v3_encoder.h5'\n",
    "encoder.save(MODEL_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_FILE\n",
    "\n",
    "# Decoder model\n",
    "_z = Input(shape=(latent_dim,))\n",
    "# _x = decoder_hid(_z)\n",
    "_x = decoder_reshape(_z)\n",
    "# _x = decoder_conv1(_x)\n",
    "# _x = decoder_upsamp1(_x)\n",
    "_x = decoder_conv2(_x)\n",
    "_x = decoder_upsamp2(_x)\n",
    "_x = decoder_conv3(_x)\n",
    "_x = decoder_upsamp3(_x)\n",
    "_x_decoded_mean = decoder_conv4(_x)\n",
    "generator = Model(_z, _x_decoded_mean)\n",
    "\n",
    "MODEL_FILE = './models/vae_v3_generator.h5'\n",
    "generator.save(MODEL_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MODEL_FILE = './models/vae_v3.h5'\n",
    "autoencoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std, 'vae_loss': vae_loss})\n",
    "MODEL_FILE = './models/vae_v3_encoder.h5'\n",
    "encoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})\n",
    "MODEL_FILE = './models/vae_v3_generator.h5'\n",
    "generator = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Run test inputs through the autoencoder\n",
    "decoded_test = autoencoder.predict(input_test)\n",
    "print decoded_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Inspect a random input-output sample\n",
    "sample_index = np.random.randint(len(input_test))\n",
    "sample_input = input_test[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "sample_full = full_test[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "sample_output = decoded_test[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "\n",
    "# Set all velocity values < threshold to zero\n",
    "sample_output[sample_output < 1] = 0\n",
    "# sample_output = np.clip(sample_output * 2, 0, 127)\n",
    "\n",
    "print(sample_input.shape)\n",
    "print(sample_full.shape)\n",
    "print(sample_output.shape)\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(1,3)\n",
    "fig.set_size_inches(10, 6, forward=True)\n",
    "ax[0].set_title('Input')\n",
    "ax[1].set_title('Target')\n",
    "ax[2].set_title('Output')\n",
    "pianoroll_utils.plot_pianoroll(ax[0], sample_input, MIN_PITCH, MAX_PITCH, beat_resolution=24, cmap='inferno')\n",
    "pianoroll_utils.plot_pianoroll(ax[1], sample_full, MIN_PITCH, MAX_PITCH, beat_resolution=24, cmap='inferno')\n",
    "pianoroll_utils.plot_pianoroll(ax[2], sample_output, MIN_PITCH, MAX_PITCH, beat_resolution=24, cmap='inferno')\n",
    "fig.tight_layout()\n",
    "\n",
    "# Play comparison\n",
    "audio0 = pianoroll_utils.play_pianoroll(sample_input.T, MIN_PITCH, MAX_PITCH, '0')\n",
    "IPython.display.display(IPython.display.Audio(audio0))\n",
    "\n",
    "audio1 = pianoroll_utils.play_pianoroll(sample_output.T, MIN_PITCH, MAX_PITCH, '1')\n",
    "IPython.display.display(IPython.display.Audio(audio1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Variational Autoencoder V4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`code given below`\n",
    "\n",
    "### Details\n",
    "\n",
    "Try to reduce the latent dimension, and add dropout layers.\n",
    "\n",
    "```\n",
    "Data: input->input\n",
    "Embedding shape: 1600\n",
    "Epochs: 50\n",
    "Batch size: 128\n",
    "Final loss:\n",
    "```\n",
    "\n",
    "### Notes\n",
    "\n",
    "#### Attempt 1\n",
    "```\n",
    "Latent dim = 1600\n",
    "Intermediate_dim = 1600\n",
    "Dropout = None\n",
    "Kernel size = (3,3)\n",
    "Output activation: sigmoid\n",
    "Loss: xent + kldiv\n",
    "Optimizer: adam\n",
    "Best error: loss: 225.7566 - val_loss: 230.2700 (after 50 epochs)\n",
    "```\n",
    "\n",
    "Easiest way to allow flexibility in latent dimension size is to add back the intermediate hidden layers. So, create intermediate hidden layers with 1600 size. Leave latent dim as 1600 for now, in the next attempt we will try to reduce this.\n",
    "\n",
    "Output is pretty decent! Comparable to the version without intermediate hidden layers, loss is a bit poorer (230 vs 210) but no discernible difference in output. \n",
    "\n",
    "#### Attempt 2\n",
    "```\n",
    "Latent dim = 100\n",
    "Best error: loss: 170.5274 - val_loss: 179.4777 (after 50 epochs)\n",
    "```\n",
    "\n",
    "Here, we drop the latent dimension from 1600 down to 100. Big change!\n",
    "\n",
    "The results are better than I expected! Loss is down to 180 (from a previous best of 210 in AEV3), and the output is about as good as it ever was. Let's try adding dropout next.\n",
    "\n",
    "#### Attempt 3\n",
    "```\n",
    "Dropout = 0.5 before latent layer in the encoder\n",
    "Best error: loss: 190.9748 - val_loss: 187.5620 (after 50 epochs)\n",
    "```\n",
    "\n",
    "Results are pretty great still! Not quite as good as the previous attempt, but this is second best. Output is more or less as usual, but really it's not really possible to know the different when you're inspecting a different random sample for each experiment.\n",
    "\n",
    "Next: Add dropout on decoder as well.\n",
    "\n",
    "#### Attempt 4\n",
    "```\n",
    "Dropout = 0.5 before latent layer in the encoder\n",
    "Dropout = 0.5 after latent layer in the decoder\n",
    "Best error: loss: 280.6763 - val_loss: 246.5374\n",
    "```\n",
    "\n",
    "Loss is quite a bit worse than the previous ones, though validation loss is still respectable. Let's try to reduce dropout by a bit in both the encoder and decoder.\n",
    "\n",
    "#### Attempt 5\n",
    "```\n",
    "Dropout = 0.3 before latent layer in the encoder\n",
    "Dropout = 0.3 after latent layer in the decoder\n",
    "Best error: loss: 237.0880 - val_loss: 220.3023\n",
    "```\n",
    "\n",
    "Seems like Dropout doesn't allow the loss to be as good as the real one. But previously I mentioned the concern that our validation set is not entirely separate from the training set, so real test loss is higher. In which case, dropout would be helpful. But for now, let's remove dropout and try compressing the latent dimension even further.\n",
    "\n",
    "#### Attempt 6\n",
    "```\n",
    "Latent dim = 10\n",
    "Best error: loss: 182.0658 - val_loss: 207.2772\n",
    "```\n",
    "Latent dimension of 10 whuuuuut. And it still works as well as before! Overfitting a bit though, maybe we can add dropout to regularize it. But let's first see how far we can go in reducing the latent dimension!\n",
    "\n",
    "#### Attempt 7\n",
    "```\n",
    "Latent dim = 2\n",
    "Best error: loss: 287.9604 - val_loss: 298.7828\n",
    "```\n",
    "Clear drop in performance, loss-wise and also in the reconstructed output. It's fine, it's all good! I think a latent dimension of 10 sounds reasonable. Let's go back to 10, and put in some dropout to finish this off. :)\n",
    "\n",
    "#### Attempt 8\n",
    "```\n",
    "Latent dim = 10\n",
    "Dropout = 0.3 before latent layer in the encoder\n",
    "Dropout = 0.3 after latent layer in the decoder\n",
    "Best error: loss: 283.4628 - val_loss: 264.9663\n",
    "```\n",
    "Interestingly, the dropout really hurts performance. But this actually makes sense, especially on the decoder side. Applying 0.3 dropout on the decoder means that for our latent variables of dimension 10, we are randomly deleting 3 of those 10, which is certainly too much. Let's do one last test, and move the dropout layers to apply to the intermediate layers instead of the latent layer.\n",
    "\n",
    "#### Attempt 9\n",
    "```\n",
    "Latent dim = 10\n",
    "Dropout = 0.3 before intermediate layer in the encoder\n",
    "Dropout = 0.3 after intermediate layer in the decoder\n",
    "Best error: loss: 203.1700 - val_loss: 202.1480\n",
    "```\n",
    "Hmm this is a little worse than Attempt 6. Maybe reduce dropout to 0.1? Don't need to overtune this also, this won't be the final model yet.\n",
    "\n",
    "\n",
    "#### Attempt 10\n",
    "```\n",
    "Latent dim = 10\n",
    "Dropout = 0.1 before intermediate layer in the encoder\n",
    "Dropout = 0.1 after intermediate layer in the decoder\n",
    "Best error: loss: 186.1247 - val_loss: 190.5778\n",
    "```\n",
    "Great! Let's leave it at this for now. \n",
    "\n",
    "### Next steps\n",
    "1. Try out some latent arithmetic! Hypeeeeee."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Autoencoder portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "latent_dim = 10\n",
    "intermediate_dim = 1600\n",
    "epsilon_std = 1.0\n",
    "NUM_PITCHES = 88\n",
    "NUM_TICKS = 96\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., # latent_dim\n",
    "                              stddev=epsilon_std) # epsilon_std\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "# Encoder stuff\n",
    "input_mat = Input(shape=(NUM_PITCHES, NUM_TICKS, 1))  # 'channels_last' data format (only 1 channel in our case)\n",
    "x = Conv2D(200, (13, 21), strides=(5,5), activation='relu', padding='valid')(input_mat)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(100, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(100, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "# x = Conv2D(100, (3, 3), activation='relu', padding='same')(x)\n",
    "# x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(intermediate_dim, activation='relu')(x)\n",
    "\n",
    "# Modelling the embedding\n",
    "z_mean = Dense(latent_dim)(x)\n",
    "z_log_var = Dense(latent_dim)(x)\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var]) # \"output_shape\" not needed using tf backend\n",
    "\n",
    "# Decoder stuff\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_hid = Dense(intermediate_dim, activation='relu')\n",
    "decoder_reshape = Reshape((4, 4, 100))\n",
    "decoder_conv1 = Conv2D(100, (3, 3), activation='relu', padding='same')\n",
    "decoder_upsamp1 = UpSampling2D((2, 2))\n",
    "decoder_conv2 = Conv2D(100, (3, 3), activation='relu', padding='same')\n",
    "decoder_upsamp2 = UpSampling2D((2, 2))\n",
    "decoder_conv3 = Conv2D(200, (3, 3), activation='relu', padding='same')\n",
    "decoder_upsamp3 = UpSampling2D((2, 2))\n",
    "decoder_conv4 = Conv2DTranspose(1, (13, 21), strides=(5,5), activation='sigmoid', padding='valid')\n",
    "# now we apply the decoder layers we previously defined\n",
    "x = decoder_hid(z)\n",
    "x = Dropout(0.1)(x)\n",
    "x = decoder_reshape(x)\n",
    "# x = decoder_conv1(x)\n",
    "# x = decoder_upsamp1(x)\n",
    "x = decoder_conv2(x)\n",
    "x = decoder_upsamp2(x)\n",
    "x = decoder_conv3(x)\n",
    "x = decoder_upsamp3(x)\n",
    "x_decoded_mean = decoder_conv4(x)\n",
    "\n",
    "# instantiate VAE model\n",
    "autoencoder = Model(input_mat, x_decoded_mean)\n",
    "\n",
    "# Compute VAE loss\n",
    "def vae_loss(input_x, output_x):\n",
    "    xent_loss = NUM_PITCHES * NUM_TICKS * metrics.binary_crossentropy(K.flatten(input_x), K.flatten(output_x))\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "# def vae_loss(input_x, output_x):\n",
    "#     mse_loss = K.mean(K.square(K.flatten(input_x) - K.flatten(output_x)))\n",
    "#     kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "#     return K.mean(mse_loss + kl_loss)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss=vae_loss)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the VAE model\n",
    "autoencoder.fit(input_train, input_train,\n",
    "        epochs=50,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        validation_data=(input_test, input_test))\n",
    "\n",
    "MODEL_FILE = './models/vae_v4.h5'\n",
    "autoencoder.save(MODEL_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Encoder model\n",
    "encoder = Model(input_mat, z_mean)\n",
    "\n",
    "MODEL_FILE = './models/vae_v4_encoder.h5'\n",
    "encoder.save(MODEL_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_FILE\n",
    "\n",
    "# Decoder model\n",
    "_z = Input(shape=(latent_dim,))\n",
    "_x = decoder_hid(_z)\n",
    "_x = Dropout(0.1)(_x)\n",
    "_x = decoder_reshape(_x)\n",
    "# _x = decoder_conv1(_x)\n",
    "# _x = decoder_upsamp1(_x)\n",
    "_x = decoder_conv2(_x)\n",
    "_x = decoder_upsamp2(_x)\n",
    "_x = decoder_conv3(_x)\n",
    "_x = decoder_upsamp3(_x)\n",
    "_x_decoded_mean = decoder_conv4(_x)\n",
    "generator = Model(_z, _x_decoded_mean)\n",
    "\n",
    "MODEL_FILE = './models/vae_v4_generator.h5'\n",
    "generator.save(MODEL_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MODEL_FILE = './models/vae_v4.h5'\n",
    "autoencoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std, 'vae_loss': vae_loss})\n",
    "MODEL_FILE = './models/vae_v4_encoder.h5'\n",
    "encoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})\n",
    "MODEL_FILE = './models/vae_v4_generator.h5'\n",
    "generator = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Run test inputs through the autoencoder\n",
    "decoded_test = autoencoder.predict(input_test)\n",
    "print decoded_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Inspect a random input-output sample\n",
    "sample_index = np.random.randint(len(input_test))\n",
    "sample_input = input_test[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "sample_full = full_test[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "sample_output = decoded_test[sample_index].swapaxes(0,1).reshape(NUM_TICKS, NUM_PITCHES) * 127\n",
    "\n",
    "# Set all velocity values < threshold to zero\n",
    "sample_output[sample_output < 1] = 0\n",
    "# sample_output = np.clip(sample_output * 2, 0, 127)\n",
    "\n",
    "print(sample_input.shape)\n",
    "print(sample_full.shape)\n",
    "print(sample_output.shape)\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(1,3)\n",
    "fig.set_size_inches(10, 6, forward=True)\n",
    "ax[0].set_title('Input')\n",
    "ax[1].set_title('Target')\n",
    "ax[2].set_title('Output')\n",
    "pianoroll_utils.plot_pianoroll(ax[0], sample_input, MIN_PITCH, MAX_PITCH, beat_resolution=24, cmap='inferno')\n",
    "pianoroll_utils.plot_pianoroll(ax[1], sample_full, MIN_PITCH, MAX_PITCH, beat_resolution=24, cmap='inferno')\n",
    "pianoroll_utils.plot_pianoroll(ax[2], sample_output, MIN_PITCH, MAX_PITCH, beat_resolution=24, cmap='inferno')\n",
    "fig.tight_layout()\n",
    "\n",
    "# Play comparison\n",
    "audio0 = pianoroll_utils.play_pianoroll(sample_input.T, MIN_PITCH, MAX_PITCH, '0')\n",
    "IPython.display.display(IPython.display.Audio(audio0))\n",
    "\n",
    "audio1 = pianoroll_utils.play_pianoroll(sample_output.T, MIN_PITCH, MAX_PITCH, '1')\n",
    "IPython.display.display(IPython.display.Audio(audio1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Variational Autoencoder V5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`code given below`\n",
    "\n",
    "### Details\n",
    "\n",
    "Try to incorporate tips from Mason:\n",
    "- Change kernel size\n",
    "- Add noise, force the autoencoder to be de-noising\n",
    "- Increase latent dimension (optional)\n",
    "\n",
    "In the spirit of changing kernel size to use musical heuristics, go from 88 keys to 96 keys so we can use a 12-key kernel size.\n",
    "\n",
    "### Notes\n",
    "\n",
    "#### Attempt 1\n",
    "```\n",
    "Latent dim = 500\n",
    "Intermediate_dim = 1000\n",
    "Best error: loss: 184.6203 - val_loss: 186.5465 (after 50 epochs)\n",
    "```\n",
    "The error to beat from previous version was `loss: 186.1247 - val_loss: 190.5778`. And we've narrowly succeeded!\n",
    "\n",
    "More importantly, the reconstructions appear much less \"blurry\" in the time-axis. Success! \n",
    "\n",
    "The next step now is to add noise to the input to make it a de-noising autoencoder.\n",
    "\n",
    "#### Attempt 2\n",
    "```\n",
    "Latent dim = 500\n",
    "Intermediate_dim = 1000\n",
    "Best error: loss: 219.3455 - val_loss: 355.9712 (after 50 epochs)\n",
    "```\n",
    "The de-noising autoencoder actually works remarkably well! The downside is that we get a lot more ghost notes, but the VAE actually works quite hard to put in more content, and I think most of the time that content is actually musically sensible! (Adding harmony notes etc)\n",
    "\n",
    "We note that the validation loss is much poorer than training loss, but this is to be expected - it's impossible to accurately reconstruct lost notes because there is no one right answer. But we have to be careful about trying to balance our losses, and also want to learn such that the model is confident about good notes, and removes all ghost notes.\n",
    "\n",
    "At this point, something worth thinking about: Instead of expecting the model the remove ghost notes completely, **we could move towards the idea of sampling from the output to create the sounds** instead of directly using the outputs. The current way of taking the mean of contiguous velocities is definitely lacklustre.\n",
    "\n",
    "### Next steps\n",
    "1. Explore latent arithmetic some more.\n",
    "2. Look into sampling from the output.\n",
    "3. Try out the prediction task.\n",
    "4. If all of that doesn't work, go back to word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "latent_dim = 500\n",
    "intermediate_dim = 1000\n",
    "epsilon_std = 1.0\n",
    "NUM_PITCHES = 96\n",
    "NUM_TICKS = 96\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., # latent_dim\n",
    "                              stddev=epsilon_std) # epsilon_std\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "kernel1 = (12, 1)\n",
    "kernel2 = (8, 4)\n",
    "kernel3 = (1, 4)\n",
    "# Encoder stuff\n",
    "input_mat = Input(shape=(NUM_PITCHES, NUM_TICKS, 1))  # 'channels_last' data format (only 1 channel in our case)\n",
    "x = Conv2D(50, kernel1, strides=kernel1, activation='relu', padding='valid')(input_mat)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(200, kernel2, strides=kernel2, activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(500, kernel3, strides=kernel3, activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(intermediate_dim, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "# Modelling the embedding\n",
    "z_mean = Dense(latent_dim)(x)\n",
    "z_log_var = Dense(latent_dim)(x)\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var]) # \"output_shape\" not needed using tf backend\n",
    "\n",
    "# Decoder stuff\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_hid = Dense(intermediate_dim, activation='relu')\n",
    "decoder_batchnorm0 = BatchNormalization()\n",
    "decoder_conv1_flat = Dense(3000, activation='relu')\n",
    "decoder_reshape = Reshape((1, 6, 500))\n",
    "decoder_batchnorm1 = BatchNormalization()\n",
    "decoder_conv1 = Conv2DTranspose(200, kernel3, strides=kernel3, activation='relu', padding='valid')\n",
    "decoder_batchnorm2 = BatchNormalization()\n",
    "decoder_conv2 = Conv2DTranspose(50, kernel2, strides=kernel2, activation='relu', padding='valid')\n",
    "decoder_batchnorm3 = BatchNormalization()\n",
    "decoder_conv3 = Conv2DTranspose(1, kernel1, strides=kernel1, activation='sigmoid', padding='valid')\n",
    "# now we apply the decoder layers we previously defined\n",
    "x = decoder_hid(z)\n",
    "x = decoder_batchnorm0(x)\n",
    "x = decoder_conv1_flat(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = decoder_batchnorm1(x)\n",
    "x = decoder_reshape(x)\n",
    "x = decoder_conv1(x)\n",
    "x = decoder_batchnorm2(x)\n",
    "x = decoder_conv2(x)\n",
    "x = decoder_batchnorm3(x)\n",
    "x_decoded_mean = decoder_conv3(x)\n",
    "\n",
    "# instantiate VAE model\n",
    "autoencoder = Model(input_mat, x_decoded_mean)\n",
    "\n",
    "# Compute VAE loss\n",
    "def vae_loss(input_x, output_x):\n",
    "    xent_loss = NUM_PITCHES * NUM_TICKS * metrics.binary_crossentropy(K.flatten(input_x), K.flatten(output_x))\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss=vae_loss)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the VAE model\n",
    "autoencoder.fit(input_train, input_train,\n",
    "        epochs=50,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        validation_data=(input_test, input_test))\n",
    "\n",
    "MODEL_FILE = './models/vae_v5.h5'\n",
    "autoencoder.save(MODEL_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the denoising-VAE model\n",
    "autoencoder.fit(input_train_noisy, input_train,\n",
    "        epochs=50,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        validation_data=(input_test_noisy, input_test))\n",
    "\n",
    "MODEL_FILE = './models/vae_v5_denoise.h5'\n",
    "autoencoder.save(MODEL_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Encoder model\n",
    "encoder = Model(input_mat, z_mean)\n",
    "\n",
    "MODEL_FILE = './models/vae_v5_encoder_denoise.h5'\n",
    "encoder.save(MODEL_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_FILE\n",
    "\n",
    "# Decoder model\n",
    "_z = Input(shape=(latent_dim,))\n",
    "_x = decoder_hid(_z)\n",
    "_x = decoder_batchnorm0(_x)\n",
    "_x = decoder_conv1_flat(_x)\n",
    "_x = Dropout(0.1)(_x)\n",
    "_x = decoder_batchnorm1(_x)\n",
    "_x = decoder_reshape(_x)\n",
    "_x = decoder_conv1(_x)\n",
    "_x = decoder_batchnorm2(_x)\n",
    "_x = decoder_conv2(_x)\n",
    "_x = decoder_batchnorm3(_x)\n",
    "_x_decoded_mean = decoder_conv3(_x)\n",
    "generator = Model(_z, _x_decoded_mean)\n",
    "\n",
    "MODEL_FILE = './models/vae_v5_generator_denoise.h5'\n",
    "generator.save(MODEL_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MODEL_FILE = './models/vae_v5_denoise.h5'\n",
    "autoencoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std, 'vae_loss': vae_loss})\n",
    "MODEL_FILE = './models/vae_v5_encoder_denoise.h5'\n",
    "encoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})\n",
    "MODEL_FILE = './models/vae_v5_generator_denoise.h5'\n",
    "generator = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Variational Autoencoder V6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`code given below`\n",
    "\n",
    "### Details\n",
    "\n",
    "Try adding weight parameter, `beta`, to control the trade-off between the reconstruction error and KL-divergence in the loss function. `beta` will be attached as a coefficient for KL-divergence, so `beta > 1` places more emphasis on following the normal distribution, whereas `beta < 1` places more emphasis on accurate reconstruction.\n",
    "\n",
    "### Notes\n",
    "\n",
    "#### Attempt 1\n",
    "```\n",
    "beta = 0.5; After 10 epochs, loss: 362.0917 - val_loss: 377.1780\n",
    "beta = 1.0; After 10 epochs, loss: 416.0928 - val_loss: 442.7350\n",
    "beta = 1.1; After 10 epochs, loss: 428.9010 - val_loss: 427.2751\n",
    "beta = 1.5; After 50 epochs, loss: 291.6167 - val_loss: 312.1719\n",
    "```\n",
    "The error to beat from previous version was `219.3455 - val_loss: 355.9712`. \n",
    "\n",
    "\n",
    "### Next steps\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beta = 1.5\n",
    "latent_dim = 500\n",
    "intermediate_dim = 1000\n",
    "epsilon_std = 1.0\n",
    "NUM_PITCHES = 96\n",
    "NUM_TICKS = 96\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., # latent_dim\n",
    "                              stddev=epsilon_std) # epsilon_std\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "kernel1 = (12, 1)\n",
    "kernel2 = (8, 4)\n",
    "kernel3 = (1, 4)\n",
    "# Encoder stuff\n",
    "input_mat = Input(shape=(NUM_PITCHES, NUM_TICKS, 1))  # 'channels_last' data format (only 1 channel in our case)\n",
    "x = Conv2D(50, kernel1, strides=kernel1, activation='relu', padding='valid')(input_mat)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(200, kernel2, strides=kernel2, activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(500, kernel3, strides=kernel3, activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(intermediate_dim, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "# Modelling the embedding\n",
    "z_mean = Dense(latent_dim)(x)\n",
    "z_log_var = Dense(latent_dim)(x)\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var]) # \"output_shape\" not needed using tf backend\n",
    "\n",
    "# Decoder stuff\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_hid = Dense(intermediate_dim, activation='relu')\n",
    "decoder_batchnorm0 = BatchNormalization()\n",
    "decoder_conv1_flat = Dense(3000, activation='relu')\n",
    "decoder_reshape = Reshape((1, 6, 500))\n",
    "decoder_batchnorm1 = BatchNormalization()\n",
    "decoder_conv1 = Conv2DTranspose(200, kernel3, strides=kernel3, activation='relu', padding='valid')\n",
    "decoder_batchnorm2 = BatchNormalization()\n",
    "decoder_conv2 = Conv2DTranspose(50, kernel2, strides=kernel2, activation='relu', padding='valid')\n",
    "decoder_batchnorm3 = BatchNormalization()\n",
    "decoder_conv3 = Conv2DTranspose(1, kernel1, strides=kernel1, activation='sigmoid', padding='valid')\n",
    "# now we apply the decoder layers we previously defined\n",
    "x = decoder_hid(z)\n",
    "x = decoder_batchnorm0(x)\n",
    "x = decoder_conv1_flat(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = decoder_batchnorm1(x)\n",
    "x = decoder_reshape(x)\n",
    "x = decoder_conv1(x)\n",
    "x = decoder_batchnorm2(x)\n",
    "x = decoder_conv2(x)\n",
    "x = decoder_batchnorm3(x)\n",
    "x_decoded_mean = decoder_conv3(x)\n",
    "\n",
    "# instantiate VAE model\n",
    "autoencoder = Model(input_mat, x_decoded_mean)\n",
    "\n",
    "# Compute VAE loss\n",
    "def vae_loss(input_x, output_x):\n",
    "    xent_loss = NUM_PITCHES * NUM_TICKS * metrics.binary_crossentropy(K.flatten(input_x), K.flatten(output_x))\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return K.mean(xent_loss + beta * kl_loss)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss=vae_loss)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the denoising-VAE model\n",
    "autoencoder.fit(input_train_noisy, input_train,\n",
    "        epochs=50,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        validation_data=(input_test_noisy, input_test))\n",
    "\n",
    "MODEL_FILE = './models/vae_v6.h5'\n",
    "autoencoder.save(MODEL_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Encoder model\n",
    "encoder = Model(input_mat, z_mean)\n",
    "\n",
    "MODEL_FILE = './models/vae_v6_encoder.h5'\n",
    "encoder.save(MODEL_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_FILE\n",
    "\n",
    "# Decoder model\n",
    "_z = Input(shape=(latent_dim,))\n",
    "_x = decoder_hid(_z)\n",
    "_x = decoder_batchnorm0(_x)\n",
    "_x = decoder_conv1_flat(_x)\n",
    "_x = Dropout(0.1)(_x)\n",
    "_x = decoder_batchnorm1(_x)\n",
    "_x = decoder_reshape(_x)\n",
    "_x = decoder_conv1(_x)\n",
    "_x = decoder_batchnorm2(_x)\n",
    "_x = decoder_conv2(_x)\n",
    "_x = decoder_batchnorm3(_x)\n",
    "_x_decoded_mean = decoder_conv3(_x)\n",
    "generator = Model(_z, _x_decoded_mean)\n",
    "\n",
    "MODEL_FILE = './models/vae_v6_generator.h5'\n",
    "generator.save(MODEL_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MODEL_FILE = './models/vae_v6.h5'\n",
    "autoencoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std, 'vae_loss': vae_loss})\n",
    "MODEL_FILE = './models/vae_v6.h5'\n",
    "encoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})\n",
    "MODEL_FILE = './models/vae_v6.h5'\n",
    "generator = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Variational Autoencoder V7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`code given below`\n",
    "\n",
    "### Details\n",
    "\n",
    "First model with new memory-mapped HDF5 dataset (to allow for giant datasets that don't fit in RAM). \n",
    "\n",
    "### Notes\n",
    "\n",
    "#### Attempt 1\n",
    "```\n",
    "Num pianorolls: 1000\n",
    "Train: (204908, 96, 96, 1)\n",
    "Test: (23796, 96, 96, 1)\n",
    "\n",
    "Batch size: 128\n",
    "Best error: loss: 249.6694 - val_loss: 220.9366\n",
    "Time/step: 154s 753us/step\n",
    "\n",
    "[Continued]\n",
    "Batch size: 4096\n",
    "Best error: loss: 209.0271 - val_loss: 199.2386\n",
    "Time/step: 121s 589us/step\n",
    "\n",
    "[Continued]\n",
    "Batch size: 1024\n",
    "Best error: loss: 201.6758 - val_loss: 190.1329\n",
    "Time/step: 123s 603us/step\n",
    "```\n",
    "The error to beat from previous version was `loss: 184.6203 - val_loss: 186.5465 (after 50 epochs)` from vae_v5 (without de-noising).\n",
    "\n",
    "Looks very promising! Reconstruction doesn't seem to be any better - and I think that V6 handled short-note reconstructions better than this, but random sampling from the latent space seems to be much better!!\n",
    "\n",
    "#### Attempt 2\n",
    "```\n",
    "Num pianorolls: 1000\n",
    "Train: (220409, 96, 96, 1)\n",
    "Noisy train: (220409, 96, 96, 1)\n",
    "Test: (20426, 96, 96, 1)\n",
    "Best error: loss: 262.9233 - val_loss: 202.1109 (after 50 epochs)\n",
    "Time/step: 146s 661us/step\n",
    "```\n",
    "\n",
    "In this attempt, we have added a noisy input training set, (but not for validation). Also added 5% empty units to encourage it to learn silence correctly.\n",
    "\n",
    "The error to beat from the vae_v5 (denoising) was `loss: 219.3455 - val_loss: 355.9712 (after 50 epochs)`.\n",
    "\n",
    "### Next steps\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beta = 1.0\n",
    "latent_dim = 500\n",
    "intermediate_dim = 1000\n",
    "epsilon_std = 1.0\n",
    "NUM_PITCHES = 96\n",
    "NUM_TICKS = 96\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., # latent_dim\n",
    "                              stddev=epsilon_std) # epsilon_std\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "kernel1 = (12, 1)\n",
    "kernel2 = (8, 4)\n",
    "kernel3 = (1, 4)\n",
    "# Encoder stuff\n",
    "input_mat = Input(shape=(NUM_PITCHES, NUM_TICKS, 1))  # 'channels_last' data format (only 1 channel in our case)\n",
    "x = Conv2D(50, kernel1, strides=kernel1, activation='relu', padding='valid')(input_mat)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(200, kernel2, strides=kernel2, activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(500, kernel3, strides=kernel3, activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(intermediate_dim, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "# Modelling the embedding\n",
    "z_mean = Dense(latent_dim)(x)\n",
    "z_log_var = Dense(latent_dim)(x)\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var]) # \"output_shape\" not needed using tf backend\n",
    "\n",
    "# Decoder stuff\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_hid = Dense(intermediate_dim, activation='relu')\n",
    "decoder_batchnorm0 = BatchNormalization()\n",
    "decoder_conv1_flat = Dense(3000, activation='relu')\n",
    "decoder_reshape = Reshape((1, 6, 500))\n",
    "decoder_batchnorm1 = BatchNormalization()\n",
    "decoder_conv1 = Conv2DTranspose(200, kernel3, strides=kernel3, activation='relu', padding='valid')\n",
    "decoder_batchnorm2 = BatchNormalization()\n",
    "decoder_conv2 = Conv2DTranspose(50, kernel2, strides=kernel2, activation='relu', padding='valid')\n",
    "decoder_batchnorm3 = BatchNormalization()\n",
    "decoder_conv3 = Conv2DTranspose(1, kernel1, strides=kernel1, activation='sigmoid', padding='valid')\n",
    "# now we apply the decoder layers we previously defined\n",
    "x = decoder_hid(z)\n",
    "x = decoder_batchnorm0(x)\n",
    "x = decoder_conv1_flat(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = decoder_batchnorm1(x)\n",
    "x = decoder_reshape(x)\n",
    "x = decoder_conv1(x)\n",
    "x = decoder_batchnorm2(x)\n",
    "x = decoder_conv2(x)\n",
    "x = decoder_batchnorm3(x)\n",
    "x_decoded_mean = decoder_conv3(x)\n",
    "\n",
    "# instantiate VAE model\n",
    "autoencoder = Model(input_mat, x_decoded_mean)\n",
    "\n",
    "# Compute VAE loss\n",
    "def vae_loss(input_x, output_x):\n",
    "    xent_loss = NUM_PITCHES * NUM_TICKS * metrics.binary_crossentropy(K.flatten(input_x), K.flatten(output_x))\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return K.mean(xent_loss + beta * kl_loss)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss=vae_loss)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the denoising-VAE model\n",
    "autoencoder.fit(input_train_noisy, input_train,\n",
    "        epochs=50,\n",
    "        batch_size=1024,\n",
    "        shuffle=\"batch\",\n",
    "        validation_data=(input_test, input_test))\n",
    "\n",
    "MODEL_FILE = './models/vae_v7.h5'\n",
    "autoencoder.save(MODEL_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Encoder model\n",
    "encoder = Model(input_mat, z_mean)\n",
    "\n",
    "MODEL_FILE = './models/vae_v7_encoder.h5'\n",
    "encoder.save(MODEL_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_FILE\n",
    "\n",
    "# Decoder model\n",
    "_z = Input(shape=(latent_dim,))\n",
    "_x = decoder_hid(_z)\n",
    "_x = decoder_batchnorm0(_x)\n",
    "_x = decoder_conv1_flat(_x)\n",
    "_x = Dropout(0.1)(_x)\n",
    "_x = decoder_batchnorm1(_x)\n",
    "_x = decoder_reshape(_x)\n",
    "_x = decoder_conv1(_x)\n",
    "_x = decoder_batchnorm2(_x)\n",
    "_x = decoder_conv2(_x)\n",
    "_x = decoder_batchnorm3(_x)\n",
    "_x_decoded_mean = decoder_conv3(_x)\n",
    "generator = Model(_z, _x_decoded_mean)\n",
    "\n",
    "MODEL_FILE = './models/vae_v7_decoder.h5'\n",
    "generator.save(MODEL_FILE)# creates a HDF5 file\n",
    "print \"Saved Keras model to\", MODEL_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MODEL_FILE = './models/vae_v7.h5'\n",
    "autoencoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std, 'vae_loss': vae_loss})\n",
    "MODEL_FILE = './models/vae_v7_encoder.h5'\n",
    "encoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})\n",
    "MODEL_FILE = './models/vae_v7_decoder.h5'\n",
    "decoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Variational Autoencoder V7 - Unit Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`code given below`\n",
    "\n",
    "### Details\n",
    "\n",
    "The focus of this model is to test nearest-neighbour unit selection with the model we built in VAEV7.\n",
    "\n",
    "### Notes\n",
    "\n",
    "\n",
    "### Next steps\n",
    "1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "latent_dim = 500\n",
    "epsilon_std = 1.0\n",
    "\n",
    "MODEL_FILE = './models/vae_v7_encoder.h5'\n",
    "encoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})\n",
    "MODEL_FILE = './models/vae_v7_decoder.h5'\n",
    "decoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Build a dataset of embeddings of our input units\n",
    "input_train_encoded = encoder.predict(input_train)\n",
    "print(input_train_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Training the nearest neighbors model\n",
    "knn_model = NearestNeighbors(n_neighbors=1).fit(input_train_encoded)\n",
    "\n",
    "# Save model along with the units used to learn the model\n",
    "KNN_MODEL_FILENAME = \"./models/vae_v7_unit_selector_knn.pkl\"\n",
    "sklearn.externals.joblib.dump((knn_model, UNITS_FILE), KNN_MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "KNN_MODEL_FILENAME = \"./models/vae_v7_unit_selector_knn.pkl\"\n",
    "knn_model, UNITS_FILE = sklearn.externals.joblib.load(KNN_MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Testing the model with training data!\n",
    "\n",
    "# Choose a sample input to find the nearest neighbor of \n",
    "# (in this case, the 1nn should be itself, or a unit which is almost identical to itself)\n",
    "sample_index = np.random.randint(len(input_test))\n",
    "sample_input = input_test[sample_index][np.newaxis, ...]\n",
    "sample_encoded = encoder.predict(sample_input)\n",
    "\n",
    "# kNN Prediction\n",
    "tic = time.time()\n",
    "knn_index = knn_model.kneighbors(sample_encoded, return_distance = False)[0][0]\n",
    "toc = time.time() - tic\n",
    "print(\"Prediction took \", toc)\n",
    "print sample_index, knn_index\n",
    "\n",
    "sample_input = input_test[sample_index].squeeze()\n",
    "sample_decoded = decoder.predict(sample_encoded).squeeze()\n",
    "sample_knn = input_train[knn_index].squeeze()\n",
    "\n",
    "print(sample_input.shape)\n",
    "print(sample_knn.shape)\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(3,1)\n",
    "fig.set_size_inches(10, 9, forward=True)\n",
    "ax[0].set_title('Input')\n",
    "ax[1].set_title('Decoded')\n",
    "ax[2].set_title('Nearest Neighbour')\n",
    "pianoroll_utils.plot_pianoroll(ax[0], sample_input, MIN_PITCH, MAX_PITCH, beat_resolution=24, cmap='inferno')\n",
    "pianoroll_utils.plot_pianoroll(ax[1], sample_decoded, MIN_PITCH, MAX_PITCH, beat_resolution=24, cmap='inferno')\n",
    "pianoroll_utils.plot_pianoroll(ax[2], sample_knn, MIN_PITCH, MAX_PITCH, beat_resolution=24, cmap='inferno')\n",
    "fig.tight_layout()\n",
    "\n",
    "# Play comparison\n",
    "pianoroll_utils.play_pianoroll(sample_input, MIN_PITCH, MAX_PITCH, '0')\n",
    "pianoroll_utils.play_pianoroll(sample_decoded, MIN_PITCH, MAX_PITCH, '1')\n",
    "pianoroll_utils.play_pianoroll(sample_knn, MIN_PITCH, MAX_PITCH, '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder V8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`code given below`\n",
    "\n",
    "### Details\n",
    "\n",
    "Building on top of V7, with some new considerations after conversation with Tobi:\n",
    "- Reduce number of trainable parameters\n",
    "- Reduce latent dimensions\n",
    "- Custom loss functions\n",
    "\n",
    "### Notes\n",
    "\n",
    "#### Attempt 0 (for comparison)\n",
    "```\n",
    "Num pianorolls: 200\n",
    "Train: (38012, 96, 96, 1)\n",
    "Noisy train: (38012, 96, 96, 1)\n",
    "Test: (3047, 96, 96, 1)\n",
    "\n",
    "Latent dimensions: 500\n",
    "Trainable params: 8,960,201\n",
    "\n",
    "Batch size: 1024\n",
    "Best error: \n",
    "Average IOU: \n",
    "Average Onset loss:\n",
    "Average smoothness loss:\n",
    "```\n",
    "Duplicate of vae_v5 (denoising) just for comparison. The error rates will not be directly comparable since we are changing the loss term, but can be used for a ballpark idea.\n",
    "\n",
    "\n",
    "### Next steps\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 96, 96, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 96, 50)    650         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 8, 96, 50)    200         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 1, 24, 200)   320200      batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 1, 24, 200)   800         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 1, 6, 500)    400500      batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 1, 6, 500)    2000        conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 3000)         0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 3000)         0           flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 1000)         3001000     dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 1000)         4000        dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 500)          500500      batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 500)          500500      batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 500)          0           dense_27[0][0]                   \n",
      "                                                                 dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 1000)         501000      lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 1000)         4000        dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 3000)         3003000     batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 3000)         0           dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 3000)         12000       dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 1, 6, 500)    0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_16 (Conv2DTran (None, 1, 24, 200)   400200      reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 1, 24, 200)   800         conv2d_transpose_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_17 (Conv2DTran (None, 8, 96, 50)    320050      batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 8, 96, 50)    200         conv2d_transpose_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_18 (Conv2DTran (None, 96, 96, 1)    601         batch_normalization_48[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 8,972,201\n",
      "Trainable params: 8,960,201\n",
      "Non-trainable params: 12,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "beta = 1.0\n",
    "latent_dim = 500\n",
    "intermediate_dim = 1000\n",
    "epsilon_std = 1.0\n",
    "NUM_PITCHES = 96\n",
    "NUM_TICKS = 96\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., # latent_dim\n",
    "                              stddev=epsilon_std) # epsilon_std\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "kernel1 = (12, 1)\n",
    "kernel2 = (8, 4)\n",
    "kernel3 = (1, 4)\n",
    "# Encoder stuff\n",
    "input_mat = Input(shape=(NUM_PITCHES, NUM_TICKS, 1))  # 'channels_last' data format (only 1 channel in our case)\n",
    "x = Conv2D(50, kernel1, strides=kernel1, activation='relu', padding='valid')(input_mat)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(200, kernel2, strides=kernel2, activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(500, kernel3, strides=kernel3, activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(intermediate_dim, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "# Modelling the embedding\n",
    "z_mean = Dense(latent_dim)(x)\n",
    "z_log_var = Dense(latent_dim)(x)\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var]) # \"output_shape\" not needed using tf backend\n",
    "\n",
    "# Decoder stuff\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_hid = Dense(intermediate_dim, activation='relu')\n",
    "decoder_batchnorm0 = BatchNormalization()\n",
    "decoder_conv1_flat = Dense(3000, activation='relu')\n",
    "decoder_reshape = Reshape((1, 6, 500))\n",
    "decoder_batchnorm1 = BatchNormalization()\n",
    "decoder_conv1 = Conv2DTranspose(200, kernel3, strides=kernel3, activation='relu', padding='valid')\n",
    "decoder_batchnorm2 = BatchNormalization()\n",
    "decoder_conv2 = Conv2DTranspose(50, kernel2, strides=kernel2, activation='relu', padding='valid')\n",
    "decoder_batchnorm3 = BatchNormalization()\n",
    "decoder_conv3 = Conv2DTranspose(1, kernel1, strides=kernel1, activation='sigmoid', padding='valid')\n",
    "# now we apply the decoder layers we previously defined\n",
    "x = decoder_hid(z)\n",
    "x = decoder_batchnorm0(x)\n",
    "x = decoder_conv1_flat(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = decoder_batchnorm1(x)\n",
    "x = decoder_reshape(x)\n",
    "x = decoder_conv1(x)\n",
    "x = decoder_batchnorm2(x)\n",
    "x = decoder_conv2(x)\n",
    "x = decoder_batchnorm3(x)\n",
    "x_decoded_mean = decoder_conv3(x)\n",
    "\n",
    "# instantiate VAE model\n",
    "autoencoder = Model(input_mat, x_decoded_mean)\n",
    "\n",
    "# Compute VAE loss\n",
    "# def vae_loss(input_x, output_x):\n",
    "#     xent_loss = NUM_PITCHES * NUM_TICKS * metrics.binary_crossentropy(K.flatten(input_x), K.flatten(output_x))\n",
    "#     kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "#     return K.mean(xent_loss + beta * kl_loss)\n",
    "\n",
    "def vae_loss(input_x, output_x):\n",
    "#     smooth_loss = custom_loss.smoothness_loss(output_x)\n",
    "    xent_loss = NUM_PITCHES * NUM_TICKS * metrics.binary_crossentropy(K.flatten(input_x), K.flatten(output_x))\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    \n",
    "    return K.mean(xent_loss + beta * kl_loss) #+ smooth_loss\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss=vae_loss, metrics=[custom_loss.smoothness_metric])\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38012 samples, validate on 3047 samples\n",
      "Epoch 1/50\n",
      "38012/38012 [==============================] - 33s 864us/step - loss: 6319.8992 - smoothness_metric: 1.0000 - val_loss: 7196.4207 - val_smoothness_metric: 0.9999\n",
      "Epoch 2/50\n",
      "38012/38012 [==============================] - 29s 758us/step - loss: 5281.7454 - smoothness_metric: 1.0000 - val_loss: 5653.4173 - val_smoothness_metric: 0.9998\n",
      "Epoch 3/50\n",
      "38012/38012 [==============================] - 29s 759us/step - loss: 3544.9334 - smoothness_metric: 1.0000 - val_loss: 2940.9304 - val_smoothness_metric: 1.0000\n",
      "Epoch 4/50\n",
      "38012/38012 [==============================] - 29s 764us/step - loss: 2001.5852 - smoothness_metric: 1.0000 - val_loss: 1710.7914 - val_smoothness_metric: 1.0000\n",
      "Epoch 5/50\n",
      "38012/38012 [==============================] - 29s 765us/step - loss: 1183.3231 - smoothness_metric: 1.0000 - val_loss: 1396.0916 - val_smoothness_metric: 1.0000\n",
      "Epoch 6/50\n",
      "38012/38012 [==============================] - 29s 769us/step - loss: 825.6480 - smoothness_metric: 1.0000 - val_loss: 1009.5068 - val_smoothness_metric: 1.0000\n",
      "Epoch 7/50\n",
      "38012/38012 [==============================] - 29s 769us/step - loss: 649.4918 - smoothness_metric: 1.0000 - val_loss: 743.0924 - val_smoothness_metric: 1.0000\n",
      "Epoch 8/50\n",
      "38012/38012 [==============================] - 29s 767us/step - loss: 568.1275 - smoothness_metric: 1.0000 - val_loss: 988.3529 - val_smoothness_metric: 1.0000\n",
      "Epoch 9/50\n",
      "38012/38012 [==============================] - 29s 761us/step - loss: 517.5626 - smoothness_metric: 1.0000 - val_loss: 605.6199 - val_smoothness_metric: 1.0000\n",
      "Epoch 10/50\n",
      "38012/38012 [==============================] - 29s 767us/step - loss: 483.5933 - smoothness_metric: 1.0000 - val_loss: 724.1381 - val_smoothness_metric: 1.0000\n",
      "Epoch 11/50\n",
      "38012/38012 [==============================] - 29s 765us/step - loss: 461.5813 - smoothness_metric: 1.0000 - val_loss: 676.1272 - val_smoothness_metric: 1.0000\n",
      "Epoch 12/50\n",
      "38012/38012 [==============================] - 29s 758us/step - loss: 450.0650 - smoothness_metric: 1.0000 - val_loss: 549.9703 - val_smoothness_metric: 1.0000\n",
      "Epoch 13/50\n",
      "38012/38012 [==============================] - 29s 766us/step - loss: 436.8677 - smoothness_metric: 1.0000 - val_loss: 720.7771 - val_smoothness_metric: 1.0000\n",
      "Epoch 14/50\n",
      "38012/38012 [==============================] - 29s 763us/step - loss: 425.2298 - smoothness_metric: 1.0000 - val_loss: 550.4338 - val_smoothness_metric: 1.0000\n",
      "Epoch 15/50\n",
      "38012/38012 [==============================] - 29s 763us/step - loss: 417.8902 - smoothness_metric: 1.0000 - val_loss: 515.8246 - val_smoothness_metric: 1.0000\n",
      "Epoch 16/50\n",
      "38012/38012 [==============================] - 29s 760us/step - loss: 414.5986 - smoothness_metric: 1.0000 - val_loss: 572.1324 - val_smoothness_metric: 1.0000\n",
      "Epoch 17/50\n",
      "38012/38012 [==============================] - 29s 757us/step - loss: 407.9111 - smoothness_metric: 1.0000 - val_loss: 505.4350 - val_smoothness_metric: 1.0000\n",
      "Epoch 18/50\n",
      "38012/38012 [==============================] - 29s 760us/step - loss: 402.5090 - smoothness_metric: 1.0000 - val_loss: 512.3965 - val_smoothness_metric: 1.0000\n",
      "Epoch 19/50\n",
      "38012/38012 [==============================] - 29s 763us/step - loss: 396.8026 - smoothness_metric: 1.0000 - val_loss: 496.1803 - val_smoothness_metric: 1.0000\n",
      "Epoch 20/50\n",
      "38012/38012 [==============================] - 29s 775us/step - loss: 390.2427 - smoothness_metric: 1.0000 - val_loss: 497.0706 - val_smoothness_metric: 1.0000\n",
      "Epoch 21/50\n",
      "38012/38012 [==============================] - 29s 763us/step - loss: 386.8493 - smoothness_metric: 1.0000 - val_loss: 497.3573 - val_smoothness_metric: 1.0000\n",
      "Epoch 22/50\n",
      "38012/38012 [==============================] - 29s 765us/step - loss: 384.7866 - smoothness_metric: 1.0000 - val_loss: 476.2593 - val_smoothness_metric: 1.0000\n",
      "Epoch 23/50\n",
      "38012/38012 [==============================] - 29s 768us/step - loss: 381.4452 - smoothness_metric: 1.0000 - val_loss: 503.7105 - val_smoothness_metric: 1.0000\n",
      "Epoch 24/50\n",
      "38012/38012 [==============================] - 29s 767us/step - loss: 378.1242 - smoothness_metric: 1.0000 - val_loss: 470.9273 - val_smoothness_metric: 1.0000\n",
      "Epoch 25/50\n",
      "38012/38012 [==============================] - 29s 766us/step - loss: 374.7163 - smoothness_metric: 1.0000 - val_loss: 472.5099 - val_smoothness_metric: 1.0000\n",
      "Epoch 26/50\n",
      "38012/38012 [==============================] - 29s 762us/step - loss: 372.0925 - smoothness_metric: 1.0000 - val_loss: 479.1520 - val_smoothness_metric: 1.0000\n",
      "Epoch 27/50\n",
      "38012/38012 [==============================] - 29s 763us/step - loss: 370.7605 - smoothness_metric: 1.0000 - val_loss: 465.2058 - val_smoothness_metric: 1.0000\n",
      "Epoch 28/50\n",
      "38012/38012 [==============================] - 29s 766us/step - loss: 367.5081 - smoothness_metric: 1.0000 - val_loss: 468.1960 - val_smoothness_metric: 1.0000\n",
      "Epoch 29/50\n",
      "38012/38012 [==============================] - 29s 764us/step - loss: 365.3117 - smoothness_metric: 1.0000 - val_loss: 457.7085 - val_smoothness_metric: 1.0000\n",
      "Epoch 30/50\n",
      "38012/38012 [==============================] - 29s 766us/step - loss: 361.9754 - smoothness_metric: 1.0000 - val_loss: 468.5890 - val_smoothness_metric: 1.0000\n",
      "Epoch 31/50\n",
      "38012/38012 [==============================] - 29s 758us/step - loss: 362.1320 - smoothness_metric: 1.0000 - val_loss: 452.5951 - val_smoothness_metric: 1.0000\n",
      "Epoch 32/50\n",
      "38012/38012 [==============================] - 29s 760us/step - loss: 358.4876 - smoothness_metric: 1.0000 - val_loss: 442.2974 - val_smoothness_metric: 1.0000\n",
      "Epoch 33/50\n",
      "34816/38012 [==========================>...] - ETA: 2s - loss: 369.0107 - smoothness_metric: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b2515db0dd31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'# Train the denoising-VAE model\\nautoencoder.fit(input_train_noisy, input_train,\\n        epochs=50,\\n        batch_size=1024,\\n        shuffle=\"batch\",#True,\\n        validation_data=(input_test, input_test))\\n\\nMODEL_FILE = \\'./models/vae_v8.h5\\'\\nautoencoder.save(MODEL_FILE)# creates a HDF5 file\\nprint(\"Saved Keras model to\", MODEL_FILE)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/chanjunshern/tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/home/chanjunshern/tensorflow/local/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/chanjunshern/tensorflow/local/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/chanjunshern/tensorflow/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/chanjunshern/tensorflow/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1219\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m                             \u001b[0;31m# Do not slice the training phase flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m                             \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m                             \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/chanjunshern/tensorflow/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_slice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/chanjunshern/tensorflow/local/lib/python2.7/site-packages/h5py/_hl/dataset.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mmspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0mfspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdxpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dxpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;31m# Patch up the output for NumPy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train the denoising-VAE model\n",
    "autoencoder.fit(input_train_noisy, input_train,\n",
    "        epochs=50,\n",
    "        batch_size=1024,\n",
    "        shuffle=\"batch\",#True,\n",
    "        validation_data=(input_test, input_test))\n",
    "\n",
    "MODEL_FILE = './models/vae_v8.h5'\n",
    "autoencoder.save(MODEL_FILE)# creates a HDF5 file\n",
    "print(\"Saved Keras model to\", MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder model\n",
    "encoder = Model(input_mat, z_mean)\n",
    "\n",
    "MODEL_FILE = './models/vae_v8_encoder.h5'\n",
    "encoder.save(MODEL_FILE)# creates a HDF5 file\n",
    "print(\"Saved Keras model to\", MODEL_FILE)\n",
    "\n",
    "# Decoder model\n",
    "_z = Input(shape=(latent_dim,))\n",
    "_x = decoder_hid(_z)\n",
    "_x = decoder_batchnorm0(_x)\n",
    "_x = decoder_conv1_flat(_x)\n",
    "_x = Dropout(0.1)(_x)\n",
    "_x = decoder_batchnorm1(_x)\n",
    "_x = decoder_reshape(_x)\n",
    "_x = decoder_conv1(_x)\n",
    "_x = decoder_batchnorm2(_x)\n",
    "_x = decoder_conv2(_x)\n",
    "_x = decoder_batchnorm3(_x)\n",
    "_x_decoded_mean = decoder_conv3(_x)\n",
    "generator = Model(_z, _x_decoded_mean)\n",
    "\n",
    "MODEL_FILE = './models/vae_v8_decoder.h5'\n",
    "generator.save(MODEL_FILE)# creates a HDF5 file\n",
    "print(\"Saved Keras model to\", MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILE = './models/vae_v8.h5'\n",
    "autoencoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std, 'vae_loss': vae_loss})\n",
    "MODEL_FILE = './models/vae_v8_encoder.h5'\n",
    "encoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})\n",
    "MODEL_FILE = './models/vae_v8_decoder.h5'\n",
    "decoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 1000 # Calculate average IOU over 100 units\n",
    "# Run test input through the autoencoder\n",
    "decoded_inputs = autoencoder.predict(input_test).squeeze()\n",
    "sample_indices = np.random.randint(len(decoded_inputs), size=SAMPLE_SIZE)\n",
    "\n",
    "total_iou = 0\n",
    "for sample_index in sample_indices:\n",
    "    sample_input = input_test[sample_index].squeeze()\n",
    "    sample_output = pianoroll_utils.pianoroll_preprocess(decoded_inputs[sample_index], MIN_PITCH, MAX_PITCH)\n",
    "    total_iou += pianoroll_utils.pitch_intersection_over_union(sample_input, sample_output, MIN_PITCH, MAX_PITCH)\n",
    "\n",
    "print('Average IOU:', total_iou/len(sample_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a random input sample\n",
    "sample_index = np.random.randint(len(input_test))\n",
    "sample_input = input_test[sample_index].squeeze()\n",
    "# Run test input through the autoencoder\n",
    "sample_output = autoencoder.predict(sample_input[np.newaxis, ..., np.newaxis]).squeeze()\n",
    "sample_output = pianoroll_utils.pianoroll_preprocess(sample_output, MIN_PITCH, MAX_PITCH)\n",
    "\n",
    "print(sample_input.shape)\n",
    "print(sample_output.shape)\n",
    "print(pianoroll_utils.get_active_pitch_classes(sample_input, MIN_PITCH, MAX_PITCH))\n",
    "print(pianoroll_utils.get_active_pitch_classes(sample_output, MIN_PITCH, MAX_PITCH))\n",
    "print(pianoroll_utils.pitch_intersection_over_union(sample_input, sample_output, MIN_PITCH, MAX_PITCH))\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(1,2)\n",
    "fig.set_size_inches(10, 6, forward=True)\n",
    "ax[0].set_title('Input')\n",
    "ax[1].set_title('Output')\n",
    "pianoroll_utils.plot_pianoroll(ax[0], sample_input, MIN_PITCH, MAX_PITCH, beat_resolution=24, cmap='inferno')\n",
    "pianoroll_utils.plot_pianoroll(ax[1], sample_output, MIN_PITCH, MAX_PITCH, beat_resolution=24, cmap='inferno')\n",
    "fig.tight_layout()\n",
    "\n",
    "# Play comparison\n",
    "pianoroll_utils.play_pianoroll(sample_input, MIN_PITCH, MAX_PITCH, '0')\n",
    "pianoroll_utils.play_pianoroll(sample_output, MIN_PITCH, MAX_PITCH, '1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 2. Latent spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A pretty big motivation behind this whole approach of learning intermediate embeddings, is the idea that we can learn a latent space which has some desirable properties including _smoothness_ and _realism_. With these properties, there are some important things we can do:\n",
    "\n",
    "- Random sampling\n",
    "- Latent arithmetic\n",
    "- Latent interpolation\n",
    "\n",
    "Each covered in its own subsection.\n",
    "\n",
    "More information about VAEs and latent spaces in [this blogpost](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "latent_dim = 500\n",
    "epsilon_std = 1.0\n",
    "\n",
    "MODEL_FILE = './models/vae_v7_encoder.h5'\n",
    "encoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})\n",
    "MODEL_FILE = './models/vae_v7_decoder.h5'\n",
    "decoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2.1 Random sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Assuming our model managed to learn the latent space properly, we should be able to generate random samples from a normal distribution, and decode these random samples to generate realistic units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 3 # Number of random units to generate\n",
    "\n",
    "# Generate random vector\n",
    "random_embed = np.random.normal(loc=0, scale=epsilon_std, size=(NUM_SAMPLES, latent_dim))\n",
    "\n",
    "# Decode\n",
    "random_units = decoder.predict(random_embed)\n",
    "\n",
    "fig, ax = plt.subplots(NUM_SAMPLES, 1)\n",
    "fig.set_size_inches(10, NUM_SAMPLES * 3, forward=True)\n",
    "for i in range(NUM_SAMPLES):\n",
    "    current_unit = random_units[i].squeeze()\n",
    "    \n",
    "    # Plot comparison\n",
    "    ax[i].set_title('Random sample ' + str(i))\n",
    "    pianoroll_utils.plot_pianoroll(ax[i], current_unit, MIN_PITCH, MAX_PITCH, beat_resolution=24, cmap='inferno')\n",
    "\n",
    "    # # Play comparison\n",
    "    pianoroll_utils.play_pianoroll(current_unit, MIN_PITCH, MAX_PITCH, str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2.2 Latent arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Basically, the idea behind latent arithmetic is that the latent variables capture meaningful features in their encoding, such that we can add and subtract latent variables, and get a latent result which can be reconstructed into something meaningful.\n",
    "\n",
    "For example, some useful musical properties we could try to manipulate using latent arithmetic include:\n",
    "1. Note velocity\n",
    "2. Chord density\n",
    "3. Note density\n",
    "4. Pitch position (higher or lower octaves)\n",
    "5. Musical forwardness (melody/background)\n",
    "\n",
    "and possibly more.\n",
    "\n",
    "In this section, we will try to construct latent vectors which capture the essence of these properties, and see if we can transform an input unit into a new unit with the desired properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Note velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Construct two units, one of high velocity and one of low velocity\n",
    "c_major = [60, 64, 67, 72]\n",
    "high_velocity_unit = np.zeros((NUM_PITCHES, NUM_TICKS))\n",
    "high_velocity_unit[c_major, :] = 127\n",
    "low_velocity_unit = np.zeros((NUM_PITCHES, NUM_TICKS))\n",
    "low_velocity_unit[c_major, :] = 40\n",
    "\n",
    "# Encode\n",
    "high_velocity_unit_embed = encoder.predict(high_velocity_unit.reshape(1, NUM_PITCHES, NUM_TICKS, 1) / 127.)\n",
    "low_velocity_unit_embed = encoder.predict(low_velocity_unit.reshape(1, NUM_PITCHES, NUM_TICKS, 1) / 127.)\n",
    "\n",
    "# Calculate the latent vector for note velocity\n",
    "positive_velocity_vector = high_velocity_unit_embed - low_velocity_unit_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Chord density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Construct two units, one of high velocity and one of low velocity\n",
    "c_major = [60, 84]\n",
    "c_major_dense = [60, 64, 67, 72, 76, 79, 84]\n",
    "low_density_unit = np.zeros((NUM_PITCHES, NUM_TICKS))\n",
    "low_density_unit[c_major, :] = 1\n",
    "high_density_unit = np.zeros((NUM_PITCHES, NUM_TICKS))\n",
    "high_density_unit[c_major_dense, :] = 1\n",
    "\n",
    "# Encode\n",
    "low_density_unit_embed = encoder.predict(low_density_unit.reshape(1, NUM_PITCHES, NUM_TICKS, 1))\n",
    "high_density_unit_embed = encoder.predict(high_density_unit.reshape(1, NUM_PITCHES, NUM_TICKS, 1))\n",
    "\n",
    "# Calculate the latent vector for chord density\n",
    "positive_density_vector = high_density_unit_embed - low_density_unit_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Transposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Construct two units, one of high velocity and one of low velocity\n",
    "c_major = np.array([48, 52, 55, 60, 64, 67, 72])\n",
    "d_major = c_major + 2\n",
    "unit_1 = np.zeros((NUM_PITCHES, NUM_TICKS))\n",
    "unit_1[c_major, :] = 0.5\n",
    "unit_2 = np.zeros((NUM_PITCHES, NUM_TICKS))\n",
    "unit_2[d_major, :] = 0.5\n",
    "\n",
    "# Encode\n",
    "unit_1_embed = encoder.predict(unit_1.reshape(1, NUM_PITCHES, NUM_TICKS, 1))\n",
    "unit_2_embed = encoder.predict(unit_2.reshape(1, NUM_PITCHES, NUM_TICKS, 1))\n",
    "\n",
    "# Calculate the latent vector for chord density\n",
    "positive_transposition_vector = unit_2_embed - unit_1_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "latent_operator = positive_transposition_vector\n",
    "\n",
    "# Inspect a random input-output sample\n",
    "sample_index = np.random.randint(len(input_test))\n",
    "sample = input_test[sample_index].squeeze()\n",
    "\n",
    "# Encode and apply latent vector\n",
    "sample_embed = encoder.predict(sample.reshape(1, NUM_PITCHES, NUM_TICKS, 1))\n",
    "sample_embed_transformed = sample_embed + latent_operator\n",
    "\n",
    "# Decode\n",
    "sample_decoded = decoder.predict(sample_embed)[0].squeeze()\n",
    "sample_transformed_decoded = decoder.predict(sample_embed_transformed)[0].squeeze()\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(3,1)\n",
    "fig.set_size_inches(16, 10, forward=True)\n",
    "ax[0].set_title('Original')\n",
    "ax[1].set_title('Decoded')\n",
    "ax[2].set_title('Transformed')\n",
    "pianoroll_utils.plot_pianoroll(ax[0], sample, MIN_PITCH, MAX_PITCH, beat_resolution=24, cmap='inferno')\n",
    "pianoroll_utils.plot_pianoroll(ax[1], sample_decoded, MIN_PITCH, MAX_PITCH, beat_resolution=24, cmap='inferno')\n",
    "pianoroll_utils.plot_pianoroll(ax[2], sample_transformed_decoded, MIN_PITCH, MAX_PITCH, beat_resolution=24, cmap='inferno')\n",
    "fig.tight_layout()\n",
    "\n",
    "# # Play comparison\n",
    "pianoroll_utils.play_pianoroll(sample, MIN_PITCH, MAX_PITCH, '0')\n",
    "pianoroll_utils.play_pianoroll(sample_decoded, MIN_PITCH, MAX_PITCH, '1')\n",
    "pianoroll_utils.play_pianoroll(sample_transformed_decoded, MIN_PITCH, MAX_PITCH, '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2.3 Latent space interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Grab two random samples\n",
    "sample_index_1 = np.random.randint(len(input_test))\n",
    "sample_1 = input_test[sample_index_1].squeeze()\n",
    "sample_index_2 = np.random.randint(len(input_test))\n",
    "sample_2 = input_test[sample_index_2].squeeze()\n",
    "\n",
    "# Encode and apply latent vector\n",
    "sample_embed_1 = encoder.predict(sample_1.reshape(1, NUM_PITCHES, NUM_TICKS, 1))\n",
    "sample_embed_2 = encoder.predict(sample_2.reshape(1, NUM_PITCHES, NUM_TICKS, 1))\n",
    "\n",
    "NUM_INTERPOLATIONS = 4\n",
    "NUM_OUTPUTS = NUM_INTERPOLATIONS + 2 # One on each end of the interpolated values\n",
    "NUM_PLOT_ROWS = int(np.ceil(NUM_OUTPUTS/2.)) # Two per row, round up if odd number\n",
    "\n",
    "fig, ax = plt.subplots(NUM_PLOT_ROWS, 2)\n",
    "fig.set_size_inches(10, NUM_PLOT_ROWS * 3, forward=True)\n",
    "for i in range(NUM_OUTPUTS):\n",
    "    alpha = float(i) / (NUM_INTERPOLATIONS + 1)\n",
    "    sample_embed_interp = sample_embed_1 * (1 - alpha) + sample_embed_2 * alpha\n",
    "\n",
    "    # Decode\n",
    "    sample_decoded_interp = decoder.predict(sample_embed_interp)[0].squeeze()\n",
    "\n",
    "    # Plot comparison\n",
    "    ax[i/2, i%2].set_title('Interpolation, alpha = ' + ('%.2f' % alpha))\n",
    "    pianoroll_utils.plot_pianoroll(ax[i/2, i%2], sample_decoded_interp, MIN_PITCH, MAX_PITCH, beat_resolution=24, cmap='inferno')\n",
    "\n",
    "    # # Play comparison\n",
    "    pianoroll_utils.play_pianoroll(sample_decoded_interp, MIN_PITCH, MAX_PITCH, str(i))\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
