{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "1. One-to-many, many-to-one and other variants on [StackOVerflow](https://stackoverflow.com/questions/43034960/many-to-one-and-many-to-many-lstm-examples-in-keras)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Imports and user variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports and variable definitions successful, using 1000 files.\n"
     ]
    }
   ],
   "source": [
    "%run -i imports_and_user_variables.py 1000\n",
    "from __future__ import print_function # Strange bug, this function can't be imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import pypianoroll\n",
    "from matplotlib import pyplot as plt\n",
    "import cPickle as pickle\n",
    "import pianoroll_utils\n",
    "import IPython\n",
    "import h5py\n",
    "\n",
    "from keras.layers import Input, Dense, Conv2D, Conv2DTranspose, BatchNormalization, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.layers import Activation, LSTM, RNN, Concatenate, concatenate, Dropout\n",
    "from keras import optimizers\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from six.moves import range\n",
    "\n",
    "NUM_FILES = 1000\n",
    "SEQ_FILE = './pickle_jar/seq_{}_songs_clipped96.h5'.format(NUM_FILES)\n",
    "WINDOW_LENGTH = 4\n",
    "\n",
    "PICKLE_FILE = './pickle_jar/norm_units_50_songs.pkl'\n",
    "SEQ_PICKLE_FILE = './pickle_jar/seq_units_200_songs_clipped96.pkl'\n",
    "SEQ_EMBED_PICKLE_FILE = './pickle_jar/seq_embed_units_200_songs_clipped96_vaev5.pkl'\n",
    "\n",
    "# Music shape\n",
    "MIN_PITCH = 13 # C-1\n",
    "MAX_PITCH = 108 # C7 (MIDI 108)\n",
    "BEATS_PER_UNIT = 4\n",
    "NUM_TRANSPOSITIONS = 3 # Number of transpositions to perform (maximum 12)\n",
    "\n",
    "# Don't change unless you know what you're doing\n",
    "BEAT_RESOLUTION = 24 # This is set by the encoding of the lpd-5 dataset, corresponds to number of ticks per beat\n",
    "PARTITION_NOTE = 60 # Break into left- and right-accompaniments at middle C\n",
    "NUM_PITCHES = MAX_PITCH - MIN_PITCH + 1\n",
    "NUM_TICKS = BEATS_PER_UNIT * BEAT_RESOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "\n",
    "class TimeseriesGeneratorTwoInputs(Sequence):\n",
    "    \"\"\"Keras data reading class\n",
    "    Utility class for generating batches of temporal data.\n",
    "    This class takes in a sequence of data-points gathered at\n",
    "    equal intervals, along with time series parameters such as\n",
    "    stride, length of history, etc., to produce batches for\n",
    "    training/validation.\n",
    "    \n",
    "    Adapted from the TimeseriesGenerator class:\n",
    "    https://github.com/keras-team/keras/blob/master/keras/utils/data_utils.py#L302\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data1, data2, targets, length,\n",
    "                 sampling_rate=1,\n",
    "                 stride=1,\n",
    "                 start_index=0,\n",
    "                 end_index=None,\n",
    "                 shuffle=False,\n",
    "                 reverse=False,\n",
    "                 batch_size=128):\n",
    "        self.data1 = data1\n",
    "        self.data2 = data2\n",
    "        assert len(data1) == len(data2)\n",
    "        self.targets = targets\n",
    "        self.length = length\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.stride = stride\n",
    "        self.start_index = start_index + length\n",
    "        if end_index is None:\n",
    "            end_index = len(data1) - 1\n",
    "        self.end_index = end_index\n",
    "        self.shuffle = shuffle\n",
    "        self.reverse = reverse\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if self.start_index > self.end_index:\n",
    "            raise ValueError('`start_index+length=%i > end_index=%i` '\n",
    "                             'is disallowed, as no part of the sequence '\n",
    "                             'would be left to be used as current step.'\n",
    "                             % (self.start_index, self.end_index))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(\n",
    "            (self.end_index - self.start_index + 1) /\n",
    "            (self.batch_size * self.stride)))\n",
    "\n",
    "    def _empty_batch(self, num_rows):\n",
    "        samples_shape = [num_rows, self.length // self.sampling_rate]\n",
    "        samples_shape.extend(self.data1.shape[1:])\n",
    "        targets_shape = [num_rows]\n",
    "        targets_shape.extend(self.targets.shape[1:])\n",
    "        return np.empty(samples_shape), np.empty(samples_shape), np.empty(targets_shape)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.shuffle:\n",
    "            rows = np.random.randint(\n",
    "                self.start_index, self.end_index + 1, size=self.batch_size)\n",
    "        else:\n",
    "            i = self.start_index + self.batch_size * self.stride * index\n",
    "            rows = np.arange(i, min(i + self.batch_size *\n",
    "                                    self.stride, self.end_index + 1), self.stride)\n",
    "\n",
    "        samples1, samples2, targets = self._empty_batch(len(rows))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - self.length, rows[j], self.sampling_rate)\n",
    "            samples1[j] = self.data1[indices]\n",
    "            samples2[j] = self.data2[indices]\n",
    "            targets[j] = self.targets[rows[j]]\n",
    "        if self.reverse:\n",
    "            return [samples1[:, ::-1, ...], samples2[:, ::-1, ...]], targets\n",
    "        return [samples1, samples2], targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### RAM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded units from ./pickle_jar/seq_units_200_songs_clipped96.pkl\n",
      "seq_units['input_train'].shape: (36812, 96, 96)\n",
      "seq_units['comp_train'].shape: (36812, 96, 96)\n",
      "seq_units['input_test'].shape: (4260, 96, 96)\n",
      "seq_units['comp_test'].shape: (4260, 96, 96)\n"
     ]
    }
   ],
   "source": [
    "# Sequence data\n",
    "seq_units = {}\n",
    "with open(SEQ_PICKLE_FILE, 'rb') as infile:\n",
    "    seq_units = pickle.load( infile )\n",
    "\n",
    "# Print info\n",
    "print(\"Loaded units from\", SEQ_PICKLE_FILE)\n",
    "print(\"seq_units['input_train'].shape:\", seq_units[\"input_train\"].shape)\n",
    "print(\"seq_units['comp_train'].shape:\", seq_units[\"comp_train\"].shape)\n",
    "print(\"seq_units['input_test'].shape:\", seq_units[\"input_test\"].shape)\n",
    "print(\"seq_units['comp_test'].shape:\", seq_units[\"comp_test\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded units from ./pickle_jar/seq_embed_units_200_songs_clipped96_vaev5.pkl\n",
      "seq_embed['input_train'].shape:  (36812, 500)\n",
      "seq_embed['comp_train'].shape:  (36812, 500)\n",
      "seq_embed['input_test'].shape:  (4260, 500)\n",
      "seq_embed['comp_test'].shape:  (4260, 500)\n"
     ]
    }
   ],
   "source": [
    "# Embedded sequence data\n",
    "seq_embed = {}\n",
    "with open(SEQ_EMBED_PICKLE_FILE, 'rb') as infile:\n",
    "    seq_embed = pickle.load( infile )\n",
    "\n",
    "# Print info\n",
    "print(\"Loaded units from\", SEQ_EMBED_PICKLE_FILE)\n",
    "print(\"seq_embed['input_train'].shape: \", seq_embed[\"input_train\"].shape)\n",
    "print(\"seq_embed['comp_train'].shape: \", seq_embed[\"comp_train\"].shape)\n",
    "print(\"seq_embed['input_test'].shape: \", seq_embed[\"input_test\"].shape)\n",
    "print(\"seq_embed['comp_test'].shape: \", seq_embed[\"comp_test\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34991, 4, 500)\n",
      "(34991, 4, 500)\n",
      "(34991, 500)\n",
      "(1817, 4, 500)\n",
      "(1817, 4, 500)\n",
      "(1817, 500)\n"
     ]
    }
   ],
   "source": [
    "# Create dataset for sequence training\n",
    "\n",
    "num_data = len(seq_embed['input_train']) - WINDOW_LENGTH\n",
    "x_input = np.zeros((num_data, WINDOW_LENGTH, latent_dim))\n",
    "x_comp = np.zeros((num_data, WINDOW_LENGTH, latent_dim))\n",
    "y_comp = np.zeros((num_data, latent_dim))\n",
    "for i in range(num_data):\n",
    "    x_input[i] = seq_embed['input_train'][i:i+WINDOW_LENGTH]\n",
    "    x_comp[i] = seq_embed['comp_train'][i:i+WINDOW_LENGTH]\n",
    "    y_comp[i] = seq_embed['comp_train'][i+WINDOW_LENGTH]\n",
    "\n",
    "train_test_indices = np.random.choice([True, False], size=len(x_input), p=[.95, .05])\n",
    "# Training data\n",
    "x_input_train = x_input[train_test_indices, ...]\n",
    "x_comp_train = x_comp[train_test_indices, ...]\n",
    "y_comp_train = y_comp[train_test_indices, ...]\n",
    "# Testing data\n",
    "x_input_test = x_input[np.invert(train_test_indices), ...]\n",
    "x_comp_test = x_comp[np.invert(train_test_indices), ...]\n",
    "y_comp_test = y_comp[np.invert(train_test_indices), ...]\n",
    "\n",
    "# Delete unecessary memory hogs\n",
    "del x_input\n",
    "del x_comp\n",
    "del y_comp\n",
    "\n",
    "print(x_input_train.shape)\n",
    "print(x_comp_train.shape)\n",
    "print(y_comp_train.shape)\n",
    "\n",
    "print(x_input_test.shape)\n",
    "print(x_comp_test.shape)\n",
    "print(y_comp_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### h5py dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(441738, 96, 96, 1)\n",
      "(441738, 96, 96, 1)\n",
      "(441738, 50)\n",
      "(441738, 50)\n",
      "3451\n",
      "410\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "f = h5py.File(ONSETS_SEQ_FILE, 'r')\n",
    "seq_units_input = f['input_train']\n",
    "seq_units_comp = f['comp_train']\n",
    "seq_embed_input = f['input_embed_train']\n",
    "seq_embed_comp = f['comp_embed_train']\n",
    "data_gen_train = TimeseriesGeneratorTwoInputs(f['input_embed_train'], f['comp_embed_train'], f['comp_embed_train'], \n",
    "                                              length=WINDOW_LENGTH, sampling_rate=1, batch_size=128)\n",
    "data_gen_test = TimeseriesGeneratorTwoInputs(f['input_embed_test'], f['comp_embed_test'], f['comp_embed_test'], \n",
    "                                              length=WINDOW_LENGTH, sampling_rate=1, batch_size=128)\n",
    "print(seq_units_input.shape)\n",
    "print(seq_units_comp.shape)\n",
    "print(seq_embed_input.shape)\n",
    "print(seq_embed_comp.shape)\n",
    "print(len(data_gen_train))\n",
    "print(len(data_gen_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Unit autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 1. Unit autoencoder using RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "Input: pianoroll-unit (128, 96)\n",
    "Output: pianoroll-unit (128, 96)\n",
    "```\n",
    "\n",
    "### Layers = 1, Hidden Size = 128\n",
    "Epoch 50: `loss: 0.0022 - acc: 0.5142 - val_loss: 0.0026 - val_acc: 0.4837`\n",
    "\n",
    "### Layers = 2, Hidden Size = 128\n",
    "Epoch 50: `loss: 0.0027 - acc: 0.4919 - val_loss: 0.0031 - val_acc: 0.4702`\n",
    "\n",
    "### Layers = 1, Hidden Size = 256\n",
    "Epoch 50: `loss: 0.0023 - acc: 0.5242 - val_loss: 0.0027 - val_acc: 0.4984`\n",
    "\n",
    "Code adapted from: https://github.com/keras-team/keras/blob/master/examples/addition_rnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loaded', 3831, 'units from', './pickle_jar/norm_units_50_songs.pkl')\n",
      "('input_units.shape: ', (3831, 128, 96))\n",
      "('comp_units.shape: ', (3831, 128, 96))\n",
      "('full_units.shape: ', (3831, 128, 96))\n",
      "('Train:', (3480, 96, 128))\n",
      "('Test:', (351, 96, 128))\n"
     ]
    }
   ],
   "source": [
    "units = {}\n",
    "with open(PICKLE_FILE, 'rb') as infile:\n",
    "    units = pickle.load( infile )\n",
    "units['full'] = units['input'] + units['comp']\n",
    "\n",
    "# Print info\n",
    "print(\"Loaded\", units[\"input\"].shape[0], \"units from\", PICKLE_FILE)\n",
    "print(\"input_units.shape: \", units[\"input\"].shape)\n",
    "print(\"comp_units.shape: \", units[\"comp\"].shape)\n",
    "print(\"full_units.shape: \", units[\"full\"].shape)\n",
    "\n",
    "# Create an array of True (train) and False (test) to split the dataset\n",
    "train_test_indices = np.random.choice([True, False], size=len(units[\"input\"]), p=[.9, .1])\n",
    "# Training data\n",
    "input_train = units[\"input\"][train_test_indices, ...].swapaxes(1,2)\n",
    "comp_train = units[\"comp\"][train_test_indices, ...].swapaxes(1,2)\n",
    "full_train = units[\"full\"][train_test_indices, ...].swapaxes(1,2)\n",
    "# Testing data\n",
    "input_test = units[\"input\"][np.invert(train_test_indices), ...].swapaxes(1,2)\n",
    "comp_test = units[\"comp\"][np.invert(train_test_indices), ...].swapaxes(1,2)\n",
    "full_test = units[\"full\"][np.invert(train_test_indices), ...].swapaxes(1,2)\n",
    "print(\"Train:\", input_train.shape)\n",
    "print(\"Test:\", input_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Parameters for the model and dataset.\n",
    "TRAINING_SIZE = 50000\n",
    "NUM_TICKS = 96\n",
    "NUM_PITCHES = 128\n",
    "REVERSE = True\n",
    "\n",
    "MAXLEN = NUM_TICKS\n",
    "\n",
    "# Try replacing GRU, or SimpleRNN.\n",
    "RNN = layers.LSTM\n",
    "HIDDEN_SIZE = 128\n",
    "LAYERS = 1\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n",
    "# Note: In a situation where your input sequences have a variable length,\n",
    "# use input_shape=(None, num_feature).\n",
    "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, NUM_PITCHES)))\n",
    "# As the decoder RNN's input, repeatedly provide with the last hidden state of\n",
    "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "model.add(layers.RepeatVector(MAXLEN))\n",
    "# The decoder RNN could be multiple layers stacked or a single layer.\n",
    "for _ in range(LAYERS):\n",
    "    # By setting return_sequences to True, return not only the last output but\n",
    "    # all the outputs so far in the form of (num_samples, timesteps,\n",
    "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "    # the first dimension to be the timesteps.\n",
    "    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "# of the output sequence, decide which character should be chosen.\n",
    "model.add(layers.TimeDistributed(layers.Dense(NUM_PITCHES)))\n",
    "model.add(layers.Activation('tanh'))\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2935 samples, validate on 333 samples\n",
      "Epoch 1/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0046 - acc: 0.3970 - val_loss: 0.0046 - val_acc: 0.4170\n",
      "Epoch 2/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0044 - acc: 0.4097 - val_loss: 0.0045 - val_acc: 0.4245\n",
      "Epoch 3/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0043 - acc: 0.4138 - val_loss: 0.0044 - val_acc: 0.4282\n",
      "Epoch 4/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0042 - acc: 0.4242 - val_loss: 0.0043 - val_acc: 0.3930\n",
      "Epoch 5/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0041 - acc: 0.4273 - val_loss: 0.0043 - val_acc: 0.4511\n",
      "Epoch 6/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0040 - acc: 0.4344 - val_loss: 0.0042 - val_acc: 0.3932\n",
      "Epoch 7/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0040 - acc: 0.4361 - val_loss: 0.0041 - val_acc: 0.4113\n",
      "Epoch 8/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0039 - acc: 0.4382 - val_loss: 0.0040 - val_acc: 0.4545\n",
      "Epoch 9/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0039 - acc: 0.4439 - val_loss: 0.0041 - val_acc: 0.4541\n",
      "Epoch 10/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0038 - acc: 0.4446 - val_loss: 0.0040 - val_acc: 0.4346\n",
      "Epoch 11/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0038 - acc: 0.4462 - val_loss: 0.0039 - val_acc: 0.4612\n",
      "Epoch 12/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0038 - acc: 0.4498 - val_loss: 0.0039 - val_acc: 0.4107\n",
      "Epoch 13/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0037 - acc: 0.4478 - val_loss: 0.0038 - val_acc: 0.4593\n",
      "Epoch 14/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0037 - acc: 0.4480 - val_loss: 0.0039 - val_acc: 0.4387\n",
      "Epoch 15/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0037 - acc: 0.4558 - val_loss: 0.0039 - val_acc: 0.4230\n",
      "Epoch 16/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0037 - acc: 0.4525 - val_loss: 0.0038 - val_acc: 0.4536\n",
      "Epoch 17/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0036 - acc: 0.4514 - val_loss: 0.0038 - val_acc: 0.4589\n",
      "Epoch 18/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0036 - acc: 0.4513 - val_loss: 0.0038 - val_acc: 0.4657\n",
      "Epoch 19/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0036 - acc: 0.4552 - val_loss: 0.0037 - val_acc: 0.4606\n",
      "Epoch 20/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0036 - acc: 0.4602 - val_loss: 0.0038 - val_acc: 0.4362\n",
      "Epoch 21/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0036 - acc: 0.4616 - val_loss: 0.0037 - val_acc: 0.4439\n",
      "Epoch 22/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0035 - acc: 0.4595 - val_loss: 0.0037 - val_acc: 0.4618\n",
      "Epoch 23/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0035 - acc: 0.4582 - val_loss: 0.0037 - val_acc: 0.4462\n",
      "Epoch 24/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0035 - acc: 0.4636 - val_loss: 0.0037 - val_acc: 0.4576\n",
      "Epoch 25/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0035 - acc: 0.4642 - val_loss: 0.0037 - val_acc: 0.4665\n",
      "Epoch 26/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0035 - acc: 0.4619 - val_loss: 0.0037 - val_acc: 0.4522\n",
      "Epoch 27/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0035 - acc: 0.4615 - val_loss: 0.0036 - val_acc: 0.4559\n",
      "Epoch 28/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0034 - acc: 0.4611 - val_loss: 0.0036 - val_acc: 0.4362\n",
      "Epoch 29/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0034 - acc: 0.4654 - val_loss: 0.0037 - val_acc: 0.4589\n",
      "Epoch 30/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0034 - acc: 0.4641 - val_loss: 0.0036 - val_acc: 0.4450\n",
      "Epoch 31/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0034 - acc: 0.4646 - val_loss: 0.0036 - val_acc: 0.4552\n",
      "Epoch 32/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0034 - acc: 0.4639 - val_loss: 0.0036 - val_acc: 0.4552\n",
      "Epoch 33/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0034 - acc: 0.4670 - val_loss: 0.0036 - val_acc: 0.4455\n",
      "Epoch 34/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0034 - acc: 0.4638 - val_loss: 0.0036 - val_acc: 0.4607\n",
      "Epoch 35/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0034 - acc: 0.4631 - val_loss: 0.0036 - val_acc: 0.4540\n",
      "Epoch 36/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0034 - acc: 0.4679 - val_loss: 0.0036 - val_acc: 0.4518\n",
      "Epoch 37/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0034 - acc: 0.4727 - val_loss: 0.0036 - val_acc: 0.4394\n",
      "Epoch 38/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0033 - acc: 0.4649 - val_loss: 0.0036 - val_acc: 0.4667\n",
      "Epoch 39/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0033 - acc: 0.4667 - val_loss: 0.0035 - val_acc: 0.4303\n",
      "Epoch 40/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0033 - acc: 0.4725 - val_loss: 0.0035 - val_acc: 0.4664\n",
      "Epoch 41/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0033 - acc: 0.4677 - val_loss: 0.0035 - val_acc: 0.4481\n",
      "Epoch 42/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0033 - acc: 0.4685 - val_loss: 0.0035 - val_acc: 0.4603\n",
      "Epoch 43/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0033 - acc: 0.4767 - val_loss: 0.0035 - val_acc: 0.4592\n",
      "Epoch 44/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0033 - acc: 0.4706 - val_loss: 0.0035 - val_acc: 0.4641\n",
      "Epoch 45/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0033 - acc: 0.4757 - val_loss: 0.0035 - val_acc: 0.4446\n",
      "Epoch 46/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0033 - acc: 0.4741 - val_loss: 0.0035 - val_acc: 0.4526\n",
      "Epoch 47/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0033 - acc: 0.4736 - val_loss: 0.0035 - val_acc: 0.4593\n",
      "Epoch 48/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0033 - acc: 0.4760 - val_loss: 0.0035 - val_acc: 0.4706\n",
      "Epoch 49/50\n",
      "2935/2935 [==============================] - 21s 7ms/step - loss: 0.0032 - acc: 0.4696 - val_loss: 0.0035 - val_acc: 0.4911\n",
      "Epoch 50/50\n",
      "2935/2935 [==============================] - 22s 7ms/step - loss: 0.0032 - acc: 0.4715 - val_loss: 0.0035 - val_acc: 0.4483\n",
      "Saved Keras model to ./models/rnn_v1.h5\n"
     ]
    }
   ],
   "source": [
    "# Train the model each generation and show predictions against the validation\n",
    "# dataset.\n",
    "\n",
    "model.fit(input_train, input_train,\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                shuffle=True,\n",
    "                validation_data=(input_test, input_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/rnn')])\n",
    "\n",
    "MODEL_RNN_V1_FILE = './models/rnn_v1.h5'\n",
    "model.save(MODEL_RNN_V1_FILE)# creates a HDF5 file\n",
    "print(\"Saved Keras model to\", MODEL_RNN_V1_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MODEL_RNN_V1_FILE = './models/rnn_v1.h5'\n",
    "model = load_model(MODEL_RNN_V1_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(351, 96, 128)\n"
     ]
    }
   ],
   "source": [
    "# Run test inputs through the autoencoder\n",
    "decoded_test = model.predict(input_test)\n",
    "print(decoded_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 128)\n",
      "(96, 128)\n",
      "(96, 128)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "data byte must be in range 0..127",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-539ddc14e404>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# pianoroll_utils.playPianoroll(sample_full)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# pianoroll_utils.playPianoroll(sample_output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mpianoroll_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayPianoroll_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/junshern/s_drive/FYP/comper/pianoroll_utils.py\u001b[0m in \u001b[0;36mplayPianoroll_events\u001b[0;34m(pianoroll)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplayPianoroll_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpianoroll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mplay_midi_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpianoroll_2_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpianoroll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplay_midi_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/junshern/s_drive/FYP/comper/pianoroll_utils.py\u001b[0m in \u001b[0;36mpianoroll_2_events\u001b[0;34m(pianoroll)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mvelocity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnote_on\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnote_offs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpitch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;31m# Create message events\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mon_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmido\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'note_on'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpitch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvelocity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvelocity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0mevents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnote_ons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mon_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnote_offs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m96\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/mido/messages/messages.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, type, **args)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sysex'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mmsgdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSysexData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_py2_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsgdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mcheck_msgdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsgdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsgdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/mido/messages/checks.pyc\u001b[0m in \u001b[0;36mcheck_msgdict\u001b[0;34m(msgdict)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 '{} message has no attribute {}'.format(spec['type'], name))\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mcheck_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/mido/messages/checks.pyc\u001b[0m in \u001b[0;36mcheck_value\u001b[0;34m(name, value)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0m_CHECKS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/junshern/Scripts/fyp-virtualenv/local/lib/python2.7/site-packages/mido/messages/checks.pyc\u001b[0m in \u001b[0;36mcheck_data_byte\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data byte must be int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m127\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data byte must be in range 0..127'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: data byte must be in range 0..127"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect a random input-output sample\n",
    "sample_index = np.random.randint(len(input_test))\n",
    "sample_input = input_test[sample_index] * 127\n",
    "sample_full = full_test[sample_index] * 127\n",
    "sample_output = decoded_test[sample_index] * 127\n",
    "\n",
    "# Set all velocity values < threshold to zero\n",
    "threshold = 10\n",
    "sample_output[sample_output < threshold] = 0\n",
    "sample_output[sample_output >= threshold] = 100\n",
    "\n",
    "print(sample_input.shape)\n",
    "print(sample_full.shape)\n",
    "print(sample_output.shape)\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(1,3)\n",
    "fig.set_size_inches(10, 6, forward=True)\n",
    "ax[0].set_title('Input')\n",
    "ax[1].set_title('Target')\n",
    "ax[2].set_title('Output')\n",
    "pypianoroll.plot_pianoroll(ax[0], sample_input, beat_resolution=24, cmap='inferno')\n",
    "pypianoroll.plot_pianoroll(ax[1], sample_full, beat_resolution=24, cmap='inferno')\n",
    "pypianoroll.plot_pianoroll(ax[2], sample_output, beat_resolution=24, cmap='inferno')\n",
    "fig.tight_layout()\n",
    "\n",
    "# Play comparison\n",
    "pianoroll_utils.playPianoroll(sample_input)\n",
    "# pianoroll_utils.playPianoroll(sample_full)\n",
    "# pianoroll_utils.playPianoroll(sample_output)\n",
    "pianoroll_utils.playPianoroll_events(sample_output.swapaxes(0,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Sequence-to-sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''Sequence to sequence example in Keras (character-level).\n",
    "This script demonstrates how to implement a basic character-level\n",
    "sequence-to-sequence model. We apply it to translating\n",
    "short English sentences into short French sentences,\n",
    "character-by-character. Note that it is fairly unusual to\n",
    "do character-level machine translation, as word-level\n",
    "models are more common in this domain.\n",
    "# Summary of the algorithm\n",
    "- We start with input sequences from a domain (e.g. English sentences)\n",
    "    and correspding target sequences from another domain\n",
    "    (e.g. French sentences).\n",
    "- An encoder LSTM turns input sequences to 2 state vectors\n",
    "    (we keep the last LSTM state and discard the outputs).\n",
    "- A decoder LSTM is trained to turn the target sequences into\n",
    "    the same sequence but offset by one timestep in the future,\n",
    "    a training process called \"teacher forcing\" in this context.\n",
    "    Is uses as initial state the state vectors from the encoder.\n",
    "    Effectively, the decoder learns to generate `targets[t+1...]`\n",
    "    given `targets[...t]`, conditioned on the input sequence.\n",
    "- In inference mode, when we want to decode unknown input sequences, we:\n",
    "    - Encode the input sequence into state vectors\n",
    "    - Start with a target sequence of size 1\n",
    "        (just the start-of-sequence character)\n",
    "    - Feed the state vectors and 1-char target sequence\n",
    "        to the decoder to produce predictions for the next character\n",
    "    - Sample the next character using these predictions\n",
    "        (we simply use argmax).\n",
    "    - Append the sampled character to the target sequence\n",
    "    - Repeat until we generate the end-of-sequence character or we\n",
    "        hit the character limit.\n",
    "# Data download\n",
    "English to French sentence pairs.\n",
    "http://www.manythings.org/anki/fra-eng.zip\n",
    "Lots of neat sentence pairs datasets can be found at:\n",
    "http://www.manythings.org/anki/\n",
    "# References\n",
    "- Sequence to Sequence Learning with Neural Networks\n",
    "    https://arxiv.org/abs/1409.3215\n",
    "- Learning Phrase Representations using\n",
    "    RNN Encoder-Decoder for Statistical Machine Translation\n",
    "    https://arxiv.org/abs/1406.1078\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'fra-eng/fra.txt'\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# START HERE\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, NUM_PITCHES))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, NUM_PITCHES))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='relu')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('s2s.h5')\n",
    "\n",
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The broad objective is to learn a sequence predictor which, given a arbitrary sequence of units (of arbitrary length, in the form of embedding vectors), predicts a new unit (in equivalent vector form). \n",
    "\n",
    "One key consideration is what **input length** to consider. We could use a fixed input length (sliding window), or a variable input length - which is ideal because we want to be able to predict when the song first starts, up to the end of the song. Fixed input lengths can still work for these cases (by padding and truncating), but this may have detrimental effects on the results.\n",
    "\n",
    "Beyond that, we also need to consider several approaches to the problem:\n",
    "\n",
    "### Regression - using VAEs\n",
    "\n",
    "The prediction will be a latent vector. For this latent vector to be decoded back into an acceptable musical unit, we use a variational autoencoder which has trained a latent space (as corresponding encoder/decoder) of musical units.\n",
    "\n",
    "### Classification - using word embeddings\n",
    "\n",
    "Alternatively, we can build a vocabulary of units and learn word embeddings of these units, then use standard NLP-type word prediction methods to choose the best new unit from our vocabulary. This shares a similar strength to unit-selection methods, in that our vocabulary will consist of \"musicality-guaranteed\" units.\n",
    "\n",
    "#### Resources\n",
    "- Multiple input RNN [video](https://www.youtube.com/watch?v=FlVJo7LbOzY)\n",
    "- Useful [Stack Overflow post](https://datascience.stackexchange.com/questions/26366/training-an-rnn-with-examples-of-different-lengths-in-keras) on variable input lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### RegressionLSTM V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Overview\n",
    "\n",
    "First attempt at creating a unit-predictor. \n",
    "\n",
    "Sources:\n",
    "- Keras [docs](https://keras.io/preprocessing/sequence/#timeseriesgenerator) for TimeSeriesGenerator\n",
    "- [StackOverflow](https://stackoverflow.com/questions/49555701/timeseriesgenerator-for-two-or-more-inputs) question on multiple input timeseriesgenerator\n",
    "\n",
    "#### Notes\n",
    "\n",
    "The output is not any good; it seems to favor consistent and non-commital predictions, simply banging out some middle-octave chord at the start and continuing the same kind of unit throughout.\n",
    "\n",
    "On the bright side, it knows when to keep quiet - it predicts true positive silences pretty well haha.\n",
    "\n",
    "One important thing to consider is that the loss/accuracy are not improving. Why? One thing to think about: **Shouldn't we be able to at least overfit the training data?** I thought it should always be possible to overfit training data, and that it's only generalization to test set which would be difficult here.\n",
    "\n",
    "PS. We don't even have a validation set yet. Welp.\n",
    "\n",
    "#### Next steps\n",
    "Hard to say exactly. But some thoughts:\n",
    "1. Simplify: word2vec, and predict directly on single ticks instead of 96-tick units.\n",
    "2. Building the embeddings by itself may not be as useful as training the embeddings simultaneously with the prediction model. Training both models in conjunction with each other might be the best way to go.\n",
    "3. Find out if it's possible to get better results by forcing the model to overfit the training data. I thought this should be possible.\n",
    "4. We might be able to make this prediction task easier if we reduce the latent dimension (intuitively, a smaller latent dimension means fewer ways to get it wrong).\n",
    "5. ~~Try a (much) larger window length?~~ Tried with WINDOW_LENGTH=39. It seems to help add confidence (velocity) to the output, but only barely. Not much difference in way of loss/accuracy, and anyways it should be possible to make predictions based on a small number of units (and even instantaneously based on the current input).\n",
    "6. To increase reliance on input (rather than comp), add a dropout layer to the comp LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "WINDOW_LENGTH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "latent_dim = 1600\n",
    "epsilon_std = 1.0\n",
    "# We need to have our encoder and decoder ready\n",
    "MODEL_FILE = './models/vae_v3_encoder.h5'\n",
    "encoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})\n",
    "MODEL_FILE = './models/vae_v3_generator.h5'\n",
    "decoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a data generator\n",
    "def get_multiple_generator(data1, data2, targets, window_length = 5, batch_size = 32):\n",
    "    data1_gen = TimeseriesGenerator(data1, targets,\n",
    "                               length=window_length, sampling_rate=1,\n",
    "                               batch_size=batch_size)\n",
    "\n",
    "    data2_gen = TimeseriesGenerator(data2, targets,\n",
    "                               length=window_length, sampling_rate=1,\n",
    "                               batch_size=batch_size)\n",
    "    while True:\n",
    "        for x in range(len(data1_gen)):\n",
    "            x_1, y = data1_gen[x]\n",
    "            x_2, _ = data2_gen[x]\n",
    "            yield [x_1, x_2], y\n",
    "\n",
    "data_gen = get_multiple_generator(seq_embed['input'], seq_embed['comp'], seq_embed['comp'], \n",
    "                                  window_length=WINDOW_LENGTH, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_27 (InputLayer)           (None, 3, 1600)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_28 (InputLayer)           (None, 3, 1600)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_27 (LSTM)                  (None, 500)          4202000     input_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_28 (LSTM)                  (None, 500)          4202000     input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 1000)         0           lstm_27[0][0]                    \n",
      "                                                                 lstm_28[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 1600)         1601600     concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 1600)         2561600     dense_27[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 12,567,200\n",
      "Trainable params: 12,567,200\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "data_dim = latent_dim # Our model predicts embeddings, so our data is the size of the embedding\n",
    "\n",
    "# First layer\n",
    "input1 = Input(shape=(WINDOW_LENGTH, data_dim))\n",
    "input2 = Input(shape=(WINDOW_LENGTH, data_dim))\n",
    "lstm1 = LSTM(500)(input1)\n",
    "lstm2 = LSTM(500)(input2)\n",
    "\n",
    "# Second layer\n",
    "merged = concatenate([lstm1, lstm2])\n",
    "\n",
    "# Third layer\n",
    "hidden = Dense(data_dim, activation='relu')(merged)\n",
    "output = Dense(data_dim, activation='linear')(hidden)\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=output)\n",
    "model.compile(loss='mse', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0357 - acc: 0.1719\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 0.0257 - acc: 0.1748\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 0.0211 - acc: 0.3135\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 0.0260 - acc: 0.2744\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 0.0278 - acc: 0.1787\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 0.0261 - acc: 0.4346\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 0.0270 - acc: 0.3564\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 0.0223 - acc: 0.4541\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 0.0288 - acc: 0.3770\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 0.0165 - acc: 0.4111\n",
      "Saved Keras model to ./models/rlstm_v1.h5\n"
     ]
    }
   ],
   "source": [
    "# and now train the model\n",
    "# batch_size should be appropriate to your memory size\n",
    "# number of epochs should be higher for real world problems\n",
    "model.fit_generator(generator=data_gen,\n",
    "                    steps_per_epoch=32,\n",
    "                    epochs=10)\n",
    "\n",
    "MODEL_FILE = './models/rlstm_v1.h5'\n",
    "model.save(MODEL_FILE)# creates a HDF5 file\n",
    "print(\"Saved Keras model to\", MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### RegressionLSTM V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Overview\n",
    "\n",
    "Trying the rlstm again, but on a much smaller latent dimension than before (10 instead of 1600).\n",
    "\n",
    "From the _Next steps_ suggestion in RLSTM V1:\n",
    "\n",
    "> \"4. We might be able to make this prediction task easier if we reduce the latent dimension (intuitively, a smaller latent dimension means fewer ways to get it wrong).\"\n",
    "\n",
    "#### Notes\n",
    "\n",
    "It feels like the main problem here is a code problem, not an approach/model problem. The loss is still behaving erratically, going up and going down unpredictably. Trying to reduce the learning rate, and trying different optimizers did not seem to help. Current suspicion (based on [this](https://github.com/keras-team/keras/issues/5818)) is that training issues are due to the data generator, which may well be the issue since I don't understand it properly.\n",
    "\n",
    "Let's move away from the data generator for now, try make a traditional dataset to train on.\n",
    "\n",
    "#### Next steps\n",
    "Hard to say exactly. But some thoughts:\n",
    "1. Simplify: word2vec, and predict directly on single ticks instead of 96-tick units.\n",
    "2. Building the embeddings by itself may not be as useful as training the embeddings simultaneously with the prediction model. Training both models in conjunction with each other might be the best way to go.\n",
    "3. Find out if it's possible to get better results by forcing the model to overfit the training data. I thought this should be possible.\n",
    "4. ~~We might be able to make this prediction task easier if we reduce the latent dimension (intuitively, a smaller latent dimension means fewer ways to get it wrong).~~ Just did this with 10 dimensions, prediction doesn't seem any better than the 1600-dimension one.\n",
    "5. ~~Try a (much) larger window length?~~ Tried with WINDOW_LENGTH=39. It seems to help add confidence (velocity) to the output, but only barely. Not much difference in way of loss/accuracy, and anyways it should be possible to make predictions based on a small number of units (and even instantaneously based on the current input).\n",
    "6. ~~To increase reliance on input (rather than comp), add a dropout layer to the comp LSTM.~~ Tried this using variations of dropout and recurrent_dropout as LSTM parameters, but didn't have much effect. Probably need to solve some underlying problem first before applying dropout.\n",
    "7. Troubleshoot / don't use data generator, which may be causing all these problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "latent_dim = 10\n",
    "epsilon_std = 1.0\n",
    "# We need to have our encoder and decoder ready\n",
    "MODEL_FILE = './models/vae_v4_encoder.h5'\n",
    "encoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})\n",
    "MODEL_FILE = './models/vae_v4_generator.h5'\n",
    "decoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "WINDOW_LENGTH = 3\n",
    "\n",
    "# Create a data generator\n",
    "def get_multiple_generator(data1, data2, targets, window_length = 5, batch_size = 128):\n",
    "    data1_gen = TimeseriesGenerator(data1, targets,\n",
    "                               length=window_length, sampling_rate=1,\n",
    "                               batch_size=batch_size)\n",
    "\n",
    "    data2_gen = TimeseriesGenerator(data2, targets,\n",
    "                               length=window_length, sampling_rate=1,\n",
    "                               batch_size=batch_size)\n",
    "    while True:\n",
    "        for x in range(len(data1_gen)):\n",
    "            x_1, y = data1_gen[x]\n",
    "            x_2, _ = data2_gen[x]\n",
    "            yield [x_1, x_2], y\n",
    "\n",
    "data_gen = get_multiple_generator(seq_embed['input'], seq_embed['comp'], seq_embed['comp'], \n",
    "                                  window_length=WINDOW_LENGTH, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_35 (InputLayer)           (None, 3, 10)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_36 (InputLayer)           (None, 3, 10)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_35 (LSTM)                  (None, 200)          168800      input_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_36 (LSTM)                  (None, 200)          168800      input_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 400)          0           lstm_35[0][0]                    \n",
      "                                                                 lstm_36[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 1000)         401000      concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1000)         0           dense_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 10)           10010       dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 748,610\n",
      "Trainable params: 748,610\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "data_dim = latent_dim # Our model predicts embeddings, so our data is the size of the embedding\n",
    "\n",
    "# First layer\n",
    "input1 = Input(shape=(WINDOW_LENGTH, data_dim))\n",
    "input2 = Input(shape=(WINDOW_LENGTH, data_dim))\n",
    "lstm1 = LSTM(200)(input1)\n",
    "lstm2 = LSTM(200)(input2)\n",
    "\n",
    "# Second layer\n",
    "merged = concatenate([lstm1, lstm2])\n",
    "\n",
    "# Third layer\n",
    "hidden = Dense(1000, activation='relu')(merged)\n",
    "dropped = Dropout(0.2)(hidden)\n",
    "output = Dense(data_dim, activation='linear')(dropped)\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=output)\n",
    "sgd = optimizers.SGD(lr=0.001)\n",
    "model.compile(loss='mse', optimizer=sgd, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-9585c37ebfb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m model.fit_generator(generator=data_gen,\n\u001b[1;32m      5\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                     epochs=50)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mMODEL_FILE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./models/rlstm_v2.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/chanjunshern/tensorflow/local/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/chanjunshern/tensorflow/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2190\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2191\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2192\u001b[0;31m                     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/chanjunshern/tensorflow/local/lib/python2.7/site-packages/keras/utils/data_utils.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m                 \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/chanjunshern/tensorflow/local/lib/python2.7/site-packages/keras/utils/data_utils.pyc\u001b[0m in \u001b[0;36m_data_generator_task\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    656\u001b[0m                             \u001b[0;31m# => Serialize calls to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                             \u001b[0;31m# infinite iterator/generator's next() function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m                             \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple object is not an iterator"
     ]
    }
   ],
   "source": [
    "# and now train the model\n",
    "# batch_size should be appropriate to your memory size\n",
    "# number of epochs should be higher for real world problems\n",
    "model.fit_generator(generator=data_gen,\n",
    "                    steps_per_epoch=128,\n",
    "                    epochs=50)\n",
    "\n",
    "MODEL_FILE = './models/rlstm_v2.h5'\n",
    "model.save(MODEL_FILE)# creates a HDF5 file\n",
    "print(\"Saved Keras model to\", MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### RegressionLSTM V3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Overview\n",
    "\n",
    "From previous attempt:\n",
    "\n",
    "> Current suspicion (based on [this](https://github.com/keras-team/keras/issues/5818)) is that training issues are due to the data generator, which may well be the issue since I don't understand it properly.\n",
    "\n",
    "And so in this model, we simply try again but instead of using a data generator, we create the standard dataset setup.\n",
    "\n",
    "#### Notes\n",
    "\n",
    "All experiments in this version use a latent dimension of 10, corresponding to vae_v4.\n",
    "\n",
    "Changing the data generator worked! The loss progression now makes sense, the predictor is actually making some decent predictions!\n",
    "\n",
    "#### Attempt 1\n",
    "```\n",
    "Window length = 3\n",
    "Recurrent node = LSTM\n",
    "Recurrent dim = 200\n",
    "Hidden dim = 1000\n",
    "After 10 epochs, loss: 0.2585 - val_loss: 0.3402\n",
    "```\n",
    "\n",
    "Let's change from window length of 3 to 4, since songs tend to repeat after 4 measures.\n",
    "\n",
    "#### Attempt 2\n",
    "```\n",
    "Window length = 4\n",
    "After 10 epochs, loss: 0.2565 - val_loss: 0.3066\n",
    "```\n",
    "\n",
    "No huge difference in loss, but validation is better. Let's play with some of the other parameters. \n",
    "\n",
    "#### Attempt 3\n",
    "```\n",
    "Window length = 4\n",
    "Recurrent node: LSTM\n",
    "Recurrent dim = 800\n",
    "Hidden dim = 1000\n",
    "After 10 epochs, loss: 0.1736 - val_loss: 0.2841\n",
    "```\n",
    "Both training and validation loss are much better after amping up the number of neurons! But validation is a lot worse. Might need to apply some regularization.\n",
    "\n",
    "#### Attempt 4\n",
    "```\n",
    "Window length = 4\n",
    "Recurrent node: LSTM\n",
    "Recurrent dim = 800\n",
    "Hidden dim = 1000\n",
    "Dropout = 0.2 after hidden dense layer\n",
    "Dropout = 0.1 in input LSTM\n",
    "Dropout = 0.7 in comp LSTM\n",
    "After 50 epochs, loss: 0.1399 - val_loss: 0.2778\n",
    "```\n",
    "Overall, the main problem with the prediction now is that it depends too strongly on the previous comp window. And so, the new predictions get weaker and weaker and eventually die off after roughly one WINDOW_LENGTH of predictions. We need to force the model to learn to predict based on the input (more than the previous comp).\n",
    "\n",
    "To do this, we focused on increasing dropout on the comp input LSTM. This seems to help! Crucially, **it appears that validation (as well as training) loss is not as good as the version without dropout, but in reality when we are predicting on top of old predictions, the dropout is so much better because it forces the model to rely on the input instead of the previous comp**. There might be a need here to reframe the training data so that the model takes into account the fact that it will be predicting based on its own imperfect input. **Perhaps add noise to the data to model an imperfect prediction being fed into the next input**?\n",
    "\n",
    "For now, the predicted comp tends to be quite repetitive and plain, barely even noticeable.\n",
    "\n",
    "And quite importantly, the problem of playing back blurred pianorolls is quite a big issue now. Taking the mean velocity of a contiguous non-zero pitch is not enough anymore because quite often multiple repeated notes across several measures tend to blur into one long note with a single onset.\n",
    "\n",
    "#### Next steps\n",
    "1. Try adding noise to the input data so that we model making predictions based on imperfect previous comp predictions.\n",
    "2. How to fix the problem of blurred pianorolls? Either fix the playback function, or fix the autoencoder. Or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "latent_dim = 10\n",
    "epsilon_std = 1.0\n",
    "# We need to have our encoder and decoder ready\n",
    "MODEL_FILE = './models/vae_v4_encoder.h5'\n",
    "encoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})\n",
    "MODEL_FILE = './models/vae_v4_generator.h5'\n",
    "decoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_51 (InputLayer)           (None, 4, 10)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_52 (InputLayer)           (None, 4, 10)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_47 (LSTM)                  (None, 800)          2595200     input_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_48 (LSTM)                  (None, 800)          2595200     input_52[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 1600)         0           lstm_47[0][0]                    \n",
      "                                                                 lstm_48[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_47 (Dense)                (None, 1000)         1601000     concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1000)         0           dense_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_48 (Dense)                (None, 10)           10010       dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,801,410\n",
      "Trainable params: 6,801,410\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "data_dim = latent_dim # Our model predicts embeddings, so our data is the size of the embedding\n",
    "recurrent_dim = 800\n",
    "dense_hidden_dim = 1000\n",
    "\n",
    "# First layer\n",
    "input1 = Input(shape=(WINDOW_LENGTH, data_dim))\n",
    "input2 = Input(shape=(WINDOW_LENGTH, data_dim))\n",
    "lstm1 = LSTM(recurrent_dim, dropout=0.1)(input1)\n",
    "lstm2 = LSTM(recurrent_dim, dropout=0.7)(input2)\n",
    "\n",
    "# Second layer\n",
    "merged = concatenate([lstm1, lstm2])\n",
    "\n",
    "# Third layer\n",
    "hidden = Dense(dense_hidden_dim, activation='relu')(merged)\n",
    "dropped = Dropout(0.2)(hidden)\n",
    "output = Dense(data_dim, activation='linear')(dropped)\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=output)\n",
    "model.compile(loss='mse', optimizer='rmsprop') #, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40044 samples, validate on 2082 samples\n",
      "Epoch 1/50\n",
      "40044/40044 [==============================] - 18s 441us/step - loss: 0.5800 - val_loss: 0.4897\n",
      "Epoch 2/50\n",
      "40044/40044 [==============================] - 12s 305us/step - loss: 0.5133 - val_loss: 0.4567\n",
      "Epoch 3/50\n",
      "40044/40044 [==============================] - 12s 307us/step - loss: 0.4841 - val_loss: 0.4309\n",
      "Epoch 4/50\n",
      "40044/40044 [==============================] - 12s 306us/step - loss: 0.4590 - val_loss: 0.4110\n",
      "Epoch 5/50\n",
      "40044/40044 [==============================] - 12s 307us/step - loss: 0.4363 - val_loss: 0.3932\n",
      "Epoch 6/50\n",
      "40044/40044 [==============================] - 12s 305us/step - loss: 0.4166 - val_loss: 0.3836\n",
      "Epoch 7/50\n",
      "40044/40044 [==============================] - 12s 304us/step - loss: 0.3972 - val_loss: 0.3681\n",
      "Epoch 8/50\n",
      "40044/40044 [==============================] - 12s 306us/step - loss: 0.3800 - val_loss: 0.3500\n",
      "Epoch 9/50\n",
      "40044/40044 [==============================] - 12s 306us/step - loss: 0.3630 - val_loss: 0.3390\n",
      "Epoch 10/50\n",
      "40044/40044 [==============================] - 12s 307us/step - loss: 0.3469 - val_loss: 0.3293\n",
      "Epoch 11/50\n",
      "40044/40044 [==============================] - 12s 306us/step - loss: 0.3356 - val_loss: 0.3233\n",
      "Epoch 12/50\n",
      "40044/40044 [==============================] - 12s 307us/step - loss: 0.3203 - val_loss: 0.3198\n",
      "Epoch 13/50\n",
      "40044/40044 [==============================] - 12s 306us/step - loss: 0.3070 - val_loss: 0.3156\n",
      "Epoch 14/50\n",
      "40044/40044 [==============================] - 12s 307us/step - loss: 0.2962 - val_loss: 0.3117\n",
      "Epoch 15/50\n",
      "40044/40044 [==============================] - 12s 306us/step - loss: 0.2843 - val_loss: 0.3020\n",
      "Epoch 16/50\n",
      "40044/40044 [==============================] - 12s 308us/step - loss: 0.2742 - val_loss: 0.3022\n",
      "Epoch 17/50\n",
      "40044/40044 [==============================] - 12s 307us/step - loss: 0.2634 - val_loss: 0.2989\n",
      "Epoch 18/50\n",
      "40044/40044 [==============================] - 12s 308us/step - loss: 0.2542 - val_loss: 0.2943\n",
      "Epoch 19/50\n",
      "40044/40044 [==============================] - 12s 307us/step - loss: 0.2438 - val_loss: 0.2918\n",
      "Epoch 20/50\n",
      "40044/40044 [==============================] - 12s 302us/step - loss: 0.2372 - val_loss: 0.2918\n",
      "Epoch 21/50\n",
      "40044/40044 [==============================] - 12s 305us/step - loss: 0.2297 - val_loss: 0.2898\n",
      "Epoch 22/50\n",
      "40044/40044 [==============================] - 12s 307us/step - loss: 0.2222 - val_loss: 0.2859\n",
      "Epoch 23/50\n",
      "40044/40044 [==============================] - 12s 308us/step - loss: 0.2170 - val_loss: 0.2835\n",
      "Epoch 24/50\n",
      "40044/40044 [==============================] - 12s 307us/step - loss: 0.2101 - val_loss: 0.2838\n",
      "Epoch 25/50\n",
      "40044/40044 [==============================] - 12s 307us/step - loss: 0.2038 - val_loss: 0.2784\n",
      "Epoch 26/50\n",
      "40044/40044 [==============================] - 12s 306us/step - loss: 0.2000 - val_loss: 0.2848\n",
      "Epoch 27/50\n",
      "40044/40044 [==============================] - 12s 304us/step - loss: 0.1957 - val_loss: 0.2814\n",
      "Epoch 28/50\n",
      "40044/40044 [==============================] - 12s 307us/step - loss: 0.1911 - val_loss: 0.2833\n",
      "Epoch 29/50\n",
      "40044/40044 [==============================] - 12s 306us/step - loss: 0.1867 - val_loss: 0.2833\n",
      "Epoch 30/50\n",
      "40044/40044 [==============================] - 12s 304us/step - loss: 0.1836 - val_loss: 0.2800\n",
      "Epoch 31/50\n",
      "40044/40044 [==============================] - 12s 304us/step - loss: 0.1799 - val_loss: 0.2806\n",
      "Epoch 32/50\n",
      "40044/40044 [==============================] - 12s 306us/step - loss: 0.1766 - val_loss: 0.2818\n",
      "Epoch 33/50\n",
      "40044/40044 [==============================] - 12s 306us/step - loss: 0.1730 - val_loss: 0.2795\n",
      "Epoch 34/50\n",
      "40044/40044 [==============================] - 12s 307us/step - loss: 0.1700 - val_loss: 0.2786\n",
      "Epoch 35/50\n",
      "40044/40044 [==============================] - 12s 305us/step - loss: 0.1666 - val_loss: 0.2800\n",
      "Epoch 36/50\n",
      "40044/40044 [==============================] - 12s 306us/step - loss: 0.1650 - val_loss: 0.2813\n",
      "Epoch 37/50\n",
      "40044/40044 [==============================] - 12s 306us/step - loss: 0.1624 - val_loss: 0.2800\n",
      "Epoch 38/50\n",
      "40044/40044 [==============================] - 12s 307us/step - loss: 0.1605 - val_loss: 0.2799\n",
      "Epoch 39/50\n",
      "40044/40044 [==============================] - 12s 306us/step - loss: 0.1591 - val_loss: 0.2789\n",
      "Epoch 40/50\n",
      "40044/40044 [==============================] - 12s 306us/step - loss: 0.1555 - val_loss: 0.2806\n",
      "Epoch 41/50\n",
      "40044/40044 [==============================] - 12s 307us/step - loss: 0.1541 - val_loss: 0.2824\n",
      "Epoch 42/50\n",
      "40044/40044 [==============================] - 12s 307us/step - loss: 0.1522 - val_loss: 0.2804\n",
      "Epoch 43/50\n",
      "40044/40044 [==============================] - 12s 306us/step - loss: 0.1506 - val_loss: 0.2812\n",
      "Epoch 44/50\n",
      "40044/40044 [==============================] - 12s 306us/step - loss: 0.1491 - val_loss: 0.2788\n",
      "Epoch 45/50\n",
      "40044/40044 [==============================] - 12s 305us/step - loss: 0.1464 - val_loss: 0.2820\n",
      "Epoch 46/50\n",
      "40044/40044 [==============================] - 12s 303us/step - loss: 0.1459 - val_loss: 0.2807\n",
      "Epoch 47/50\n",
      "40044/40044 [==============================] - 12s 304us/step - loss: 0.1447 - val_loss: 0.2762\n",
      "Epoch 48/50\n",
      "40044/40044 [==============================] - 12s 306us/step - loss: 0.1431 - val_loss: 0.2802\n",
      "Epoch 49/50\n",
      "40044/40044 [==============================] - 12s 308us/step - loss: 0.1415 - val_loss: 0.2780\n",
      "Epoch 50/50\n",
      "40044/40044 [==============================] - 12s 307us/step - loss: 0.1399 - val_loss: 0.2778\n",
      "Saved Keras model to ./models/rlstm_v3.h5\n"
     ]
    }
   ],
   "source": [
    "# and now train the model\n",
    "# batch_size should be appropriate to your memory size\n",
    "# number of epochs should be higher for real world problems\n",
    "model.fit([x_input_train, x_comp_train], y_comp_train,\n",
    "          epochs=50,\n",
    "          batch_size=128,\n",
    "          shuffle=True,\n",
    "          validation_data=([x_input_test, x_comp_test], y_comp_test))\n",
    "\n",
    "MODEL_FILE = './models/rlstm_v3.h5'\n",
    "model.save(MODEL_FILE)# creates a HDF5 file\n",
    "print(\"Saved Keras model to\", MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### RegressionLSTM V4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Overview\n",
    "\n",
    "Carried forward from previous model, with some new tests:\n",
    "- Uses VAE_V5, latent dimension of 500 and pianorolls with NUM_PITCHES = 96\n",
    "- Add noise to the training comp embeddings, to model imperfect predictions\n",
    "\n",
    "#### Notes\n",
    "\n",
    "All experiments in this version use a latent dimension of 500, corresponding to vae_v5.\n",
    "\n",
    "#### Attempt 1\n",
    "```\n",
    "Window length = 4\n",
    "Recurrent node: LSTM\n",
    "Recurrent dim = 800\n",
    "Hidden dim = 1000\n",
    "Dropout = 0.2 after hidden dense layer\n",
    "Dropout = 0.1 in input LSTM\n",
    "Dropout = 0.7 in comp LSTM\n",
    "After 50 epochs, \n",
    "```\n",
    "Loss from previous model was `loss: 0.1399 - val_loss: 0.2778`, but apparently that's not comparable, I'm guessing because the latent dimension size has changed? But why would the dimension affect the _mean_ squared error?\n",
    "\n",
    "But frankly, the predicted output looks pretty _perfect_. This is almost certainly a case of overfitting, unfortunately. We need to prepare a test set of sequence data to properly visualize against (we cannot use the same test data we used to get the validation error because those are not in sequence).\n",
    "\n",
    "#### Update\n",
    "\n",
    "Yes, after testing on a completely separate test dataset, I can indeed confirm that the model was just totally overfitting, and the predictions on the test dataset are mostly ghost notes. But now we have a proper target to aim for. Onwards!\n",
    "\n",
    "#### Next steps\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "latent_dim = 500\n",
    "epsilon_std = 1.0\n",
    "# We need to have our encoder and decoder ready\n",
    "MODEL_FILE = './models/vae_v5_encoder_denoise.h5'\n",
    "encoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})\n",
    "MODEL_FILE = './models/vae_v5_generator_denoise.h5'\n",
    "decoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 4, 500)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 4, 500)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 800)          4163200     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 800)          4163200     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1600)         0           lstm_1[0][0]                     \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1000)         1601000     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1000)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 500)          500500      dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 10,427,900\n",
      "Trainable params: 10,427,900\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "data_dim = latent_dim # Our model predicts embeddings, so our data is the size of the embedding\n",
    "recurrent_dim = 800\n",
    "dense_hidden_dim = 1000\n",
    "\n",
    "# First layer\n",
    "input1 = Input(shape=(WINDOW_LENGTH, data_dim))\n",
    "input2 = Input(shape=(WINDOW_LENGTH, data_dim))\n",
    "lstm1 = LSTM(recurrent_dim, dropout=0.1)(input1)\n",
    "lstm2 = LSTM(recurrent_dim, dropout=0.7)(input2)\n",
    "\n",
    "# Second layer\n",
    "merged = concatenate([lstm1, lstm2])\n",
    "\n",
    "# Third layer\n",
    "hidden = Dense(dense_hidden_dim, activation='relu')(merged)\n",
    "dropped = Dropout(0.2)(hidden)\n",
    "output = Dense(data_dim, activation='linear')(dropped)\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=output)\n",
    "model.compile(loss='mse', optimizer='rmsprop') #, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34991 samples, validate on 1817 samples\n",
      "Epoch 1/50\n",
      "34991/34991 [==============================] - 14s 399us/step - loss: 0.0348 - val_loss: 0.0309\n",
      "Epoch 2/50\n",
      "34991/34991 [==============================] - 14s 394us/step - loss: 0.0275 - val_loss: 0.0289\n",
      "Epoch 3/50\n",
      "34991/34991 [==============================] - 14s 394us/step - loss: 0.0237 - val_loss: 0.0279\n",
      "Epoch 4/50\n",
      "34991/34991 [==============================] - 14s 398us/step - loss: 0.0213 - val_loss: 0.0273\n",
      "Epoch 5/50\n",
      "34991/34991 [==============================] - 14s 397us/step - loss: 0.0195 - val_loss: 0.0269\n",
      "Epoch 6/50\n",
      "34991/34991 [==============================] - 14s 394us/step - loss: 0.0182 - val_loss: 0.0266\n",
      "Epoch 7/50\n",
      "34991/34991 [==============================] - 14s 395us/step - loss: 0.0171 - val_loss: 0.0264\n",
      "Epoch 8/50\n",
      "34991/34991 [==============================] - 14s 394us/step - loss: 0.0163 - val_loss: 0.0265\n",
      "Epoch 9/50\n",
      "34991/34991 [==============================] - 14s 394us/step - loss: 0.0156 - val_loss: 0.0267\n",
      "Epoch 10/50\n",
      "34991/34991 [==============================] - 14s 396us/step - loss: 0.0151 - val_loss: 0.0262\n",
      "Epoch 11/50\n",
      "34991/34991 [==============================] - 14s 398us/step - loss: 0.0145 - val_loss: 0.0262\n",
      "Epoch 12/50\n",
      "34991/34991 [==============================] - 14s 395us/step - loss: 0.0142 - val_loss: 0.0262\n",
      "Epoch 13/50\n",
      "34991/34991 [==============================] - 14s 396us/step - loss: 0.0139 - val_loss: 0.0260\n",
      "Epoch 14/50\n",
      "34991/34991 [==============================] - 14s 397us/step - loss: 0.0135 - val_loss: 0.0261\n",
      "Epoch 15/50\n",
      "34991/34991 [==============================] - 14s 399us/step - loss: 0.0132 - val_loss: 0.0263\n",
      "Epoch 16/50\n",
      "34991/34991 [==============================] - 14s 399us/step - loss: 0.0130 - val_loss: 0.0263\n",
      "Epoch 17/50\n",
      "34991/34991 [==============================] - 14s 397us/step - loss: 0.0127 - val_loss: 0.0261\n",
      "Epoch 18/50\n",
      "34991/34991 [==============================] - 14s 397us/step - loss: 0.0126 - val_loss: 0.0262\n",
      "Epoch 19/50\n",
      "34991/34991 [==============================] - 14s 394us/step - loss: 0.0124 - val_loss: 0.0260\n",
      "Epoch 20/50\n",
      "34991/34991 [==============================] - 14s 395us/step - loss: 0.0122 - val_loss: 0.0263\n",
      "Epoch 21/50\n",
      "34991/34991 [==============================] - 14s 397us/step - loss: 0.0120 - val_loss: 0.0262\n",
      "Epoch 22/50\n",
      "34991/34991 [==============================] - 14s 399us/step - loss: 0.0119 - val_loss: 0.0263\n",
      "Epoch 23/50\n",
      "34991/34991 [==============================] - 14s 398us/step - loss: 0.0117 - val_loss: 0.0264\n",
      "Epoch 24/50\n",
      "34991/34991 [==============================] - 14s 397us/step - loss: 0.0116 - val_loss: 0.0264\n",
      "Epoch 25/50\n",
      "34991/34991 [==============================] - 14s 395us/step - loss: 0.0115 - val_loss: 0.0265\n",
      "Epoch 26/50\n",
      "34991/34991 [==============================] - 14s 394us/step - loss: 0.0114 - val_loss: 0.0266\n",
      "Epoch 27/50\n",
      "34991/34991 [==============================] - 14s 397us/step - loss: 0.0113 - val_loss: 0.0262\n",
      "Epoch 28/50\n",
      "34991/34991 [==============================] - 14s 397us/step - loss: 0.0112 - val_loss: 0.0264\n",
      "Epoch 29/50\n",
      "34991/34991 [==============================] - 14s 397us/step - loss: 0.0111 - val_loss: 0.0263\n",
      "Epoch 30/50\n",
      "34991/34991 [==============================] - 14s 398us/step - loss: 0.0110 - val_loss: 0.0263\n",
      "Epoch 31/50\n",
      "34991/34991 [==============================] - 14s 398us/step - loss: 0.0109 - val_loss: 0.0264\n",
      "Epoch 32/50\n",
      "34991/34991 [==============================] - 14s 398us/step - loss: 0.0108 - val_loss: 0.0264\n",
      "Epoch 33/50\n",
      "34991/34991 [==============================] - 14s 398us/step - loss: 0.0107 - val_loss: 0.0264\n",
      "Epoch 34/50\n",
      "34991/34991 [==============================] - 14s 397us/step - loss: 0.0106 - val_loss: 0.0264\n",
      "Epoch 35/50\n",
      "34991/34991 [==============================] - 14s 397us/step - loss: 0.0106 - val_loss: 0.0266\n",
      "Epoch 36/50\n",
      "34991/34991 [==============================] - 14s 394us/step - loss: 0.0105 - val_loss: 0.0264\n",
      "Epoch 37/50\n",
      "34991/34991 [==============================] - 14s 392us/step - loss: 0.0104 - val_loss: 0.0265\n",
      "Epoch 38/50\n",
      "34991/34991 [==============================] - 14s 395us/step - loss: 0.0103 - val_loss: 0.0267\n",
      "Epoch 39/50\n",
      "34991/34991 [==============================] - 14s 397us/step - loss: 0.0103 - val_loss: 0.0267\n",
      "Epoch 40/50\n",
      "34991/34991 [==============================] - 14s 395us/step - loss: 0.0102 - val_loss: 0.0266\n",
      "Epoch 41/50\n",
      "34991/34991 [==============================] - 14s 397us/step - loss: 0.0102 - val_loss: 0.0266\n",
      "Epoch 42/50\n",
      "34991/34991 [==============================] - 14s 395us/step - loss: 0.0101 - val_loss: 0.0265\n",
      "Epoch 43/50\n",
      "34991/34991 [==============================] - 14s 397us/step - loss: 0.0101 - val_loss: 0.0265\n",
      "Epoch 44/50\n",
      "34991/34991 [==============================] - 14s 396us/step - loss: 0.0100 - val_loss: 0.0265\n",
      "Epoch 45/50\n",
      "34991/34991 [==============================] - 14s 399us/step - loss: 0.0099 - val_loss: 0.0267\n",
      "Epoch 46/50\n",
      "34991/34991 [==============================] - 14s 398us/step - loss: 0.0099 - val_loss: 0.0267\n",
      "Epoch 47/50\n",
      "34991/34991 [==============================] - 14s 399us/step - loss: 0.0099 - val_loss: 0.0266\n",
      "Epoch 48/50\n",
      "34991/34991 [==============================] - 14s 392us/step - loss: 0.0098 - val_loss: 0.0266\n",
      "Epoch 49/50\n",
      "34991/34991 [==============================] - 14s 393us/step - loss: 0.0098 - val_loss: 0.0267\n",
      "Epoch 50/50\n",
      "34991/34991 [==============================] - 14s 396us/step - loss: 0.0097 - val_loss: 0.0267\n",
      "Saved Keras model to ./models/rlstm_v4.h5\n"
     ]
    }
   ],
   "source": [
    "# and now train the model\n",
    "# batch_size should be appropriate to your memory size\n",
    "# number of epochs should be higher for real world problems\n",
    "model.fit([x_input_train, x_comp_train], y_comp_train,\n",
    "          epochs=50,\n",
    "          batch_size=128,\n",
    "          shuffle=True,\n",
    "          validation_data=([x_input_test, x_comp_test], y_comp_test))\n",
    "\n",
    "MODEL_FILE = './models/rlstm_v4.h5'\n",
    "model.save(MODEL_FILE)# creates a HDF5 file\n",
    "print(\"Saved Keras model to\", MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### RegressionLSTM V5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Overview\n",
    "\n",
    "Building on top of V4 model, but with crucial new changes:\n",
    "- Uses VAE_V7 (latent dimension of 500 and pianorolls with NUM_PITCHES = 96)\n",
    "- Trying something new: using `fit_generator` to generate windowed data on the fly for data efficiency\n",
    "\n",
    "#### Notes\n",
    "\n",
    "All experiments in this version use a latent dimension of 500, corresponding to vae_v7.\n",
    "\n",
    "#### Attempt 1\n",
    "```\n",
    "Num pianorolls: 1000\n",
    "input.shape == comp.shape: (23086, 500)\n",
    "Time: ~7 minutes per epoch\n",
    "Time per step: 453s 264ms/step\n",
    "Best error: loss: 0.0235 - val_loss: 0.0317\n",
    "```\n",
    "Validation loss is quite disappointing, it scarcely improves at all from epoch 1 to epoch 50. Training loss goes down nicely, and we can confirm that testing on training data we get pretty perfect results, but on test data it is mostly ghost output (but very slightly better than the previous model?).\n",
    "\n",
    "#### Attempt 1.b\n",
    "\n",
    "Use Attempt 1's model to predict accompaniments with a unit-selection scheme. Unforunately, output is very poor. Selected units don't seem to be in the same key as the input. On the bright side, at least it does something.\n",
    "\n",
    "#### Next steps\n",
    "1. Try using the predicted output with unit selection!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "latent_dim = 500\n",
    "epsilon_std = 1.0\n",
    "# We need to have our encoder and decoder ready\n",
    "MODEL_FILE = './models/vae_v7_encoder.h5'\n",
    "encoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})\n",
    "MODEL_FILE = './models/vae_v7_decoder.h5'\n",
    "decoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 4, 500)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 4, 500)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 800)          4163200     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 800)          4163200     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1600)         0           lstm_1[0][0]                     \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1000)         1601000     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1000)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 500)          500500      dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 10,427,900\n",
      "Trainable params: 10,427,900\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "data_dim = latent_dim # Our model predicts embeddings, so our data is the size of the embedding\n",
    "recurrent_dim = 800\n",
    "dense_hidden_dim = 1000\n",
    "\n",
    "# First layer\n",
    "input1 = Input(shape=(WINDOW_LENGTH, data_dim))\n",
    "input2 = Input(shape=(WINDOW_LENGTH, data_dim))\n",
    "lstm1 = LSTM(recurrent_dim, dropout=0.1)(input1)\n",
    "lstm2 = LSTM(recurrent_dim, dropout=0.7)(input2)\n",
    "\n",
    "# Second layer\n",
    "merged = concatenate([lstm1, lstm2])\n",
    "\n",
    "# Third layer\n",
    "hidden = Dense(dense_hidden_dim, activation='relu')(merged)\n",
    "dropped = Dropout(0.2)(hidden)\n",
    "output = Dense(data_dim, activation='linear')(dropped)\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=output)\n",
    "model.compile(loss='mse', optimizer='rmsprop') #, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1716/1716 [==============================] - 461s 269ms/step - loss: 0.0342 - val_loss: 0.0305\n",
      "Epoch 2/10\n",
      "1716/1716 [==============================] - 459s 267ms/step - loss: 0.0312 - val_loss: 0.0300\n",
      "Epoch 3/10\n",
      "1716/1716 [==============================] - 455s 265ms/step - loss: 0.0302 - val_loss: 0.0299\n",
      "Epoch 4/10\n",
      "1716/1716 [==============================] - 455s 265ms/step - loss: 0.0294 - val_loss: 0.0301\n",
      "Epoch 5/10\n",
      "1716/1716 [==============================] - 456s 266ms/step - loss: 0.0285 - val_loss: 0.0303\n",
      "Epoch 6/10\n",
      "1716/1716 [==============================] - 455s 265ms/step - loss: 0.0276 - val_loss: 0.0304\n",
      "Epoch 7/10\n",
      "1716/1716 [==============================] - 451s 263ms/step - loss: 0.0266 - val_loss: 0.0305\n",
      "Epoch 8/10\n",
      "1716/1716 [==============================] - 455s 265ms/step - loss: 0.0255 - val_loss: 0.0309\n",
      "Epoch 9/10\n",
      "1716/1716 [==============================] - 454s 265ms/step - loss: 0.0245 - val_loss: 0.0313\n",
      "Epoch 10/10\n",
      "1716/1716 [==============================] - 453s 264ms/step - loss: 0.0235 - val_loss: 0.0317\n",
      "Saved Keras model to ./models/rlstm_v5.h5\n"
     ]
    }
   ],
   "source": [
    "# and now train the model\n",
    "# batch_size should be appropriate to your memory size\n",
    "# number of epochs should be higher for real world problems\n",
    "\n",
    "batch_size=128\n",
    "model.fit_generator(generator=data_gen_train,\n",
    "                    steps_per_epoch=len(data_gen_train),#/batch_size,\n",
    "                    epochs=10,\n",
    "                    validation_data=data_gen_test)\n",
    "\n",
    "MODEL_FILE = './models/rlstm_v5.h5'\n",
    "model.save(MODEL_FILE)# creates a HDF5 file\n",
    "print(\"Saved Keras model to\", MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### RegressionLSTM V6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Overview\n",
    "\n",
    "Onsets-only variation of V5 model:\n",
    "- Uses VAE_V9 (latent dimension of 50 and pianorolls with NUM_PITCHES = 96)\n",
    "- MUUCH smaller network since latent dimension is also much smaller (dropped from 10,427,900 to )\n",
    "- Add Gaussian Noise to input_2\n",
    "\n",
    "#### Attempt 1\n",
    "```\n",
    "Num pianorolls: 1000\n",
    "input.shape == comp.shape: (52540, 50)\n",
    "\n",
    "GaussianNoise@input_2: 0.2\n",
    "\n",
    "After 10 epochs,\n",
    "loss: 0.4549 - val_loss: 0.5434\n",
    "\n",
    "Saved Keras model to ./models/rlstm_v6.h5\n",
    "CPU times: user 1h 15min 56s, sys: 42min 14s, total: 1h 58min 10s\n",
    "Wall time: 1h 35min 30s\n",
    "```\n",
    "Like in previous models, validation loss scarcely improves at all from epoch 1 to epoch 10. Training loss goes down slowly.\n",
    "\n",
    "#### Next steps\n",
    "1. Try using the predicted output with unit selection!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "latent_dim = 50\n",
    "epsilon_std = 1.0\n",
    "# We need to have our encoder and decoder ready\n",
    "MODEL_FILE = './models/vae_v9_encoder_latent50.h5'\n",
    "encoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})\n",
    "MODEL_FILE = './models/vae_v9_decoder_latent50.h5'\n",
    "decoder = load_model(MODEL_FILE, \n",
    "                         custom_objects={'latent_dim': latent_dim, 'epsilon_std': epsilon_std})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 4, 50)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 4, 50)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_4 (GaussianNoise (None, 4, 50)        0           input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, 200)          200800      input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 200)          200800      gaussian_noise_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 400)          0           lstm_7[0][0]                     \n",
      "                                                                 lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 500)          200500      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 50)           25050       dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 627,150\n",
      "Trainable params: 627,150\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "WINDOW_LENGTH = 4\n",
    "data_dim = latent_dim # Our model predicts embeddings, so our data is the size of the embedding\n",
    "recurrent_dim = 200\n",
    "dense_hidden_dim = 500\n",
    "\n",
    "# First layer\n",
    "input1 = Input(shape=(WINDOW_LENGTH, data_dim))\n",
    "input2 = Input(shape=(WINDOW_LENGTH, data_dim))\n",
    "input2_noisy = GaussianNoise(0.2)(input2)\n",
    "lstm1 = LSTM(recurrent_dim)(input1)\n",
    "lstm2 = LSTM(recurrent_dim)(input2_noisy)\n",
    "\n",
    "# Second layer\n",
    "merged = concatenate([lstm1, lstm2])\n",
    "\n",
    "# Third layer\n",
    "hidden = Dense(dense_hidden_dim, activation='relu')(merged)\n",
    "output = Dense(data_dim, activation='linear')(hidden)\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=output)\n",
    "model.compile(loss='mse', optimizer='adam') #, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3451/3451 [==============================] - 563s 163ms/step - loss: 0.5571 - val_loss: 0.5323\n",
      "Epoch 2/10\n",
      "3451/3451 [==============================] - 569s 165ms/step - loss: 0.5254 - val_loss: 0.5345\n",
      "Epoch 3/10\n",
      "3451/3451 [==============================] - 568s 165ms/step - loss: 0.5090 - val_loss: 0.5301\n",
      "Epoch 4/10\n",
      "3451/3451 [==============================] - 560s 162ms/step - loss: 0.4975 - val_loss: 0.5325\n",
      "Epoch 5/10\n",
      "3451/3451 [==============================] - 560s 162ms/step - loss: 0.4884 - val_loss: 0.5318\n",
      "Epoch 6/10\n",
      "3451/3451 [==============================] - 582s 169ms/step - loss: 0.4801 - val_loss: 0.5343\n",
      "Epoch 7/10\n",
      "3451/3451 [==============================] - 583s 169ms/step - loss: 0.4726 - val_loss: 0.5377\n",
      "Epoch 8/10\n",
      "3451/3451 [==============================] - 586s 170ms/step - loss: 0.4666 - val_loss: 0.5393\n",
      "Epoch 9/10\n",
      "3451/3451 [==============================] - 585s 170ms/step - loss: 0.4612 - val_loss: 0.5398\n",
      "Epoch 10/10\n",
      "3451/3451 [==============================] - 573s 166ms/step - loss: 0.4549 - val_loss: 0.5434\n",
      "Saved Keras model to ./models/rlstm_v6.h5\n",
      "CPU times: user 1h 15min 56s, sys: 42min 14s, total: 1h 58min 10s\n",
      "Wall time: 1h 35min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# and now train the model\n",
    "# batch_size should be appropriate to your memory size\n",
    "# number of epochs should be higher for real world problems\n",
    "\n",
    "batch_size=128\n",
    "model.fit_generator(generator=data_gen_train,\n",
    "                    steps_per_epoch=len(data_gen_train),#/batch_size,\n",
    "                    epochs=10,\n",
    "                    validation_data=data_gen_test)\n",
    "\n",
    "MODEL_FILE = './models/rlstm_v6.h5'\n",
    "model.save(MODEL_FILE)# creates a HDF5 file\n",
    "print(\"Saved Keras model to\", MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MODEL_FILE = './models/rlstm_v6.h5'\n",
    "model = load_model(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Prediction and results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Use the models trained above to predict an accompanying sequence.\n",
    "\n",
    "#### DEPRECATED\n",
    "The dataset used in this section are completely separated from the model training processes above. \n",
    "\n",
    "Model training had their own training/validation dataset, but here we actually have a previously unseen dataset `seq_units['input_test']` and `seq_units['comp_test']` and their corresponding embeddings dataset for prediction. \n",
    "\n",
    "The reason we cannot use the train/validation set here is because the order beyond a fixed window size is not respected in the train/validation dataset. This test set retains the full order of many songs together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn_units: (220409, 96, 96, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import sklearn.externals\n",
    "\n",
    "# Load up the embeddings prediction model\n",
    "MODEL_FILE = './models/rlstm_v5.h5'\n",
    "model = load_model(MODEL_FILE)\n",
    "\n",
    "# Load up the KNN model\n",
    "KNN_MODEL_FILENAME = \"./models/vae_v7_unit_selector_knn.pkl\"\n",
    "knn_model, UNITS_FILE = sklearn.externals.joblib.load(KNN_MODEL_FILENAME)\n",
    "# Load KNN units\n",
    "units_f = h5py.File(UNITS_FILE, 'r')\n",
    "knn_units = units_f['units_train']\n",
    "print(\"knn_units:\", knn_units.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# If not using HDF5\n",
    "seq_units_input = seq_units['input_train']\n",
    "seq_units_comp = seq_units['comp_train']\n",
    "seq_embed_input = seq_embed['input_train']\n",
    "seq_embed_comp = seq_embed['comp_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011178691864\n",
      "CPU times: user 15.6 s, sys: 3.22 s, total: 18.8 s\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "NUM_PREDICTIONS = 8\n",
    "num_units = WINDOW_LENGTH + NUM_PREDICTIONS\n",
    "# Grab a random sequence of embeddings as input\n",
    "unit_index = np.random.randint(seq_embed_comp.shape[0] - num_units)\n",
    "\n",
    "# Take the full WINDOW_LENGTH + 1 embeddings for reference\n",
    "decoded_comp_embed = seq_embed_comp[unit_index:unit_index + num_units]\n",
    "\n",
    "# Predict NUM_PREDICTIONS embeddings\n",
    "predicted_comp_embed = seq_embed_input[unit_index:unit_index + WINDOW_LENGTH].copy() #seq_embed_comp[unit_index:unit_index + WINDOW_LENGTH].copy()\n",
    "# knn_comp = np.zeros((WINDOW_LENGTH, NUM_PITCHES, NUM_TICKS)) #seq_units_comp[unit_index:unit_index + WINDOW_LENGTH].copy().squeeze()\n",
    "for i, _ in enumerate(range(NUM_PREDICTIONS)):\n",
    "    input1 = np.array([\n",
    "        seq_embed_input[unit_index + i : unit_index + WINDOW_LENGTH + i]\n",
    "    ])\n",
    "    input2 = np.array([\n",
    "        predicted_comp_embed[-WINDOW_LENGTH:] # The WINDOW_LENGTH latest units\n",
    "    ])\n",
    "    # Predict new embedding\n",
    "    y_comp_embed = model.predict([input1,input2])\n",
    "    # Append predicted embedding\n",
    "    predicted_comp_embed = np.concatenate([predicted_comp_embed, y_comp_embed], axis=0)\n",
    "    \n",
    "    # Get nearest neighbor\n",
    "#     knn_index = knn_model.kneighbors(y_comp_embed, return_distance = False)[0][0]\n",
    "#     sample_knn = knn_units[knn_index].squeeze()[np.newaxis, ...]\n",
    "#     knn_comp = np.concatenate([knn_comp, sample_knn], axis=0)\n",
    "\n",
    "# knn_comp = np.concatenate(knn_comp, axis=1)\n",
    "print(predicted_comp_embed.shape)\n",
    "# print(knn_comp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x475.2 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reload(pianoroll_utils)\n",
    "# Decode the embeddings\n",
    "predicted_comp_probs = np.concatenate(decoder.predict(predicted_comp_embed), axis=1).squeeze()\n",
    "# predicted_comp = np.clip(predicted_comp * 0.2, 0, 1)\n",
    "predicted_comp = pianoroll_utils.pianoroll_preprocess(predicted_comp_probs, MIN_PITCH, MAX_PITCH, is_onsets_matrix=True)\n",
    "predicted_comp[:43] = 0\n",
    "predicted_comp[:,:16*24] = 0\n",
    "# Original embeddings for reference\n",
    "decoded_comp = np.concatenate(decoder.predict(decoded_comp_embed), axis=1).squeeze()\n",
    "# Also grab the original units for comparison\n",
    "model_comp = np.concatenate(seq_units_comp[unit_index:unit_index + num_units], axis=1).squeeze()\n",
    "model_input = np.concatenate(seq_units_input[unit_index:unit_index + num_units], axis=1).squeeze()\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(3,1)\n",
    "fig.set_size_inches(10, 3*2.2, forward=True)\n",
    "ax[0].set_title('Input')\n",
    "ax[1].set_title('True Comp')\n",
    "# ax[2].set_title('Decoded Comp')\n",
    "ax[2].set_title('Predicted Comp')\n",
    "# ax[4].set_title('KNN Comp')\n",
    "\n",
    "min_pitch = 36\n",
    "max_pitch = 72\n",
    "model_input_zoom = pianoroll_utils.crop_pianoroll(pianoroll_utils.pad_pianoroll(model_input, MIN_PITCH, MAX_PITCH), min_pitch, max_pitch)\n",
    "model_comp_zoom = pianoroll_utils.crop_pianoroll(pianoroll_utils.pad_pianoroll(model_comp, MIN_PITCH, MAX_PITCH), min_pitch, max_pitch)\n",
    "predicted_comp_zoom = pianoroll_utils.crop_pianoroll(pianoroll_utils.pad_pianoroll(predicted_comp, MIN_PITCH, MAX_PITCH), min_pitch, max_pitch)\n",
    "\n",
    "pianoroll_utils.plot_pianoroll(ax[0], model_input_zoom, min_pitch, max_pitch, cmap='bone_r')\n",
    "pianoroll_utils.plot_pianoroll(ax[1], model_comp_zoom, min_pitch, max_pitch, cmap='bone_r')\n",
    "# pianoroll_utils.plot_pianoroll(ax[2], decoded_comp, MIN_PITCH, MAX_PITCH, cmap='bone_r')\n",
    "pianoroll_utils.plot_pianoroll(ax[2], predicted_comp_zoom, min_pitch, max_pitch, cmap='bone_r')\n",
    "# pianoroll_utils.plot_pianoroll(ax[4], knn_comp, MIN_PITCH, MAX_PITCH, beat_resolution=24, cmap='Blues')\n",
    "fig.tight_layout()\n",
    "plt.savefig('fig/latent_rnn_traindata.eps', format='eps', dpi=1000)\n",
    "\n",
    "# Listen to the MIDI files\n",
    "# pianoroll_utils.play_pianoroll(model_input, MIN_PITCH, MAX_PITCH, 'ori', is_onsets_matrix=True, program_number=13)\n",
    "# # pianoroll_utils.play_pianoroll(model_comp, MIN_PITCH, MAX_PITCH, '0', is_onsets_matrix=True, program_number=13)\n",
    "# # pianoroll_utils.play_pianoroll(decoded_comp, MIN_PITCH, MAX_PITCH, '1', is_onsets_matrix=True, program_number=13)\n",
    "# pianoroll_utils.play_pianoroll(predicted_comp, MIN_PITCH, MAX_PITCH, '2', is_onsets_matrix=True, program_number=13)\n",
    "# # pianoroll_utils.play_pianoroll(knn_comp, MIN_PITCH, MAX_PITCH, '3', is_onsets_matrix=True, program_number=13)\n",
    "# pianoroll_utils.play_pianoroll( (model_comp + model_input).clip(0,1), MIN_PITCH, MAX_PITCH, '4', is_onsets_matrix=True, program_number=13)\n",
    "# # pianoroll_utils.play_pianoroll( (decoded_comp + model_input).clip(0,1), MIN_PITCH, MAX_PITCH, '5', is_onsets_matrix=True, program_number=13)\n",
    "# pianoroll_utils.play_pianoroll( (predicted_comp + model_input).clip(0,1), MIN_PITCH, MAX_PITCH, '6', is_onsets_matrix=True, program_number=13)\n",
    "# # pianoroll_utils.play_pianoroll( (knn_comp + model_input).clip(0,1), MIN_PITCH, MAX_PITCH, '7', is_onsets_matrix=True, program_number=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "unit_index = 148868"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
